--------------------------------------------
Timing phi-4 with NONE

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.95s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.94s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.86s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.80s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.48s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.76s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:09, 96.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 135.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 161.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 178.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 191.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 198.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 203.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 208.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 210.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 213.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 215.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 215.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 215.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 215.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 217.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 216.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 216.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 217.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 217.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 217.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 218.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 218.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 218.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 218.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 217.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 218.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 218.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 218.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 218.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 218.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 218.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 212.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 209.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 234.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 237.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 203.14 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747604137.2182362
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:54, 18.43s/it]  7%|▋         | 2/30 [00:19<03:57,  8.47s/it] 10%|█         | 3/30 [00:21<02:22,  5.30s/it] 13%|█▎        | 4/30 [00:23<01:39,  3.84s/it] 17%|█▋        | 5/30 [00:24<01:17,  3.11s/it] 20%|██        | 6/30 [00:26<01:02,  2.60s/it] 23%|██▎       | 7/30 [00:27<00:51,  2.24s/it] 27%|██▋       | 8/30 [00:29<00:44,  2.01s/it] 30%|███       | 9/30 [00:31<00:40,  1.95s/it] 33%|███▎      | 10/30 [00:32<00:36,  1.81s/it] 37%|███▋      | 11/30 [00:34<00:32,  1.70s/it] 40%|████      | 12/30 [00:35<00:29,  1.63s/it] 43%|████▎     | 13/30 [00:37<00:26,  1.59s/it] 47%|████▋     | 14/30 [00:38<00:24,  1.55s/it] 50%|█████     | 15/30 [00:40<00:23,  1.54s/it]                                                50%|█████     | 15/30 [00:40<00:23,  1.54s/it] 53%|█████▎    | 16/30 [00:41<00:21,  1.52s/it] 57%|█████▋    | 17/30 [00:43<00:19,  1.50s/it] 60%|██████    | 18/30 [00:44<00:18,  1.57s/it] 63%|██████▎   | 19/30 [00:46<00:16,  1.54s/it] 67%|██████▋   | 20/30 [00:47<00:15,  1.52s/it] 70%|███████   | 21/30 [00:49<00:13,  1.50s/it] 73%|███████▎  | 22/30 [00:50<00:11,  1.49s/it] 77%|███████▋  | 23/30 [00:52<00:10,  1.48s/it] 80%|████████  | 24/30 [00:53<00:09,  1.55s/it] 83%|████████▎ | 25/30 [00:55<00:07,  1.53s/it] 87%|████████▋ | 26/30 [00:57<00:06,  1.60s/it] 90%|█████████ | 27/30 [00:58<00:04,  1.59s/it] 93%|█████████▎| 28/30 [01:00<00:03,  1.55s/it] 97%|█████████▋| 29/30 [01:01<00:01,  1.52s/it]100%|██████████| 30/30 [01:03<00:00,  1.52s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.52s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.52s/it]100%|██████████| 30/30 [01:03<00:00,  2.10s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7471, 'grad_norm': 0.12347446382045746, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7169, 'grad_norm': 0.15691813826560974, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 63.139, 'train_samples_per_second': 3.801, 'train_steps_per_second': 0.475, 'train_loss': 0.7320372263590494, 'epoch': 0.24}
END_PROFILE: 1747604200.8543336
Run  1: 65.202882814 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.92s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.21s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.92s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:51<00:25, 12.83s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.78s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.48s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.75s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:25, 37.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:09, 98.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 135.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 162.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 179.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 191.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 199.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 204.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 209.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 210.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 211.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 216.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:02, 216.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 217.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 217.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 217.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 216.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 217.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 218.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 218.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 219.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 217.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 219.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 218.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 217.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 218.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 218.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 216.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 217.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 216.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 217.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 217.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 212.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 209.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 236.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 239.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 203.63 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747604326.5050151
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:49, 20.33s/it]  7%|▋         | 2/30 [00:21<04:18,  9.24s/it] 10%|█         | 3/30 [00:23<02:33,  5.70s/it] 13%|█▎        | 4/30 [00:24<01:45,  4.07s/it] 17%|█▋        | 5/30 [00:26<01:21,  3.26s/it] 20%|██        | 6/30 [00:28<01:04,  2.69s/it] 23%|██▎       | 7/30 [00:29<00:52,  2.29s/it] 27%|██▋       | 8/30 [00:31<00:44,  2.04s/it] 30%|███       | 9/30 [00:33<00:41,  1.96s/it] 33%|███▎      | 10/30 [00:34<00:36,  1.81s/it] 37%|███▋      | 11/30 [00:35<00:32,  1.70s/it] 40%|████      | 12/30 [00:37<00:29,  1.63s/it] 43%|████▎     | 13/30 [00:38<00:26,  1.58s/it] 47%|████▋     | 14/30 [00:40<00:24,  1.54s/it] 50%|█████     | 15/30 [00:41<00:22,  1.52s/it]                                                50%|█████     | 15/30 [00:41<00:22,  1.52s/it] 53%|█████▎    | 16/30 [00:43<00:21,  1.51s/it] 57%|█████▋    | 17/30 [00:44<00:19,  1.48s/it] 60%|██████    | 18/30 [00:46<00:18,  1.56s/it] 63%|██████▎   | 19/30 [00:47<00:16,  1.52s/it] 67%|██████▋   | 20/30 [00:49<00:15,  1.50s/it] 70%|███████   | 21/30 [00:50<00:13,  1.49s/it] 73%|███████▎  | 22/30 [00:52<00:11,  1.47s/it] 77%|███████▋  | 23/30 [00:53<00:10,  1.47s/it] 80%|████████  | 24/30 [00:55<00:09,  1.54s/it] 83%|████████▎ | 25/30 [00:56<00:07,  1.51s/it] 87%|████████▋ | 26/30 [00:58<00:06,  1.58s/it] 90%|█████████ | 27/30 [01:00<00:04,  1.57s/it] 93%|█████████▎| 28/30 [01:01<00:03,  1.54s/it] 97%|█████████▋| 29/30 [01:03<00:01,  1.51s/it]100%|██████████| 30/30 [01:04<00:00,  1.51s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.51s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.51s/it]100%|██████████| 30/30 [01:04<00:00,  2.15s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7473, 'grad_norm': 0.12261445820331573, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7168, 'grad_norm': 0.1525590568780899, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 64.5659, 'train_samples_per_second': 3.717, 'train_steps_per_second': 0.465, 'train_loss': 0.7320386250813802, 'epoch': 0.24}
END_PROFILE: 1747604391.5548294
Run  2: 66.575941394 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.99s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:51, 12.97s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:38<00:38, 12.78s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:51<00:25, 12.74s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.71s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.41s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.66s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 36.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:10, 94.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:07, 132.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 159.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 175.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 188.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 195.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 200.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 205.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 209.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 210.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 211.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 211.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 213.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 215.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 215.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 216.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 216.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 215.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 216.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 215.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 215.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 213.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 214.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 213.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 214.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 215.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 213.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:04<00:00, 214.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 216.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 215.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 215.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 211.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 207.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 233.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 237.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 200.83 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747604515.4512954
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:32, 19.73s/it]  7%|▋         | 2/30 [00:21<04:12,  9.02s/it] 10%|█         | 3/30 [00:22<02:31,  5.60s/it] 13%|█▎        | 4/30 [00:24<01:44,  4.02s/it] 17%|█▋        | 5/30 [00:26<01:20,  3.23s/it] 20%|██        | 6/30 [00:27<01:04,  2.68s/it] 23%|██▎       | 7/30 [00:29<00:53,  2.30s/it] 27%|██▋       | 8/30 [00:30<00:45,  2.06s/it] 30%|███       | 9/30 [00:32<00:41,  1.98s/it] 33%|███▎      | 10/30 [00:34<00:36,  1.84s/it] 37%|███▋      | 11/30 [00:35<00:32,  1.73s/it] 40%|████      | 12/30 [00:37<00:29,  1.66s/it] 43%|████▎     | 13/30 [00:38<00:27,  1.62s/it] 47%|████▋     | 14/30 [00:40<00:25,  1.58s/it] 50%|█████     | 15/30 [00:41<00:23,  1.56s/it]                                                50%|█████     | 15/30 [00:41<00:23,  1.56s/it] 53%|█████▎    | 16/30 [00:43<00:21,  1.54s/it] 57%|█████▋    | 17/30 [00:44<00:19,  1.52s/it] 60%|██████    | 18/30 [00:46<00:19,  1.59s/it] 63%|██████▎   | 19/30 [00:47<00:17,  1.56s/it] 67%|██████▋   | 20/30 [00:49<00:15,  1.54s/it] 70%|███████   | 21/30 [00:50<00:13,  1.52s/it] 73%|███████▎  | 22/30 [00:52<00:12,  1.51s/it] 77%|███████▋  | 23/30 [00:53<00:10,  1.50s/it] 80%|████████  | 24/30 [00:55<00:09,  1.57s/it] 83%|████████▎ | 25/30 [00:57<00:07,  1.55s/it] 87%|████████▋ | 26/30 [00:58<00:06,  1.62s/it] 90%|█████████ | 27/30 [01:00<00:04,  1.61s/it] 93%|█████████▎| 28/30 [01:02<00:03,  1.58s/it] 97%|█████████▋| 29/30 [01:03<00:01,  1.55s/it]100%|██████████| 30/30 [01:05<00:00,  1.54s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.54s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.54s/it]100%|██████████| 30/30 [01:05<00:00,  2.17s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7472, 'grad_norm': 0.12602633237838745, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7172, 'grad_norm': 0.15523579716682434, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 65.0251, 'train_samples_per_second': 3.691, 'train_steps_per_second': 0.461, 'train_loss': 0.7321858723958333, 'epoch': 0.24}
END_PROFILE: 1747604580.9609857
Run  3: 67.024773542 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 65.202882814 seconds
  Max elapsed : 67.024773542 seconds
  Avg elapsed : 66.26786591666666666666 seconds
===================================
NSYS

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.98s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.95s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.86s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.78s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:10, 95.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 134.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 160.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 177.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 189.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 197.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 202.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 206.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 209.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 211.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 212.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 214.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 214.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 216.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 215.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 215.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 216.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 216.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 216.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 216.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 216.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 216.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 215.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 215.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 216.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 216.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 216.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:04<00:00, 215.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 216.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 216.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 215.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 211.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 209.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 229.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 236.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 201.56 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747604726.7787478
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:47, 18.18s/it]  7%|▋         | 2/30 [00:20<04:03,  8.68s/it] 10%|█         | 3/30 [00:22<02:32,  5.66s/it] 13%|█▎        | 4/30 [00:24<01:50,  4.27s/it] 17%|█▋        | 5/30 [00:26<01:28,  3.54s/it] 20%|██        | 6/30 [00:28<01:12,  3.03s/it] 23%|██▎       | 7/30 [00:30<01:02,  2.71s/it] 27%|██▋       | 8/30 [00:32<00:55,  2.50s/it] 30%|███       | 9/30 [00:35<00:50,  2.42s/it] 33%|███▎      | 10/30 [00:37<00:46,  2.31s/it] 37%|███▋      | 11/30 [00:39<00:42,  2.22s/it] 40%|████      | 12/30 [00:41<00:38,  2.17s/it] 43%|████▎     | 13/30 [00:43<00:36,  2.13s/it] 47%|████▋     | 14/30 [00:45<00:33,  2.09s/it] 50%|█████     | 15/30 [00:47<00:31,  2.07s/it]                                                50%|█████     | 15/30 [00:47<00:31,  2.07s/it] 53%|█████▎    | 16/30 [00:49<00:28,  2.07s/it] 57%|█████▋    | 17/30 [00:51<00:26,  2.05s/it] 60%|██████    | 18/30 [00:53<00:25,  2.09s/it] 63%|██████▎   | 19/30 [00:55<00:22,  2.07s/it] 67%|██████▋   | 20/30 [00:57<00:20,  2.06s/it] 70%|███████   | 21/30 [00:59<00:18,  2.05s/it] 73%|███████▎  | 22/30 [01:01<00:16,  2.04s/it] 77%|███████▋  | 23/30 [01:03<00:14,  2.03s/it] 80%|████████  | 24/30 [01:05<00:12,  2.08s/it] 83%|████████▎ | 25/30 [01:07<00:10,  2.07s/it] 87%|████████▋ | 26/30 [01:10<00:08,  2.12s/it] 90%|█████████ | 27/30 [01:12<00:06,  2.09s/it] 93%|█████████▎| 28/30 [01:14<00:04,  2.07s/it] 97%|█████████▋| 29/30 [01:16<00:02,  2.06s/it]100%|██████████| 30/30 [01:18<00:00,  2.05s/it]                                               100%|██████████| 30/30 [01:18<00:00,  2.05s/it]                                               100%|██████████| 30/30 [01:18<00:00,  2.05s/it]100%|██████████| 30/30 [01:18<00:00,  2.61s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7472, 'grad_norm': 0.12861131131649017, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7172, 'grad_norm': 0.15394259989261627, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 78.1842, 'train_samples_per_second': 3.07, 'train_steps_per_second': 0.384, 'train_loss': 0.7321938196818034, 'epoch': 0.24}
END_PROFILE: 1747604805.4535282
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/phi-4.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-11c0.qdstrm'
[1/1] [0%                          ] nsys-report-28a1.nsys-rep[1/1] [0%                          ] nsys-report-28a1.nsys-rep[1/1] [0%                          ] nsys-report-28a1.nsys-rep[1/1] [7%                          ] nsys-report-28a1.nsys-rep[1/1] [6%                          ] nsys-report-28a1.nsys-rep[1/1] [7%                          ] nsys-report-28a1.nsys-rep[1/1] [6%                          ] nsys-report-28a1.nsys-rep[1/1] [7%                          ] nsys-report-28a1.nsys-rep[1/1] [6%                          ] nsys-report-28a1.nsys-rep[1/1] [7%                          ] nsys-report-28a1.nsys-rep[1/1] [6%                          ] nsys-report-28a1.nsys-rep[1/1] [8%                          ] nsys-report-28a1.nsys-rep[1/1] [7%                          ] nsys-report-28a1.nsys-rep[1/1] [6%                          ] nsys-report-28a1.nsys-rep[1/1] [5%                          ] nsys-report-28a1.nsys-rep[1/1] [5%                          ] nsys-report-28a1.nsys-rep[1/1] [6%                          ] nsys-report-28a1.nsys-rep[1/1] [7%                          ] nsys-report-28a1.nsys-rep[1/1] [8%                          ] nsys-report-28a1.nsys-rep[1/1] [9%                          ] nsys-report-28a1.nsys-rep[1/1] [10%                         ] nsys-report-28a1.nsys-rep[1/1] [11%                         ] nsys-report-28a1.nsys-rep[1/1] [12%                         ] nsys-report-28a1.nsys-rep[1/1] [13%                         ] nsys-report-28a1.nsys-rep[1/1] [14%                         ] nsys-report-28a1.nsys-rep[1/1] [=15%                        ] nsys-report-28a1.nsys-rep[1/1] [=16%                        ] nsys-report-28a1.nsys-rep[1/1] [=17%                        ] nsys-report-28a1.nsys-rep[1/1] [==18%                       ] nsys-report-28a1.nsys-rep[1/1] [==19%                       ] nsys-report-28a1.nsys-rep[1/1] [==20%                       ] nsys-report-28a1.nsys-rep[1/1] [==21%                       ] nsys-report-28a1.nsys-rep[1/1] [===22%                      ] nsys-report-28a1.nsys-rep[1/1] [===23%                      ] nsys-report-28a1.nsys-rep[1/1] [===24%                      ] nsys-report-28a1.nsys-rep[1/1] [====25%                     ] nsys-report-28a1.nsys-rep[1/1] [====26%                     ] nsys-report-28a1.nsys-rep[1/1] [====27%                     ] nsys-report-28a1.nsys-rep[1/1] [====28%                     ] nsys-report-28a1.nsys-rep[1/1] [=====29%                    ] nsys-report-28a1.nsys-rep[1/1] [=====30%                    ] nsys-report-28a1.nsys-rep[1/1] [=====31%                    ] nsys-report-28a1.nsys-rep[1/1] [=====32%                    ] nsys-report-28a1.nsys-rep[1/1] [======33%                   ] nsys-report-28a1.nsys-rep[1/1] [======34%                   ] nsys-report-28a1.nsys-rep[1/1] [======35%                   ] nsys-report-28a1.nsys-rep[1/1] [=======36%                  ] nsys-report-28a1.nsys-rep[1/1] [=======37%                  ] nsys-report-28a1.nsys-rep[1/1] [=======38%                  ] nsys-report-28a1.nsys-rep[1/1] [=======39%                  ] nsys-report-28a1.nsys-rep[1/1] [========40%                 ] nsys-report-28a1.nsys-rep[1/1] [========41%                 ] nsys-report-28a1.nsys-rep[1/1] [========42%                 ] nsys-report-28a1.nsys-rep[1/1] [=========43%                ] nsys-report-28a1.nsys-rep[1/1] [=========44%                ] nsys-report-28a1.nsys-rep[1/1] [=========45%                ] nsys-report-28a1.nsys-rep[1/1] [=========46%                ] nsys-report-28a1.nsys-rep[1/1] [==========47%               ] nsys-report-28a1.nsys-rep[1/1] [==========48%               ] nsys-report-28a1.nsys-rep[1/1] [==========49%               ] nsys-report-28a1.nsys-rep[1/1] [===========50%              ] nsys-report-28a1.nsys-rep[1/1] [===========51%              ] nsys-report-28a1.nsys-rep[1/1] [===========52%              ] nsys-report-28a1.nsys-rep[1/1] [===========53%              ] nsys-report-28a1.nsys-rep[1/1] [============54%             ] nsys-report-28a1.nsys-rep[1/1] [============55%             ] nsys-report-28a1.nsys-rep[1/1] [============56%             ] nsys-report-28a1.nsys-rep[1/1] [============57%             ] nsys-report-28a1.nsys-rep[1/1] [=============58%            ] nsys-report-28a1.nsys-rep[1/1] [=============59%            ] nsys-report-28a1.nsys-rep[1/1] [=============60%            ] nsys-report-28a1.nsys-rep[1/1] [==============61%           ] nsys-report-28a1.nsys-rep[1/1] [==============62%           ] nsys-report-28a1.nsys-rep[1/1] [==============63%           ] nsys-report-28a1.nsys-rep[1/1] [==============64%           ] nsys-report-28a1.nsys-rep[1/1] [===============65%          ] nsys-report-28a1.nsys-rep[1/1] [===============66%          ] nsys-report-28a1.nsys-rep[1/1] [===============67%          ] nsys-report-28a1.nsys-rep[1/1] [================68%         ] nsys-report-28a1.nsys-rep[1/1] [================69%         ] nsys-report-28a1.nsys-rep[1/1] [================70%         ] nsys-report-28a1.nsys-rep[1/1] [================71%         ] nsys-report-28a1.nsys-rep[1/1] [=================72%        ] nsys-report-28a1.nsys-rep[1/1] [=================73%        ] nsys-report-28a1.nsys-rep[1/1] [=================74%        ] nsys-report-28a1.nsys-rep[1/1] [==================75%       ] nsys-report-28a1.nsys-rep[1/1] [==================76%       ] nsys-report-28a1.nsys-rep[1/1] [==================77%       ] nsys-report-28a1.nsys-rep[1/1] [==================78%       ] nsys-report-28a1.nsys-rep[1/1] [===================79%      ] nsys-report-28a1.nsys-rep[1/1] [===================80%      ] nsys-report-28a1.nsys-rep[1/1] [===================81%      ] nsys-report-28a1.nsys-rep[1/1] [========================100%] nsys-report-28a1.nsys-rep[1/1] [========================100%] nsys-report-28a1.nsys-rep
Generated:
	/tmp/nsys-report-28a1.nsys-rep
Run  1: 93.208198455 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.97s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.94s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.86s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.81s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.48s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.77s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:09, 96.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 135.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 161.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 180.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 192.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 200.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 205.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 210.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 211.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 214.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 216.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 216.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 217.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 217.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 217.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 217.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 219.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 220.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 218.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 219.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 218.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 220.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 219.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 217.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 218.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 219.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 220.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 221.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 219.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 220.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 219.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 215.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 212.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 237.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 240.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 204.41 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747604941.6514642
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:57, 18.52s/it]  7%|▋         | 2/30 [00:20<04:05,  8.79s/it] 10%|█         | 3/30 [00:22<02:33,  5.69s/it] 13%|█▎        | 4/30 [00:24<01:50,  4.25s/it] 17%|█▋        | 5/30 [00:26<01:27,  3.49s/it] 20%|██        | 6/30 [00:28<01:11,  2.98s/it] 23%|██▎       | 7/30 [00:30<01:01,  2.67s/it] 27%|██▋       | 8/30 [00:32<00:53,  2.45s/it] 30%|███       | 9/30 [00:34<00:49,  2.37s/it] 33%|███▎      | 10/30 [00:36<00:45,  2.25s/it] 37%|███▋      | 11/30 [00:38<00:41,  2.16s/it] 40%|████      | 12/30 [00:40<00:37,  2.10s/it] 43%|████▎     | 13/30 [00:42<00:35,  2.06s/it] 47%|████▋     | 14/30 [00:44<00:32,  2.03s/it] 50%|█████     | 15/30 [00:46<00:30,  2.01s/it]                                                50%|█████     | 15/30 [00:46<00:30,  2.01s/it] 53%|█████▎    | 16/30 [00:48<00:27,  2.00s/it] 57%|█████▋    | 17/30 [00:50<00:25,  1.98s/it] 60%|██████    | 18/30 [00:52<00:24,  2.02s/it] 63%|██████▎   | 19/30 [00:54<00:22,  2.00s/it] 67%|██████▋   | 20/30 [00:56<00:19,  1.99s/it] 70%|███████   | 21/30 [00:58<00:17,  1.99s/it] 73%|███████▎  | 22/30 [01:00<00:15,  1.98s/it] 77%|███████▋  | 23/30 [01:02<00:13,  1.97s/it] 80%|████████  | 24/30 [01:04<00:12,  2.02s/it] 83%|████████▎ | 25/30 [01:06<00:10,  2.01s/it] 87%|████████▋ | 26/30 [01:08<00:08,  2.06s/it] 90%|█████████ | 27/30 [01:10<00:06,  2.04s/it] 93%|█████████▎| 28/30 [01:12<00:04,  2.02s/it] 97%|█████████▋| 29/30 [01:14<00:02,  2.00s/it]100%|██████████| 30/30 [01:16<00:00,  1.99s/it]                                               100%|██████████| 30/30 [01:16<00:00,  1.99s/it]                                               100%|██████████| 30/30 [01:16<00:00,  1.99s/it]100%|██████████| 30/30 [01:16<00:00,  2.56s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7474, 'grad_norm': 0.12588217854499817, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7173, 'grad_norm': 0.15491394698619843, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 76.726, 'train_samples_per_second': 3.128, 'train_steps_per_second': 0.391, 'train_loss': 0.7323397954305013, 'epoch': 0.24}
END_PROFILE: 1747605018.8779876
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/phi-4.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-9149.qdstrm'
[1/1] [0%                          ] nsys-report-e695.nsys-rep[1/1] [0%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [8%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [5%                          ] nsys-report-e695.nsys-rep[1/1] [5%                          ] nsys-report-e695.nsys-rep[1/1] [6%                          ] nsys-report-e695.nsys-rep[1/1] [7%                          ] nsys-report-e695.nsys-rep[1/1] [8%                          ] nsys-report-e695.nsys-rep[1/1] [9%                          ] nsys-report-e695.nsys-rep[1/1] [10%                         ] nsys-report-e695.nsys-rep[1/1] [11%                         ] nsys-report-e695.nsys-rep[1/1] [12%                         ] nsys-report-e695.nsys-rep[1/1] [13%                         ] nsys-report-e695.nsys-rep[1/1] [14%                         ] nsys-report-e695.nsys-rep[1/1] [=15%                        ] nsys-report-e695.nsys-rep[1/1] [=16%                        ] nsys-report-e695.nsys-rep[1/1] [=17%                        ] nsys-report-e695.nsys-rep[1/1] [==18%                       ] nsys-report-e695.nsys-rep[1/1] [==19%                       ] nsys-report-e695.nsys-rep[1/1] [==20%                       ] nsys-report-e695.nsys-rep[1/1] [==21%                       ] nsys-report-e695.nsys-rep[1/1] [===22%                      ] nsys-report-e695.nsys-rep[1/1] [===23%                      ] nsys-report-e695.nsys-rep[1/1] [===24%                      ] nsys-report-e695.nsys-rep[1/1] [====25%                     ] nsys-report-e695.nsys-rep[1/1] [====26%                     ] nsys-report-e695.nsys-rep[1/1] [====27%                     ] nsys-report-e695.nsys-rep[1/1] [====28%                     ] nsys-report-e695.nsys-rep[1/1] [=====29%                    ] nsys-report-e695.nsys-rep[1/1] [=====30%                    ] nsys-report-e695.nsys-rep[1/1] [=====31%                    ] nsys-report-e695.nsys-rep[1/1] [=====32%                    ] nsys-report-e695.nsys-rep[1/1] [======33%                   ] nsys-report-e695.nsys-rep[1/1] [======34%                   ] nsys-report-e695.nsys-rep[1/1] [======35%                   ] nsys-report-e695.nsys-rep[1/1] [=======36%                  ] nsys-report-e695.nsys-rep[1/1] [=======37%                  ] nsys-report-e695.nsys-rep[1/1] [=======38%                  ] nsys-report-e695.nsys-rep[1/1] [=======39%                  ] nsys-report-e695.nsys-rep[1/1] [========40%                 ] nsys-report-e695.nsys-rep[1/1] [========41%                 ] nsys-report-e695.nsys-rep[1/1] [========42%                 ] nsys-report-e695.nsys-rep[1/1] [=========43%                ] nsys-report-e695.nsys-rep[1/1] [=========44%                ] nsys-report-e695.nsys-rep[1/1] [=========45%                ] nsys-report-e695.nsys-rep[1/1] [=========46%                ] nsys-report-e695.nsys-rep[1/1] [==========47%               ] nsys-report-e695.nsys-rep[1/1] [==========48%               ] nsys-report-e695.nsys-rep[1/1] [==========49%               ] nsys-report-e695.nsys-rep[1/1] [===========50%              ] nsys-report-e695.nsys-rep[1/1] [===========51%              ] nsys-report-e695.nsys-rep[1/1] [===========52%              ] nsys-report-e695.nsys-rep[1/1] [===========53%              ] nsys-report-e695.nsys-rep[1/1] [============54%             ] nsys-report-e695.nsys-rep[1/1] [============55%             ] nsys-report-e695.nsys-rep[1/1] [============56%             ] nsys-report-e695.nsys-rep[1/1] [============57%             ] nsys-report-e695.nsys-rep[1/1] [=============58%            ] nsys-report-e695.nsys-rep[1/1] [=============59%            ] nsys-report-e695.nsys-rep[1/1] [=============60%            ] nsys-report-e695.nsys-rep[1/1] [==============61%           ] nsys-report-e695.nsys-rep[1/1] [==============62%           ] nsys-report-e695.nsys-rep[1/1] [==============63%           ] nsys-report-e695.nsys-rep[1/1] [==============64%           ] nsys-report-e695.nsys-rep[1/1] [===============65%          ] nsys-report-e695.nsys-rep[1/1] [===============66%          ] nsys-report-e695.nsys-rep[1/1] [===============67%          ] nsys-report-e695.nsys-rep[1/1] [================68%         ] nsys-report-e695.nsys-rep[1/1] [================69%         ] nsys-report-e695.nsys-rep[1/1] [================70%         ] nsys-report-e695.nsys-rep[1/1] [================71%         ] nsys-report-e695.nsys-rep[1/1] [=================72%        ] nsys-report-e695.nsys-rep[1/1] [=================73%        ] nsys-report-e695.nsys-rep[1/1] [=================74%        ] nsys-report-e695.nsys-rep[1/1] [==================75%       ] nsys-report-e695.nsys-rep[1/1] [==================76%       ] nsys-report-e695.nsys-rep[1/1] [==================77%       ] nsys-report-e695.nsys-rep[1/1] [==================78%       ] nsys-report-e695.nsys-rep[1/1] [===================79%      ] nsys-report-e695.nsys-rep[1/1] [===================80%      ] nsys-report-e695.nsys-rep[1/1] [===================81%      ] nsys-report-e695.nsys-rep[1/1] [===================82%      ] nsys-report-e695.nsys-rep[1/1] [========================100%] nsys-report-e695.nsys-rep[1/1] [========================100%] nsys-report-e695.nsys-rep
Generated:
	/tmp/nsys-report-e695.nsys-rep
Run  2: 91.756923161 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.95s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.94s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:51<00:25, 12.85s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.80s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.77s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:09, 97.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 136.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 161.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 178.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 191.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 197.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 202.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 206.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 210.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 212.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 213.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 215.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 216.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 216.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 216.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 217.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 218.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 218.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 220.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 219.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 220.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 219.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 219.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 219.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 219.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 220.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 218.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 216.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 216.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 216.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 215.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 213.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 211.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 234.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 203.66 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747605156.9326599
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:13, 19.07s/it]  7%|▋         | 2/30 [00:21<04:12,  9.00s/it] 10%|█         | 3/30 [00:22<02:36,  5.79s/it] 13%|█▎        | 4/30 [00:25<01:52,  4.32s/it] 17%|█▋        | 5/30 [00:27<01:28,  3.53s/it] 20%|██        | 6/30 [00:29<01:11,  3.00s/it] 23%|██▎       | 7/30 [00:31<01:01,  2.66s/it] 27%|██▋       | 8/30 [00:33<00:53,  2.43s/it] 30%|███       | 9/30 [00:35<00:49,  2.35s/it] 33%|███▎      | 10/30 [00:37<00:44,  2.23s/it] 37%|███▋      | 11/30 [00:39<00:40,  2.14s/it] 40%|████      | 12/30 [00:41<00:37,  2.08s/it] 43%|████▎     | 13/30 [00:43<00:34,  2.04s/it] 47%|████▋     | 14/30 [00:44<00:32,  2.00s/it] 50%|█████     | 15/30 [00:46<00:29,  1.99s/it]                                                50%|█████     | 15/30 [00:46<00:29,  1.99s/it] 53%|█████▎    | 16/30 [00:48<00:27,  1.98s/it] 57%|█████▋    | 17/30 [00:50<00:25,  1.95s/it] 60%|██████    | 18/30 [00:52<00:23,  2.00s/it] 63%|██████▎   | 19/30 [00:54<00:21,  1.98s/it] 67%|██████▋   | 20/30 [00:56<00:19,  1.97s/it] 70%|███████   | 21/30 [00:58<00:17,  1.95s/it] 73%|███████▎  | 22/30 [01:00<00:15,  1.94s/it] 77%|███████▋  | 23/30 [01:02<00:13,  1.94s/it] 80%|████████  | 24/30 [01:04<00:11,  1.99s/it] 83%|████████▎ | 25/30 [01:06<00:09,  1.99s/it] 87%|████████▋ | 26/30 [01:08<00:08,  2.04s/it] 90%|█████████ | 27/30 [01:10<00:06,  2.02s/it] 93%|█████████▎| 28/30 [01:12<00:03,  1.99s/it] 97%|█████████▋| 29/30 [01:14<00:01,  1.97s/it]100%|██████████| 30/30 [01:16<00:00,  1.96s/it]                                               100%|██████████| 30/30 [01:16<00:00,  1.96s/it]                                               100%|██████████| 30/30 [01:16<00:00,  1.96s/it]100%|██████████| 30/30 [01:16<00:00,  2.55s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7472, 'grad_norm': 0.13029293715953827, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.717, 'grad_norm': 0.15429985523223877, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 76.4618, 'train_samples_per_second': 3.139, 'train_steps_per_second': 0.392, 'train_loss': 0.7321255366007487, 'epoch': 0.24}
END_PROFILE: 1747605233.8780859
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/phi-4.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-07a0.qdstrm'
[1/1] [0%                          ] nsys-report-8ad6.nsys-rep[1/1] [0%                          ] nsys-report-8ad6.nsys-rep[1/1] [0%                          ] nsys-report-8ad6.nsys-rep[1/1] [7%                          ] nsys-report-8ad6.nsys-rep[1/1] [6%                          ] nsys-report-8ad6.nsys-rep[1/1] [8%                          ] nsys-report-8ad6.nsys-rep[1/1] [7%                          ] nsys-report-8ad6.nsys-rep[1/1] [6%                          ] nsys-report-8ad6.nsys-rep[1/1] [5%                          ] nsys-report-8ad6.nsys-rep[1/1] [5%                          ] nsys-report-8ad6.nsys-rep[1/1] [6%                          ] nsys-report-8ad6.nsys-rep[1/1] [7%                          ] nsys-report-8ad6.nsys-rep[1/1] [8%                          ] nsys-report-8ad6.nsys-rep[1/1] [9%                          ] nsys-report-8ad6.nsys-rep[1/1] [10%                         ] nsys-report-8ad6.nsys-rep[1/1] [11%                         ] nsys-report-8ad6.nsys-rep[1/1] [12%                         ] nsys-report-8ad6.nsys-rep[1/1] [13%                         ] nsys-report-8ad6.nsys-rep[1/1] [14%                         ] nsys-report-8ad6.nsys-rep[1/1] [=15%                        ] nsys-report-8ad6.nsys-rep[1/1] [=16%                        ] nsys-report-8ad6.nsys-rep[1/1] [=17%                        ] nsys-report-8ad6.nsys-rep[1/1] [==18%                       ] nsys-report-8ad6.nsys-rep[1/1] [==19%                       ] nsys-report-8ad6.nsys-rep[1/1] [==20%                       ] nsys-report-8ad6.nsys-rep[1/1] [==21%                       ] nsys-report-8ad6.nsys-rep[1/1] [===22%                      ] nsys-report-8ad6.nsys-rep[1/1] [===23%                      ] nsys-report-8ad6.nsys-rep[1/1] [===24%                      ] nsys-report-8ad6.nsys-rep[1/1] [====25%                     ] nsys-report-8ad6.nsys-rep[1/1] [====26%                     ] nsys-report-8ad6.nsys-rep[1/1] [====27%                     ] nsys-report-8ad6.nsys-rep[1/1] [====28%                     ] nsys-report-8ad6.nsys-rep[1/1] [=====29%                    ] nsys-report-8ad6.nsys-rep[1/1] [=====30%                    ] nsys-report-8ad6.nsys-rep[1/1] [=====31%                    ] nsys-report-8ad6.nsys-rep[1/1] [=====32%                    ] nsys-report-8ad6.nsys-rep[1/1] [======33%                   ] nsys-report-8ad6.nsys-rep[1/1] [======34%                   ] nsys-report-8ad6.nsys-rep[1/1] [======35%                   ] nsys-report-8ad6.nsys-rep[1/1] [=======36%                  ] nsys-report-8ad6.nsys-rep[1/1] [=======37%                  ] nsys-report-8ad6.nsys-rep[1/1] [=======38%                  ] nsys-report-8ad6.nsys-rep[1/1] [=======39%                  ] nsys-report-8ad6.nsys-rep[1/1] [========40%                 ] nsys-report-8ad6.nsys-rep[1/1] [========41%                 ] nsys-report-8ad6.nsys-rep[1/1] [========42%                 ] nsys-report-8ad6.nsys-rep[1/1] [=========43%                ] nsys-report-8ad6.nsys-rep[1/1] [=========44%                ] nsys-report-8ad6.nsys-rep[1/1] [=========45%                ] nsys-report-8ad6.nsys-rep[1/1] [=========46%                ] nsys-report-8ad6.nsys-rep[1/1] [==========47%               ] nsys-report-8ad6.nsys-rep[1/1] [==========48%               ] nsys-report-8ad6.nsys-rep[1/1] [==========49%               ] nsys-report-8ad6.nsys-rep[1/1] [===========50%              ] nsys-report-8ad6.nsys-rep[1/1] [===========51%              ] nsys-report-8ad6.nsys-rep[1/1] [===========52%              ] nsys-report-8ad6.nsys-rep[1/1] [===========53%              ] nsys-report-8ad6.nsys-rep[1/1] [============54%             ] nsys-report-8ad6.nsys-rep[1/1] [============55%             ] nsys-report-8ad6.nsys-rep[1/1] [============56%             ] nsys-report-8ad6.nsys-rep[1/1] [============57%             ] nsys-report-8ad6.nsys-rep[1/1] [=============58%            ] nsys-report-8ad6.nsys-rep[1/1] [=============59%            ] nsys-report-8ad6.nsys-rep[1/1] [=============60%            ] nsys-report-8ad6.nsys-rep[1/1] [==============61%           ] nsys-report-8ad6.nsys-rep[1/1] [==============62%           ] nsys-report-8ad6.nsys-rep[1/1] [==============63%           ] nsys-report-8ad6.nsys-rep[1/1] [==============64%           ] nsys-report-8ad6.nsys-rep[1/1] [===============65%          ] nsys-report-8ad6.nsys-rep[1/1] [===============66%          ] nsys-report-8ad6.nsys-rep[1/1] [===============67%          ] nsys-report-8ad6.nsys-rep[1/1] [================68%         ] nsys-report-8ad6.nsys-rep[1/1] [================69%         ] nsys-report-8ad6.nsys-rep[1/1] [================70%         ] nsys-report-8ad6.nsys-rep[1/1] [================71%         ] nsys-report-8ad6.nsys-rep[1/1] [=================72%        ] nsys-report-8ad6.nsys-rep[1/1] [=================73%        ] nsys-report-8ad6.nsys-rep[1/1] [=================74%        ] nsys-report-8ad6.nsys-rep[1/1] [==================75%       ] nsys-report-8ad6.nsys-rep[1/1] [==================76%       ] nsys-report-8ad6.nsys-rep[1/1] [==================77%       ] nsys-report-8ad6.nsys-rep[1/1] [==================78%       ] nsys-report-8ad6.nsys-rep[1/1] [===================79%      ] nsys-report-8ad6.nsys-rep[1/1] [===================80%      ] nsys-report-8ad6.nsys-rep[1/1] [===================81%      ] nsys-report-8ad6.nsys-rep[1/1] [===================82%      ] nsys-report-8ad6.nsys-rep[1/1] [========================100%] nsys-report-8ad6.nsys-rep[1/1] [========================100%] nsys-report-8ad6.nsys-rep
Generated:
	/tmp/nsys-report-8ad6.nsys-rep
Run  3: 91.534892343 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 91.534892343 seconds
  Max elapsed : 93.208198455 seconds
  Avg elapsed : 92.16667131966666666666 seconds
===================================
--------------------------------------------
PROTON

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:12<01:03, 12.75s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:25<00:50, 12.73s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:38<00:38, 12.68s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:50<00:25, 12.70s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:03<00:12, 12.71s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.43s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.57s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:10, 95.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 134.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 160.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 178.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 190.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 198.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 203.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 208.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 212.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 213.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 214.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 215.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 217.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 217.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 216.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 218.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 217.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 217.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 218.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 219.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 219.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 217.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 218.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 219.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 218.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 217.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 218.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 219.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 219.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 219.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 217.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 214.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 211.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 234.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 203.63 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747605397.1446452
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<09:01, 18.68s/it]  7%|▋         | 2/30 [00:20<04:06,  8.81s/it] 10%|█         | 3/30 [00:22<02:33,  5.67s/it] 13%|█▎        | 4/30 [00:24<01:49,  4.22s/it] 17%|█▋        | 5/30 [00:26<01:26,  3.47s/it] 20%|██        | 6/30 [00:28<01:10,  2.93s/it] 23%|██▎       | 7/30 [00:30<00:59,  2.59s/it] 27%|██▋       | 8/30 [00:32<00:52,  2.38s/it] 30%|███       | 9/30 [00:34<00:48,  2.31s/it] 33%|███▎      | 10/30 [00:36<00:45,  2.26s/it] 37%|███▋      | 11/30 [00:38<00:41,  2.16s/it] 40%|████      | 12/30 [00:40<00:37,  2.11s/it] 43%|████▎     | 13/30 [00:42<00:34,  2.04s/it] 47%|████▋     | 14/30 [00:44<00:31,  2.00s/it] 50%|█████     | 15/30 [00:46<00:29,  1.97s/it]                                                50%|█████     | 15/30 [00:46<00:29,  1.97s/it] 53%|█████▎    | 16/30 [00:48<00:27,  1.94s/it] 57%|█████▋    | 17/30 [00:50<00:24,  1.92s/it] 60%|██████    | 18/30 [00:52<00:23,  1.96s/it] 63%|██████▎   | 19/30 [00:54<00:21,  2.00s/it] 67%|██████▋   | 20/30 [00:56<00:20,  2.01s/it] 70%|███████   | 21/30 [00:58<00:17,  1.99s/it] 73%|███████▎  | 22/30 [01:00<00:15,  1.97s/it] 77%|███████▋  | 23/30 [01:01<00:13,  1.96s/it] 80%|████████  | 24/30 [01:04<00:12,  2.04s/it] 83%|████████▎ | 25/30 [01:06<00:09,  1.99s/it] 87%|████████▋ | 26/30 [01:08<00:08,  2.02s/it] 90%|█████████ | 27/30 [01:10<00:05,  1.99s/it] 93%|█████████▎| 28/30 [01:11<00:03,  1.95s/it] 97%|█████████▋| 29/30 [01:14<00:02,  2.02s/it]100%|██████████| 30/30 [01:16<00:00,  2.01s/it]                                               100%|██████████| 30/30 [01:16<00:00,  2.01s/it]                                               100%|██████████| 30/30 [01:16<00:00,  2.01s/it]100%|██████████| 30/30 [01:16<00:00,  2.54s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7473, 'grad_norm': 0.12412123382091522, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7171, 'grad_norm': 0.15311117470264435, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 76.1348, 'train_samples_per_second': 3.152, 'train_steps_per_second': 0.394, 'train_loss': 0.7322282791137695, 'epoch': 0.24}
END_PROFILE: 1747605474.0014572
Run  1: 79.736965384 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:12<01:03, 12.79s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:25<00:51, 12.77s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:38<00:38, 12.70s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:50<00:25, 12.53s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:03<00:12, 12.60s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.35s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.50s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:27, 36.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:10, 95.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 133.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 159.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 176.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 191.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 197.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 201.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 204.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 207.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 209.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 210.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 211.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 212.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 214.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 213.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 215.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 215.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 213.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 213.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 213.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 214.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 214.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 215.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 214.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 214.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 213.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 214.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:04<00:00, 214.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 213.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 213.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 211.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 210.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 207.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 231.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 237.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 200.17 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747605597.3797126
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<10:08, 20.99s/it]  7%|▋         | 2/30 [00:22<04:33,  9.75s/it] 10%|█         | 3/30 [00:24<02:46,  6.16s/it] 13%|█▎        | 4/30 [00:26<01:56,  4.50s/it] 17%|█▋        | 5/30 [00:28<01:30,  3.63s/it] 20%|██        | 6/30 [00:30<01:12,  3.03s/it] 23%|██▎       | 7/30 [00:32<01:00,  2.65s/it] 27%|██▋       | 8/30 [00:34<00:52,  2.40s/it] 30%|███       | 9/30 [00:36<00:48,  2.32s/it] 33%|███▎      | 10/30 [00:38<00:45,  2.26s/it] 37%|███▋      | 11/30 [00:40<00:40,  2.15s/it] 40%|████      | 12/30 [00:42<00:37,  2.09s/it] 43%|████▎     | 13/30 [00:44<00:34,  2.02s/it] 47%|████▋     | 14/30 [00:46<00:31,  1.97s/it] 50%|█████     | 15/30 [00:48<00:29,  1.94s/it]                                                50%|█████     | 15/30 [00:48<00:29,  1.94s/it] 53%|█████▎    | 16/30 [00:49<00:26,  1.91s/it] 57%|█████▋    | 17/30 [00:51<00:24,  1.88s/it] 60%|██████    | 18/30 [00:53<00:23,  1.93s/it] 63%|██████▎   | 19/30 [00:55<00:21,  1.97s/it] 67%|██████▋   | 20/30 [00:57<00:19,  1.98s/it] 70%|███████   | 21/30 [00:59<00:17,  1.96s/it] 73%|███████▎  | 22/30 [01:01<00:15,  1.94s/it] 77%|███████▋  | 23/30 [01:03<00:13,  1.93s/it] 80%|████████  | 24/30 [01:05<00:12,  2.00s/it] 83%|████████▎ | 25/30 [01:07<00:09,  1.96s/it] 87%|████████▋ | 26/30 [01:09<00:07,  1.99s/it] 90%|█████████ | 27/30 [01:11<00:05,  1.96s/it] 93%|█████████▎| 28/30 [01:13<00:03,  1.92s/it] 97%|█████████▋| 29/30 [01:15<00:01,  1.99s/it]100%|██████████| 30/30 [01:17<00:00,  1.98s/it]                                               100%|██████████| 30/30 [01:17<00:00,  1.98s/it]                                               100%|██████████| 30/30 [01:17<00:00,  1.98s/it]100%|██████████| 30/30 [01:17<00:00,  2.58s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7471, 'grad_norm': 0.12491089850664139, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7169, 'grad_norm': 0.15581493079662323, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 77.4927, 'train_samples_per_second': 3.097, 'train_steps_per_second': 0.387, 'train_loss': 0.7319697380065918, 'epoch': 0.24}
END_PROFILE: 1747605675.5734174
Run  2: 80.995403602 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:12<01:03, 12.69s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:25<00:50, 12.70s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:37<00:37, 12.64s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:50<00:25, 12.66s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:03<00:12, 12.67s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.40s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.54s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 36.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:10, 95.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 133.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 160.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 176.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 188.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 197.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 202.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 204.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 207.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 208.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 210.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 213.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 214.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 214.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 216.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 215.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 214.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 217.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 217.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 215.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 214.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 214.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 215.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 214.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 214.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 215.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 215.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:04<00:00, 215.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 215.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 215.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 217.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 211.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 207.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 233.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 236.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 200.93 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747605807.3142738
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<10:44, 22.23s/it]  7%|▋         | 2/30 [00:24<04:46, 10.24s/it] 10%|█         | 3/30 [00:25<02:53,  6.41s/it] 13%|█▎        | 4/30 [00:27<02:00,  4.63s/it] 17%|█▋        | 5/30 [00:29<01:32,  3.71s/it] 20%|██        | 6/30 [00:31<01:13,  3.07s/it] 23%|██▎       | 7/30 [00:33<01:01,  2.66s/it] 27%|██▋       | 8/30 [00:35<00:52,  2.40s/it] 30%|███       | 9/30 [00:37<00:48,  2.31s/it] 33%|███▎      | 10/30 [00:39<00:44,  2.24s/it] 37%|███▋      | 11/30 [00:41<00:40,  2.13s/it] 40%|████      | 12/30 [00:43<00:37,  2.06s/it] 43%|████▎     | 13/30 [00:45<00:33,  1.99s/it] 47%|████▋     | 14/30 [00:47<00:31,  1.94s/it] 50%|█████     | 15/30 [00:48<00:28,  1.90s/it]                                                50%|█████     | 15/30 [00:48<00:28,  1.90s/it] 53%|█████▎    | 16/30 [00:50<00:26,  1.88s/it] 57%|█████▋    | 17/30 [00:52<00:24,  1.85s/it] 60%|██████    | 18/30 [00:54<00:22,  1.90s/it] 63%|██████▎   | 19/30 [00:56<00:21,  1.94s/it] 67%|██████▋   | 20/30 [00:58<00:19,  1.94s/it] 70%|███████   | 21/30 [01:00<00:17,  1.93s/it] 73%|███████▎  | 22/30 [01:02<00:15,  1.91s/it] 77%|███████▋  | 23/30 [01:04<00:13,  1.89s/it] 80%|████████  | 24/30 [01:06<00:11,  1.97s/it] 83%|████████▎ | 25/30 [01:08<00:09,  1.92s/it] 87%|████████▋ | 26/30 [01:10<00:07,  1.96s/it] 90%|█████████ | 27/30 [01:11<00:05,  1.93s/it] 93%|█████████▎| 28/30 [01:13<00:03,  1.89s/it] 97%|█████████▋| 29/30 [01:15<00:01,  1.96s/it]100%|██████████| 30/30 [01:17<00:00,  1.95s/it]                                               100%|██████████| 30/30 [01:17<00:00,  1.95s/it]                                               100%|██████████| 30/30 [01:17<00:00,  1.95s/it]100%|██████████| 30/30 [01:17<00:00,  2.59s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.747, 'grad_norm': 0.12266912311315536, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7169, 'grad_norm': 0.15315808355808258, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 77.7697, 'train_samples_per_second': 3.086, 'train_steps_per_second': 0.386, 'train_loss': 0.7319753011067708, 'epoch': 0.24}
END_PROFILE: 1747605885.814477
Run  3: 80.084634836 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 79.736965384 seconds
  Max elapsed : 80.995403602 seconds
  Avg elapsed : 80.27233460733333333333 seconds
===================================
--------------------------------------------
TORCH

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.99s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.95s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.87s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.78s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:09, 96.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 135.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 159.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 177.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 188.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 195.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 200.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 204.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 207.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 210.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 212.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 213.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 216.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 215.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 216.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 216.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 216.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 217.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 216.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 218.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 217.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 218.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 218.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 218.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 216.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 217.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 216.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 217.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 217.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 217.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 215.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 212.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 210.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 234.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 236.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 202.01 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747606033.672003
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:48, 20.29s/it]  7%|▋         | 2/30 [00:22<04:27,  9.54s/it] 10%|█         | 3/30 [00:24<02:45,  6.11s/it] 13%|█▎        | 4/30 [00:26<01:57,  4.53s/it] 17%|█▋        | 5/30 [00:28<01:32,  3.69s/it] 20%|██        | 6/30 [00:30<01:14,  3.12s/it] 23%|██▎       | 7/30 [00:32<01:03,  2.77s/it] 27%|██▋       | 8/30 [00:34<00:55,  2.53s/it] 30%|███       | 9/30 [00:36<00:51,  2.44s/it] 33%|███▎      | 10/30 [00:38<00:46,  2.31s/it] 37%|███▋      | 11/30 [00:40<00:42,  2.21s/it] 40%|████      | 12/30 [00:43<00:38,  2.15s/it] 43%|████▎     | 13/30 [00:45<00:35,  2.11s/it] 47%|████▋     | 14/30 [00:47<00:33,  2.08s/it] 50%|█████     | 15/30 [00:49<00:30,  2.06s/it]                                                50%|█████     | 15/30 [00:49<00:30,  2.06s/it] 53%|█████▎    | 16/30 [00:51<00:28,  2.04s/it] 57%|█████▋    | 17/30 [00:53<00:26,  2.03s/it] 60%|██████    | 18/30 [00:55<00:24,  2.07s/it] 63%|██████▎   | 19/30 [00:57<00:22,  2.04s/it] 67%|██████▋   | 20/30 [00:59<00:20,  2.03s/it] 70%|███████   | 21/30 [01:01<00:18,  2.02s/it] 73%|███████▎  | 22/30 [01:03<00:16,  2.01s/it] 77%|███████▋  | 23/30 [01:05<00:14,  2.01s/it] 80%|████████  | 24/30 [01:07<00:12,  2.06s/it] 83%|████████▎ | 25/30 [01:09<00:10,  2.04s/it] 87%|████████▋ | 26/30 [01:11<00:08,  2.09s/it] 90%|█████████ | 27/30 [01:13<00:06,  2.07s/it] 93%|█████████▎| 28/30 [01:15<00:04,  2.05s/it] 97%|█████████▋| 29/30 [01:17<00:02,  2.03s/it]100%|██████████| 30/30 [01:19<00:00,  2.03s/it]                                               100%|██████████| 30/30 [01:19<00:00,  2.03s/it]                                               100%|██████████| 30/30 [01:19<00:00,  2.03s/it]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7471, 'grad_norm': 0.12057797610759735, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7172, 'grad_norm': 0.15246781706809998, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 79.5675, 'train_samples_per_second': 3.016, 'train_steps_per_second': 0.377, 'train_loss': 0.7321669578552246, 'epoch': 0.24}
END_PROFILE: 1747606130.583968
Run  1: 100.372382339 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.95s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.93s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:51<00:25, 12.85s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.78s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.48s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.75s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:10, 95.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 134.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 160.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 177.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 192.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 202.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 206.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 209.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 211.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 214.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 215.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 215.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 218.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 219.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 217.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 216.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 216.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 218.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 217.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 218.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 218.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 217.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 219.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 219.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 217.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 218.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 218.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 218.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 216.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 217.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 216.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 213.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 210.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 232.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 203.47 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747606260.3546236
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<11:03, 22.87s/it]  7%|▋         | 2/30 [00:24<04:55, 10.54s/it] 10%|█         | 3/30 [00:26<02:58,  6.61s/it] 13%|█▎        | 4/30 [00:28<02:04,  4.79s/it] 17%|█▋        | 5/30 [00:30<01:35,  3.83s/it] 20%|██        | 6/30 [00:32<01:16,  3.18s/it] 23%|██▎       | 7/30 [00:34<01:04,  2.78s/it] 27%|██▋       | 8/30 [00:36<00:55,  2.51s/it] 30%|███       | 9/30 [00:38<00:50,  2.40s/it] 33%|███▎      | 10/30 [00:40<00:45,  2.26s/it] 37%|███▋      | 11/30 [00:42<00:40,  2.15s/it] 40%|████      | 12/30 [00:44<00:37,  2.08s/it] 43%|████▎     | 13/30 [00:46<00:34,  2.03s/it] 47%|████▋     | 14/30 [00:48<00:31,  1.99s/it] 50%|█████     | 15/30 [00:50<00:29,  1.97s/it]                                                50%|█████     | 15/30 [00:50<00:29,  1.97s/it] 53%|█████▎    | 16/30 [00:52<00:27,  1.95s/it] 57%|█████▋    | 17/30 [00:54<00:25,  1.93s/it] 60%|██████    | 18/30 [00:56<00:23,  1.98s/it] 63%|██████▎   | 19/30 [00:58<00:21,  1.96s/it] 67%|██████▋   | 20/30 [01:00<00:19,  1.94s/it] 70%|███████   | 21/30 [01:01<00:17,  1.93s/it] 73%|███████▎  | 22/30 [01:03<00:15,  1.92s/it] 77%|███████▋  | 23/30 [01:05<00:13,  1.92s/it] 80%|████████  | 24/30 [01:07<00:11,  1.97s/it] 83%|████████▎ | 25/30 [01:09<00:09,  1.95s/it] 87%|████████▋ | 26/30 [01:11<00:08,  2.00s/it] 90%|█████████ | 27/30 [01:13<00:05,  1.98s/it] 93%|█████████▎| 28/30 [01:15<00:03,  1.96s/it] 97%|█████████▋| 29/30 [01:17<00:01,  1.94s/it]100%|██████████| 30/30 [01:19<00:00,  1.93s/it]                                               100%|██████████| 30/30 [01:19<00:00,  1.93s/it]                                               100%|██████████| 30/30 [01:19<00:00,  1.93s/it]100%|██████████| 30/30 [01:19<00:00,  2.65s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7471, 'grad_norm': 0.13376417756080627, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7172, 'grad_norm': 0.15383902192115784, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 79.5245, 'train_samples_per_second': 3.018, 'train_steps_per_second': 0.377, 'train_loss': 0.7321206410725911, 'epoch': 0.24}
END_PROFILE: 1747606357.0733876
Run  2: 99.051700702 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for phi-4...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:09, 13.94s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.94s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:51<00:25, 12.85s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.80s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.77s/it]
Adding LoRA adapters for phi-4...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:26, 37.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:09, 96.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:06, 135.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:05, 161.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:04, 179.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:04, 191.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:04, 197.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:01<00:03, 202.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:01<00:03, 206.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:01<00:03, 210.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:01<00:03, 212.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:01<00:03, 213.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:01<00:03, 214.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:02<00:02, 214.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:02<00:02, 215.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:02<00:02, 217.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:02<00:02, 217.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:02<00:02, 217.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:02<00:02, 217.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:02<00:02, 219.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:02<00:01, 217.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:03<00:01, 217.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:03<00:01, 218.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:03<00:01, 218.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:03<00:01, 218.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:03<00:01, 218.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:03<00:01, 218.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:03<00:01, 218.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:03<00:00, 217.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:04<00:00, 217.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:04<00:00, 218.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:04<00:00, 217.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:04<00:00, 212.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:04<00:00, 210.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:04<00:00, 234.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 239.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:04<00:00, 203.13 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,768,000/14,692,275,200 (0.22% trained)
START_PROFILE: 1747606489.0150483
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:21<10:11, 21.08s/it]  7%|▋         | 2/30 [00:23<04:35,  9.84s/it] 10%|█         | 3/30 [00:25<02:48,  6.26s/it] 13%|█▎        | 4/30 [00:27<01:59,  4.60s/it] 17%|█▋        | 5/30 [00:29<01:33,  3.73s/it] 20%|██        | 6/30 [00:31<01:15,  3.13s/it] 23%|██▎       | 7/30 [00:33<01:03,  2.75s/it] 27%|██▋       | 8/30 [00:35<00:55,  2.51s/it] 30%|███       | 9/30 [00:37<00:50,  2.41s/it] 33%|███▎      | 10/30 [00:39<00:45,  2.28s/it] 37%|███▋      | 11/30 [00:41<00:41,  2.18s/it] 40%|████      | 12/30 [00:43<00:38,  2.12s/it] 43%|████▎     | 13/30 [00:45<00:35,  2.07s/it] 47%|████▋     | 14/30 [00:47<00:32,  2.03s/it] 50%|█████     | 15/30 [00:49<00:30,  2.01s/it]                                                50%|█████     | 15/30 [00:49<00:30,  2.01s/it] 53%|█████▎    | 16/30 [00:51<00:27,  2.00s/it] 57%|█████▋    | 17/30 [00:53<00:25,  1.98s/it] 60%|██████    | 18/30 [00:55<00:24,  2.02s/it] 63%|██████▎   | 19/30 [00:57<00:21,  2.00s/it] 67%|██████▋   | 20/30 [00:59<00:19,  1.98s/it] 70%|███████   | 21/30 [01:01<00:17,  1.97s/it] 73%|███████▎  | 22/30 [01:02<00:15,  1.96s/it] 77%|███████▋  | 23/30 [01:04<00:13,  1.96s/it] 80%|████████  | 24/30 [01:07<00:12,  2.01s/it] 83%|████████▎ | 25/30 [01:09<00:09,  2.00s/it] 87%|████████▋ | 26/30 [01:11<00:08,  2.05s/it] 90%|█████████ | 27/30 [01:13<00:06,  2.03s/it] 93%|█████████▎| 28/30 [01:15<00:03,  2.00s/it] 97%|█████████▋| 29/30 [01:17<00:01,  1.98s/it]100%|██████████| 30/30 [01:19<00:00,  1.97s/it]                                               100%|██████████| 30/30 [01:19<00:00,  1.97s/it]                                               100%|██████████| 30/30 [01:19<00:00,  1.97s/it]100%|██████████| 30/30 [01:19<00:00,  2.63s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.7484, 'grad_norm': 0.13397319614887238, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.7176, 'grad_norm': 0.15205954015254974, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 79.015, 'train_samples_per_second': 3.037, 'train_steps_per_second': 0.38, 'train_loss': 0.7329806963602702, 'epoch': 0.24}
END_PROFILE: 1747606585.2285593
Run  3: 98.386568202 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 98.386568202 seconds
  Max elapsed : 100.372382339 seconds
  Avg elapsed : 99.27021708100000000000 seconds
===================================
--------------------------------------------
--------------------------------------------
Timing gemma-3-4b with NONE

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.31s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:30, 10.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:50, 19.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 24.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:21, 38.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:21, 38.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:18, 39.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:13, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:12, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:17<00:08, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:22<00:03, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:03, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 37.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 36.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 43.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:25<00:00, 41.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 42.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 37.97 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747606711.852066
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<10:03, 20.83s/it]  7%|▋         | 2/30 [00:22<04:26,  9.53s/it] 10%|█         | 3/30 [00:24<02:39,  5.92s/it] 13%|█▎        | 4/30 [00:25<01:49,  4.21s/it] 17%|█▋        | 5/30 [00:27<01:23,  3.32s/it] 20%|██        | 6/30 [00:29<01:06,  2.76s/it] 23%|██▎       | 7/30 [00:30<00:54,  2.37s/it] 27%|██▋       | 8/30 [00:32<00:46,  2.12s/it] 30%|███       | 9/30 [00:33<00:41,  1.98s/it] 33%|███▎      | 10/30 [00:35<00:37,  1.86s/it] 37%|███▋      | 11/30 [00:37<00:33,  1.78s/it] 40%|████      | 12/30 [00:38<00:30,  1.71s/it] 43%|████▎     | 13/30 [00:40<00:28,  1.66s/it] 47%|████▋     | 14/30 [00:41<00:26,  1.63s/it] 50%|█████     | 15/30 [00:43<00:24,  1.61s/it]                                                50%|█████     | 15/30 [00:43<00:24,  1.61s/it] 53%|█████▎    | 16/30 [00:44<00:22,  1.60s/it] 57%|█████▋    | 17/30 [00:46<00:20,  1.60s/it] 60%|██████    | 18/30 [00:48<00:19,  1.62s/it] 63%|██████▎   | 19/30 [00:49<00:17,  1.61s/it] 67%|██████▋   | 20/30 [00:51<00:16,  1.60s/it] 70%|███████   | 21/30 [00:52<00:14,  1.59s/it] 73%|███████▎  | 22/30 [00:54<00:12,  1.58s/it] 77%|███████▋  | 23/30 [00:56<00:11,  1.58s/it] 80%|████████  | 24/30 [00:57<00:09,  1.62s/it] 83%|████████▎ | 25/30 [00:59<00:08,  1.60s/it] 87%|████████▋ | 26/30 [01:01<00:06,  1.64s/it] 90%|█████████ | 27/30 [01:02<00:04,  1.62s/it] 93%|█████████▎| 28/30 [01:04<00:03,  1.60s/it] 97%|█████████▋| 29/30 [01:05<00:01,  1.59s/it]100%|██████████| 30/30 [01:07<00:00,  1.57s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.57s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.57s/it]100%|██████████| 30/30 [01:07<00:00,  2.24s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5839, 'grad_norm': 0.3411080837249756, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0773, 'grad_norm': 0.28794699907302856, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 67.2795, 'train_samples_per_second': 3.567, 'train_steps_per_second': 0.446, 'train_loss': 1.3306046803792317, 'epoch': 0.24}
END_PROFILE: 1747606779.654009
Run  1: 69.308064824 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.32s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:28, 11.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:50, 19.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 25.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:21, 38.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:20, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:18, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:12<00:13, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:12, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:17<00:08, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:22<00:03, 39.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:02, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:23<00:02, 38.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 38.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 37.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 43.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:25<00:00, 42.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 42.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 38.05 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747606880.1175783
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:21<10:15, 21.22s/it]  7%|▋         | 2/30 [00:22<04:31,  9.69s/it] 10%|█         | 3/30 [00:24<02:42,  6.00s/it] 13%|█▎        | 4/30 [00:26<01:50,  4.26s/it] 17%|█▋        | 5/30 [00:27<01:23,  3.35s/it] 20%|██        | 6/30 [00:29<01:06,  2.78s/it] 23%|██▎       | 7/30 [00:31<00:54,  2.38s/it] 27%|██▋       | 8/30 [00:32<00:46,  2.13s/it] 30%|███       | 9/30 [00:34<00:41,  1.99s/it] 33%|███▎      | 10/30 [00:35<00:37,  1.87s/it] 37%|███▋      | 11/30 [00:37<00:33,  1.78s/it] 40%|████      | 12/30 [00:38<00:30,  1.71s/it] 43%|████▎     | 13/30 [00:40<00:28,  1.65s/it] 47%|████▋     | 14/30 [00:42<00:25,  1.62s/it] 50%|█████     | 15/30 [00:43<00:23,  1.60s/it]                                                50%|█████     | 15/30 [00:43<00:23,  1.60s/it] 53%|█████▎    | 16/30 [00:45<00:22,  1.59s/it] 57%|█████▋    | 17/30 [00:46<00:20,  1.59s/it] 60%|██████    | 18/30 [00:48<00:19,  1.61s/it] 63%|██████▎   | 19/30 [00:50<00:17,  1.60s/it] 67%|██████▋   | 20/30 [00:51<00:15,  1.60s/it] 70%|███████   | 21/30 [00:53<00:14,  1.59s/it] 73%|███████▎  | 22/30 [00:54<00:12,  1.58s/it] 77%|███████▋  | 23/30 [00:56<00:11,  1.58s/it] 80%|████████  | 24/30 [00:58<00:09,  1.62s/it] 83%|████████▎ | 25/30 [00:59<00:08,  1.60s/it] 87%|████████▋ | 26/30 [01:01<00:06,  1.64s/it] 90%|█████████ | 27/30 [01:02<00:04,  1.62s/it] 93%|█████████▎| 28/30 [01:04<00:03,  1.60s/it] 97%|█████████▋| 29/30 [01:06<00:01,  1.59s/it]100%|██████████| 30/30 [01:07<00:00,  1.57s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.57s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.57s/it]100%|██████████| 30/30 [01:07<00:00,  2.25s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5839, 'grad_norm': 0.34226223826408386, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0773, 'grad_norm': 0.2878551483154297, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 67.5456, 'train_samples_per_second': 3.553, 'train_steps_per_second': 0.444, 'train_loss': 1.3305716832478842, 'epoch': 0.24}
END_PROFILE: 1747606948.1789465
Run  2: 69.585766695 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.29s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:30, 10.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:51, 19.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 24.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:29, 32.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:22, 38.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:20, 38.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:13, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:18<00:08, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:23<00:03, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:03, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 37.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 36.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 42.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:26<00:00, 42.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 43.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 37.89 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747607046.9242086
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:21<10:28, 21.68s/it]  7%|▋         | 2/30 [00:23<04:35,  9.84s/it] 10%|█         | 3/30 [00:24<02:43,  6.06s/it] 13%|█▎        | 4/30 [00:26<01:50,  4.27s/it] 17%|█▋        | 5/30 [00:27<01:23,  3.33s/it] 20%|██        | 6/30 [00:29<01:05,  2.75s/it] 23%|██▎       | 7/30 [00:31<00:53,  2.34s/it] 27%|██▋       | 8/30 [00:32<00:45,  2.07s/it] 30%|███       | 9/30 [00:34<00:40,  1.93s/it] 33%|███▎      | 10/30 [00:35<00:36,  1.81s/it] 37%|███▋      | 11/30 [00:37<00:32,  1.71s/it] 40%|████      | 12/30 [00:38<00:29,  1.64s/it] 43%|████▎     | 13/30 [00:40<00:26,  1.58s/it] 47%|████▋     | 14/30 [00:41<00:24,  1.55s/it] 50%|█████     | 15/30 [00:43<00:23,  1.53s/it]                                                50%|█████     | 15/30 [00:43<00:23,  1.53s/it] 53%|█████▎    | 16/30 [00:44<00:21,  1.53s/it] 57%|█████▋    | 17/30 [00:46<00:19,  1.52s/it] 60%|██████    | 18/30 [00:47<00:18,  1.55s/it] 63%|██████▎   | 19/30 [00:49<00:16,  1.53s/it] 67%|██████▋   | 20/30 [00:50<00:15,  1.53s/it] 70%|███████   | 21/30 [00:52<00:13,  1.51s/it] 73%|███████▎  | 22/30 [00:53<00:12,  1.51s/it] 77%|███████▋  | 23/30 [00:55<00:10,  1.51s/it] 80%|████████  | 24/30 [00:56<00:09,  1.55s/it] 83%|████████▎ | 25/30 [00:58<00:07,  1.53s/it] 87%|████████▋ | 26/30 [01:00<00:06,  1.57s/it] 90%|█████████ | 27/30 [01:01<00:04,  1.55s/it] 93%|█████████▎| 28/30 [01:03<00:03,  1.54s/it] 97%|█████████▋| 29/30 [01:04<00:01,  1.53s/it]100%|██████████| 30/30 [01:06<00:00,  1.51s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.51s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.51s/it]100%|██████████| 30/30 [01:06<00:00,  2.20s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5837, 'grad_norm': 0.342022567987442, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0772, 'grad_norm': 0.2887924313545227, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 66.0524, 'train_samples_per_second': 3.633, 'train_steps_per_second': 0.454, 'train_loss': 1.330480448404948, 'epoch': 0.24}
END_PROFILE: 1747607113.4835658
Run  3: 68.145255895 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 68.145255895 seconds
  Max elapsed : 69.585766695 seconds
  Avg elapsed : 69.01302913800000000000 seconds
===================================
NSYS

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.22s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:29, 10.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:51, 18.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:02<00:38, 24.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 28.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:29, 31.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:27, 33.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 37.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:22, 38.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:21, 38.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 38.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 38.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:20, 38.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:07<00:19, 38.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 38.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 38.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 38.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:18, 38.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 38.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 38.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:17, 38.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 38.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 38.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:16, 38.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:11<00:15, 38.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 38.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:15, 38.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:12<00:14, 38.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 38.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:14, 38.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 38.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:13, 38.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 38.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 38.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:12, 38.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:15<00:11, 38.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 38.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:11, 38.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:16<00:10, 38.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 38.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 38.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:17<00:09, 38.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 38.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 38.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:18<00:08, 38.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:08, 38.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 38.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 38.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:07, 38.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:20<00:06, 38.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 38.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 38.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:21<00:05, 38.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 38.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 38.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:22<00:04, 38.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 38.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 38.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:23<00:03, 38.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:03, 38.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:24<00:02, 38.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 37.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 37.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:25<00:01, 36.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 36.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 36.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:26<00:00, 40.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:26<00:00, 40.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 41.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 37.36 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747607233.3614874
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:23<11:07, 23.03s/it]  7%|▋         | 2/30 [00:25<05:01, 10.75s/it] 10%|█         | 3/30 [00:27<03:04,  6.84s/it] 13%|█▎        | 4/30 [00:29<02:09,  4.98s/it] 17%|█▋        | 5/30 [00:31<01:40,  4.00s/it] 20%|██        | 6/30 [00:33<01:21,  3.38s/it] 23%|██▎       | 7/30 [00:36<01:08,  2.97s/it] 27%|██▋       | 8/30 [00:38<00:59,  2.69s/it] 30%|███       | 9/30 [00:40<00:53,  2.53s/it] 33%|███▎      | 10/30 [00:42<00:48,  2.41s/it] 37%|███▋      | 11/30 [00:44<00:44,  2.32s/it] 40%|████      | 12/30 [00:46<00:40,  2.25s/it] 43%|████▎     | 13/30 [00:48<00:37,  2.20s/it] 47%|████▋     | 14/30 [00:50<00:34,  2.17s/it] 50%|█████     | 15/30 [00:52<00:32,  2.15s/it]                                                50%|█████     | 15/30 [00:52<00:32,  2.15s/it] 53%|█████▎    | 16/30 [00:55<00:29,  2.14s/it] 57%|█████▋    | 17/30 [00:57<00:27,  2.14s/it] 60%|██████    | 18/30 [00:59<00:25,  2.15s/it] 63%|██████▎   | 19/30 [01:01<00:23,  2.15s/it] 67%|██████▋   | 20/30 [01:03<00:21,  2.14s/it] 70%|███████   | 21/30 [01:05<00:19,  2.13s/it] 73%|███████▎  | 22/30 [01:07<00:16,  2.12s/it] 77%|███████▋  | 23/30 [01:10<00:14,  2.12s/it] 80%|████████  | 24/30 [01:12<00:12,  2.13s/it] 83%|████████▎ | 25/30 [01:14<00:10,  2.12s/it] 87%|████████▋ | 26/30 [01:16<00:08,  2.13s/it] 90%|█████████ | 27/30 [01:18<00:06,  2.12s/it] 93%|█████████▎| 28/30 [01:20<00:04,  2.12s/it] 97%|█████████▋| 29/30 [01:22<00:02,  2.11s/it]100%|██████████| 30/30 [01:24<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.10s/it]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5838, 'grad_norm': 0.3413899540901184, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0771, 'grad_norm': 0.2880481481552124, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 84.7888, 'train_samples_per_second': 2.831, 'train_steps_per_second': 0.354, 'train_loss': 1.3304474512736002, 'epoch': 0.24}
END_PROFILE: 1747607318.6560469
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/gemma-3-4b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-6276.qdstrm'
[1/1] [0%                          ] nsys-report-45b8.nsys-rep[1/1] [0%                          ] nsys-report-45b8.nsys-rep[1/1] [0%                          ] nsys-report-45b8.nsys-rep[1/1] [7%                          ] nsys-report-45b8.nsys-rep[1/1] [6%                          ] nsys-report-45b8.nsys-rep[1/1] [8%                          ] nsys-report-45b8.nsys-rep[1/1] [7%                          ] nsys-report-45b8.nsys-rep[1/1] [6%                          ] nsys-report-45b8.nsys-rep[1/1] [5%                          ] nsys-report-45b8.nsys-rep[1/1] [5%                          ] nsys-report-45b8.nsys-rep[1/1] [6%                          ] nsys-report-45b8.nsys-rep[1/1] [7%                          ] nsys-report-45b8.nsys-rep[1/1] [8%                          ] nsys-report-45b8.nsys-rep[1/1] [9%                          ] nsys-report-45b8.nsys-rep[1/1] [10%                         ] nsys-report-45b8.nsys-rep[1/1] [11%                         ] nsys-report-45b8.nsys-rep[1/1] [12%                         ] nsys-report-45b8.nsys-rep[1/1] [13%                         ] nsys-report-45b8.nsys-rep[1/1] [14%                         ] nsys-report-45b8.nsys-rep[1/1] [=15%                        ] nsys-report-45b8.nsys-rep[1/1] [=16%                        ] nsys-report-45b8.nsys-rep[1/1] [=17%                        ] nsys-report-45b8.nsys-rep[1/1] [==18%                       ] nsys-report-45b8.nsys-rep[1/1] [==19%                       ] nsys-report-45b8.nsys-rep[1/1] [==20%                       ] nsys-report-45b8.nsys-rep[1/1] [==21%                       ] nsys-report-45b8.nsys-rep[1/1] [===22%                      ] nsys-report-45b8.nsys-rep[1/1] [===23%                      ] nsys-report-45b8.nsys-rep[1/1] [===24%                      ] nsys-report-45b8.nsys-rep[1/1] [====25%                     ] nsys-report-45b8.nsys-rep[1/1] [====26%                     ] nsys-report-45b8.nsys-rep[1/1] [====27%                     ] nsys-report-45b8.nsys-rep[1/1] [====28%                     ] nsys-report-45b8.nsys-rep[1/1] [=====29%                    ] nsys-report-45b8.nsys-rep[1/1] [=====30%                    ] nsys-report-45b8.nsys-rep[1/1] [=====31%                    ] nsys-report-45b8.nsys-rep[1/1] [=====32%                    ] nsys-report-45b8.nsys-rep[1/1] [======33%                   ] nsys-report-45b8.nsys-rep[1/1] [======34%                   ] nsys-report-45b8.nsys-rep[1/1] [======35%                   ] nsys-report-45b8.nsys-rep[1/1] [=======36%                  ] nsys-report-45b8.nsys-rep[1/1] [=======37%                  ] nsys-report-45b8.nsys-rep[1/1] [=======38%                  ] nsys-report-45b8.nsys-rep[1/1] [=======39%                  ] nsys-report-45b8.nsys-rep[1/1] [========40%                 ] nsys-report-45b8.nsys-rep[1/1] [========41%                 ] nsys-report-45b8.nsys-rep[1/1] [========42%                 ] nsys-report-45b8.nsys-rep[1/1] [=========43%                ] nsys-report-45b8.nsys-rep[1/1] [=========44%                ] nsys-report-45b8.nsys-rep[1/1] [=========45%                ] nsys-report-45b8.nsys-rep[1/1] [=========46%                ] nsys-report-45b8.nsys-rep[1/1] [==========47%               ] nsys-report-45b8.nsys-rep[1/1] [==========48%               ] nsys-report-45b8.nsys-rep[1/1] [==========49%               ] nsys-report-45b8.nsys-rep[1/1] [===========50%              ] nsys-report-45b8.nsys-rep[1/1] [===========51%              ] nsys-report-45b8.nsys-rep[1/1] [===========52%              ] nsys-report-45b8.nsys-rep[1/1] [===========53%              ] nsys-report-45b8.nsys-rep[1/1] [============54%             ] nsys-report-45b8.nsys-rep[1/1] [============55%             ] nsys-report-45b8.nsys-rep[1/1] [============56%             ] nsys-report-45b8.nsys-rep[1/1] [============57%             ] nsys-report-45b8.nsys-rep[1/1] [=============58%            ] nsys-report-45b8.nsys-rep[1/1] [=============59%            ] nsys-report-45b8.nsys-rep[1/1] [=============60%            ] nsys-report-45b8.nsys-rep[1/1] [==============61%           ] nsys-report-45b8.nsys-rep[1/1] [==============62%           ] nsys-report-45b8.nsys-rep[1/1] [==============63%           ] nsys-report-45b8.nsys-rep[1/1] [==============64%           ] nsys-report-45b8.nsys-rep[1/1] [===============65%          ] nsys-report-45b8.nsys-rep[1/1] [===============66%          ] nsys-report-45b8.nsys-rep[1/1] [===============67%          ] nsys-report-45b8.nsys-rep[1/1] [================68%         ] nsys-report-45b8.nsys-rep[1/1] [================69%         ] nsys-report-45b8.nsys-rep[1/1] [================70%         ] nsys-report-45b8.nsys-rep[1/1] [================71%         ] nsys-report-45b8.nsys-rep[1/1] [=================72%        ] nsys-report-45b8.nsys-rep[1/1] [=================73%        ] nsys-report-45b8.nsys-rep[1/1] [=================74%        ] nsys-report-45b8.nsys-rep[1/1] [==================75%       ] nsys-report-45b8.nsys-rep[1/1] [==================76%       ] nsys-report-45b8.nsys-rep[1/1] [==================77%       ] nsys-report-45b8.nsys-rep[1/1] [==================78%       ] nsys-report-45b8.nsys-rep[1/1] [===================79%      ] nsys-report-45b8.nsys-rep[1/1] [===================80%      ] nsys-report-45b8.nsys-rep[1/1] [===================81%      ] nsys-report-45b8.nsys-rep[1/1] [========================100%] nsys-report-45b8.nsys-rep[1/1] [========================100%] nsys-report-45b8.nsys-rep
Generated:
	/tmp/nsys-report-45b8.nsys-rep
Run  1: 99.218480182 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.33s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:29, 10.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:51, 19.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 24.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:29, 32.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:22, 38.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:21, 38.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:18, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:12, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 38.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:18<00:08, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:08, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:23<00:03, 39.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:02, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 38.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 37.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 43.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:26<00:00, 42.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 43.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 37.90 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747607432.3734481
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:15, 19.16s/it]  7%|▋         | 2/30 [00:21<04:16,  9.16s/it] 10%|█         | 3/30 [00:23<02:41,  5.97s/it] 13%|█▎        | 4/30 [00:25<01:55,  4.45s/it] 17%|█▋        | 5/30 [00:27<01:31,  3.66s/it] 20%|██        | 6/30 [00:30<01:15,  3.15s/it] 23%|██▎       | 7/30 [00:32<01:04,  2.81s/it] 27%|██▋       | 8/30 [00:34<00:56,  2.58s/it] 30%|███       | 9/30 [00:36<00:51,  2.45s/it] 33%|███▎      | 10/30 [00:38<00:47,  2.36s/it] 37%|███▋      | 11/30 [00:40<00:43,  2.28s/it] 40%|████      | 12/30 [00:42<00:40,  2.22s/it] 43%|████▎     | 13/30 [00:44<00:37,  2.18s/it] 47%|████▋     | 14/30 [00:46<00:34,  2.15s/it] 50%|█████     | 15/30 [00:49<00:32,  2.14s/it]                                                50%|█████     | 15/30 [00:49<00:32,  2.14s/it] 53%|█████▎    | 16/30 [00:51<00:29,  2.13s/it] 57%|█████▋    | 17/30 [00:53<00:27,  2.12s/it] 60%|██████    | 18/30 [00:55<00:25,  2.14s/it] 63%|██████▎   | 19/30 [00:57<00:23,  2.13s/it] 67%|██████▋   | 20/30 [00:59<00:21,  2.12s/it] 70%|███████   | 21/30 [01:01<00:19,  2.12s/it] 73%|███████▎  | 22/30 [01:03<00:16,  2.12s/it] 77%|███████▋  | 23/30 [01:05<00:14,  2.12s/it] 80%|████████  | 24/30 [01:08<00:12,  2.13s/it] 83%|████████▎ | 25/30 [01:10<00:10,  2.12s/it] 87%|████████▋ | 26/30 [01:12<00:08,  2.13s/it] 90%|█████████ | 27/30 [01:14<00:06,  2.12s/it] 93%|█████████▎| 28/30 [01:16<00:04,  2.11s/it] 97%|█████████▋| 29/30 [01:18<00:02,  2.11s/it]100%|██████████| 30/30 [01:20<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:20<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:20<00:00,  2.10s/it]100%|██████████| 30/30 [01:20<00:00,  2.69s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5837, 'grad_norm': 0.3414872884750366, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0773, 'grad_norm': 0.289827823638916, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 80.7592, 'train_samples_per_second': 2.972, 'train_steps_per_second': 0.371, 'train_loss': 1.3305104573567708, 'epoch': 0.24}
END_PROFILE: 1747607513.6464555
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/gemma-3-4b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-7dfc.qdstrm'
[1/1] [0%                          ] nsys-report-fc2e.nsys-rep[1/1] [0%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [8%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [5%                          ] nsys-report-fc2e.nsys-rep[1/1] [5%                          ] nsys-report-fc2e.nsys-rep[1/1] [6%                          ] nsys-report-fc2e.nsys-rep[1/1] [7%                          ] nsys-report-fc2e.nsys-rep[1/1] [8%                          ] nsys-report-fc2e.nsys-rep[1/1] [9%                          ] nsys-report-fc2e.nsys-rep[1/1] [10%                         ] nsys-report-fc2e.nsys-rep[1/1] [11%                         ] nsys-report-fc2e.nsys-rep[1/1] [12%                         ] nsys-report-fc2e.nsys-rep[1/1] [13%                         ] nsys-report-fc2e.nsys-rep[1/1] [14%                         ] nsys-report-fc2e.nsys-rep[1/1] [=15%                        ] nsys-report-fc2e.nsys-rep[1/1] [=16%                        ] nsys-report-fc2e.nsys-rep[1/1] [=17%                        ] nsys-report-fc2e.nsys-rep[1/1] [==18%                       ] nsys-report-fc2e.nsys-rep[1/1] [==19%                       ] nsys-report-fc2e.nsys-rep[1/1] [==20%                       ] nsys-report-fc2e.nsys-rep[1/1] [==21%                       ] nsys-report-fc2e.nsys-rep[1/1] [===22%                      ] nsys-report-fc2e.nsys-rep[1/1] [===23%                      ] nsys-report-fc2e.nsys-rep[1/1] [===24%                      ] nsys-report-fc2e.nsys-rep[1/1] [====25%                     ] nsys-report-fc2e.nsys-rep[1/1] [====26%                     ] nsys-report-fc2e.nsys-rep[1/1] [====27%                     ] nsys-report-fc2e.nsys-rep[1/1] [====28%                     ] nsys-report-fc2e.nsys-rep[1/1] [=====29%                    ] nsys-report-fc2e.nsys-rep[1/1] [=====30%                    ] nsys-report-fc2e.nsys-rep[1/1] [=====31%                    ] nsys-report-fc2e.nsys-rep[1/1] [=====32%                    ] nsys-report-fc2e.nsys-rep[1/1] [======33%                   ] nsys-report-fc2e.nsys-rep[1/1] [======34%                   ] nsys-report-fc2e.nsys-rep[1/1] [======35%                   ] nsys-report-fc2e.nsys-rep[1/1] [=======36%                  ] nsys-report-fc2e.nsys-rep[1/1] [=======37%                  ] nsys-report-fc2e.nsys-rep[1/1] [=======38%                  ] nsys-report-fc2e.nsys-rep[1/1] [=======39%                  ] nsys-report-fc2e.nsys-rep[1/1] [========40%                 ] nsys-report-fc2e.nsys-rep[1/1] [========41%                 ] nsys-report-fc2e.nsys-rep[1/1] [========42%                 ] nsys-report-fc2e.nsys-rep[1/1] [=========43%                ] nsys-report-fc2e.nsys-rep[1/1] [=========44%                ] nsys-report-fc2e.nsys-rep[1/1] [=========45%                ] nsys-report-fc2e.nsys-rep[1/1] [=========46%                ] nsys-report-fc2e.nsys-rep[1/1] [==========47%               ] nsys-report-fc2e.nsys-rep[1/1] [==========48%               ] nsys-report-fc2e.nsys-rep[1/1] [==========49%               ] nsys-report-fc2e.nsys-rep[1/1] [===========50%              ] nsys-report-fc2e.nsys-rep[1/1] [===========51%              ] nsys-report-fc2e.nsys-rep[1/1] [===========52%              ] nsys-report-fc2e.nsys-rep[1/1] [===========53%              ] nsys-report-fc2e.nsys-rep[1/1] [============54%             ] nsys-report-fc2e.nsys-rep[1/1] [============55%             ] nsys-report-fc2e.nsys-rep[1/1] [============56%             ] nsys-report-fc2e.nsys-rep[1/1] [============57%             ] nsys-report-fc2e.nsys-rep[1/1] [=============58%            ] nsys-report-fc2e.nsys-rep[1/1] [=============59%            ] nsys-report-fc2e.nsys-rep[1/1] [=============60%            ] nsys-report-fc2e.nsys-rep[1/1] [==============61%           ] nsys-report-fc2e.nsys-rep[1/1] [==============62%           ] nsys-report-fc2e.nsys-rep[1/1] [==============63%           ] nsys-report-fc2e.nsys-rep[1/1] [==============64%           ] nsys-report-fc2e.nsys-rep[1/1] [===============65%          ] nsys-report-fc2e.nsys-rep[1/1] [===============66%          ] nsys-report-fc2e.nsys-rep[1/1] [===============67%          ] nsys-report-fc2e.nsys-rep[1/1] [================68%         ] nsys-report-fc2e.nsys-rep[1/1] [================69%         ] nsys-report-fc2e.nsys-rep[1/1] [================70%         ] nsys-report-fc2e.nsys-rep[1/1] [================71%         ] nsys-report-fc2e.nsys-rep[1/1] [=================72%        ] nsys-report-fc2e.nsys-rep[1/1] [=================73%        ] nsys-report-fc2e.nsys-rep[1/1] [=================74%        ] nsys-report-fc2e.nsys-rep[1/1] [==================75%       ] nsys-report-fc2e.nsys-rep[1/1] [==================76%       ] nsys-report-fc2e.nsys-rep[1/1] [==================77%       ] nsys-report-fc2e.nsys-rep[1/1] [==================78%       ] nsys-report-fc2e.nsys-rep[1/1] [===================79%      ] nsys-report-fc2e.nsys-rep[1/1] [===================80%      ] nsys-report-fc2e.nsys-rep[1/1] [===================81%      ] nsys-report-fc2e.nsys-rep[1/1] [========================100%] nsys-report-fc2e.nsys-rep[1/1] [========================100%] nsys-report-fc2e.nsys-rep
Generated:
	/tmp/nsys-report-fc2e.nsys-rep
Run  2: 95.146543695 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.33s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:30, 10.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:51, 19.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 24.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:22, 38.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:21, 38.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 38.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 38.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 38.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:13, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:18<00:08, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 38.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:23<00:03, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:03, 38.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 37.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 36.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 43.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:26<00:00, 41.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 42.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 37.87 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747607629.9849188
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<10:57, 22.68s/it]  7%|▋         | 2/30 [00:24<04:55, 10.56s/it] 10%|█         | 3/30 [00:26<03:00,  6.70s/it] 13%|█▎        | 4/30 [00:28<02:06,  4.86s/it] 17%|█▋        | 5/30 [00:31<01:37,  3.90s/it] 20%|██        | 6/30 [00:33<01:18,  3.29s/it] 23%|██▎       | 7/30 [00:35<01:06,  2.89s/it] 27%|██▋       | 8/30 [00:37<00:57,  2.61s/it] 30%|███       | 9/30 [00:39<00:51,  2.45s/it] 33%|███▎      | 10/30 [00:41<00:46,  2.34s/it] 37%|███▋      | 11/30 [00:43<00:42,  2.25s/it] 40%|████      | 12/30 [00:45<00:39,  2.18s/it] 43%|████▎     | 13/30 [00:47<00:36,  2.13s/it] 47%|████▋     | 14/30 [00:49<00:33,  2.10s/it] 50%|█████     | 15/30 [00:51<00:31,  2.08s/it]                                                50%|█████     | 15/30 [00:51<00:31,  2.08s/it] 53%|█████▎    | 16/30 [00:53<00:29,  2.07s/it] 57%|█████▋    | 17/30 [00:55<00:26,  2.07s/it] 60%|██████    | 18/30 [00:57<00:24,  2.08s/it] 63%|██████▎   | 19/30 [00:59<00:22,  2.07s/it] 67%|██████▋   | 20/30 [01:01<00:20,  2.07s/it] 70%|███████   | 21/30 [01:03<00:18,  2.06s/it] 73%|███████▎  | 22/30 [01:06<00:16,  2.05s/it] 77%|███████▋  | 23/30 [01:08<00:14,  2.05s/it] 80%|████████  | 24/30 [01:10<00:12,  2.07s/it] 83%|████████▎ | 25/30 [01:12<00:10,  2.06s/it] 87%|████████▋ | 26/30 [01:14<00:08,  2.07s/it] 90%|█████████ | 27/30 [01:16<00:06,  2.06s/it] 93%|█████████▎| 28/30 [01:18<00:04,  2.05s/it] 97%|█████████▋| 29/30 [01:20<00:02,  2.05s/it]100%|██████████| 30/30 [01:22<00:00,  2.04s/it]                                               100%|██████████| 30/30 [01:22<00:00,  2.04s/it]                                               100%|██████████| 30/30 [01:22<00:00,  2.04s/it]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5835, 'grad_norm': 0.340950608253479, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.077, 'grad_norm': 0.289976567029953, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 82.4228, 'train_samples_per_second': 2.912, 'train_steps_per_second': 0.364, 'train_loss': 1.3302375157674153, 'epoch': 0.24}
END_PROFILE: 1747607712.9298778
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/gemma-3-4b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-0b0f.qdstrm'
[1/1] [0%                          ] nsys-report-bca1.nsys-rep[1/1] [0%                          ] nsys-report-bca1.nsys-rep[1/1] [0%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [5%                          ] nsys-report-bca1.nsys-rep[1/1] [8%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [5%                          ] nsys-report-bca1.nsys-rep[1/1] [5%                          ] nsys-report-bca1.nsys-rep[1/1] [6%                          ] nsys-report-bca1.nsys-rep[1/1] [7%                          ] nsys-report-bca1.nsys-rep[1/1] [8%                          ] nsys-report-bca1.nsys-rep[1/1] [9%                          ] nsys-report-bca1.nsys-rep[1/1] [10%                         ] nsys-report-bca1.nsys-rep[1/1] [11%                         ] nsys-report-bca1.nsys-rep[1/1] [12%                         ] nsys-report-bca1.nsys-rep[1/1] [13%                         ] nsys-report-bca1.nsys-rep[1/1] [14%                         ] nsys-report-bca1.nsys-rep[1/1] [=15%                        ] nsys-report-bca1.nsys-rep[1/1] [=16%                        ] nsys-report-bca1.nsys-rep[1/1] [=17%                        ] nsys-report-bca1.nsys-rep[1/1] [==18%                       ] nsys-report-bca1.nsys-rep[1/1] [==19%                       ] nsys-report-bca1.nsys-rep[1/1] [==20%                       ] nsys-report-bca1.nsys-rep[1/1] [==21%                       ] nsys-report-bca1.nsys-rep[1/1] [===22%                      ] nsys-report-bca1.nsys-rep[1/1] [===23%                      ] nsys-report-bca1.nsys-rep[1/1] [===24%                      ] nsys-report-bca1.nsys-rep[1/1] [====25%                     ] nsys-report-bca1.nsys-rep[1/1] [====26%                     ] nsys-report-bca1.nsys-rep[1/1] [====27%                     ] nsys-report-bca1.nsys-rep[1/1] [====28%                     ] nsys-report-bca1.nsys-rep[1/1] [=====29%                    ] nsys-report-bca1.nsys-rep[1/1] [=====30%                    ] nsys-report-bca1.nsys-rep[1/1] [=====31%                    ] nsys-report-bca1.nsys-rep[1/1] [=====32%                    ] nsys-report-bca1.nsys-rep[1/1] [======33%                   ] nsys-report-bca1.nsys-rep[1/1] [======34%                   ] nsys-report-bca1.nsys-rep[1/1] [======35%                   ] nsys-report-bca1.nsys-rep[1/1] [=======36%                  ] nsys-report-bca1.nsys-rep[1/1] [=======37%                  ] nsys-report-bca1.nsys-rep[1/1] [=======38%                  ] nsys-report-bca1.nsys-rep[1/1] [=======39%                  ] nsys-report-bca1.nsys-rep[1/1] [========40%                 ] nsys-report-bca1.nsys-rep[1/1] [========41%                 ] nsys-report-bca1.nsys-rep[1/1] [========42%                 ] nsys-report-bca1.nsys-rep[1/1] [=========43%                ] nsys-report-bca1.nsys-rep[1/1] [=========44%                ] nsys-report-bca1.nsys-rep[1/1] [=========45%                ] nsys-report-bca1.nsys-rep[1/1] [=========46%                ] nsys-report-bca1.nsys-rep[1/1] [==========47%               ] nsys-report-bca1.nsys-rep[1/1] [==========48%               ] nsys-report-bca1.nsys-rep[1/1] [==========49%               ] nsys-report-bca1.nsys-rep[1/1] [===========50%              ] nsys-report-bca1.nsys-rep[1/1] [===========51%              ] nsys-report-bca1.nsys-rep[1/1] [===========52%              ] nsys-report-bca1.nsys-rep[1/1] [===========53%              ] nsys-report-bca1.nsys-rep[1/1] [============54%             ] nsys-report-bca1.nsys-rep[1/1] [============55%             ] nsys-report-bca1.nsys-rep[1/1] [============56%             ] nsys-report-bca1.nsys-rep[1/1] [============57%             ] nsys-report-bca1.nsys-rep[1/1] [=============58%            ] nsys-report-bca1.nsys-rep[1/1] [=============59%            ] nsys-report-bca1.nsys-rep[1/1] [=============60%            ] nsys-report-bca1.nsys-rep[1/1] [==============61%           ] nsys-report-bca1.nsys-rep[1/1] [==============62%           ] nsys-report-bca1.nsys-rep[1/1] [==============63%           ] nsys-report-bca1.nsys-rep[1/1] [==============64%           ] nsys-report-bca1.nsys-rep[1/1] [===============65%          ] nsys-report-bca1.nsys-rep[1/1] [===============66%          ] nsys-report-bca1.nsys-rep[1/1] [===============67%          ] nsys-report-bca1.nsys-rep[1/1] [================68%         ] nsys-report-bca1.nsys-rep[1/1] [================69%         ] nsys-report-bca1.nsys-rep[1/1] [================70%         ] nsys-report-bca1.nsys-rep[1/1] [================71%         ] nsys-report-bca1.nsys-rep[1/1] [=================72%        ] nsys-report-bca1.nsys-rep[1/1] [=================73%        ] nsys-report-bca1.nsys-rep[1/1] [=================74%        ] nsys-report-bca1.nsys-rep[1/1] [==================75%       ] nsys-report-bca1.nsys-rep[1/1] [==================76%       ] nsys-report-bca1.nsys-rep[1/1] [==================77%       ] nsys-report-bca1.nsys-rep[1/1] [==================78%       ] nsys-report-bca1.nsys-rep[1/1] [===================79%      ] nsys-report-bca1.nsys-rep[1/1] [===================80%      ] nsys-report-bca1.nsys-rep[1/1] [===================81%      ] nsys-report-bca1.nsys-rep[1/1] [========================100%] nsys-report-bca1.nsys-rep[1/1] [========================100%] nsys-report-bca1.nsys-rep
Generated:
	/tmp/nsys-report-bca1.nsys-rep
Run  3: 96.959979085 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 95.146543695 seconds
  Max elapsed : 99.218480182 seconds
  Avg elapsed : 97.10833432066666666666 seconds
===================================
--------------------------------------------
PROTON

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 11.00s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:28, 11.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:50, 19.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 25.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:21, 38.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:20, 39.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:13, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:17<00:08, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:22<00:03, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:03, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 38.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 37.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 43.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:25<00:00, 41.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 42.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 38.02 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747607845.3203685
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:21<10:26, 21.61s/it]  7%|▋         | 2/30 [00:23<04:43, 10.11s/it] 10%|█         | 3/30 [00:25<02:53,  6.43s/it] 13%|█▎        | 4/30 [00:27<02:01,  4.68s/it] 17%|█▋        | 5/30 [00:29<01:34,  3.78s/it] 20%|██        | 6/30 [00:31<01:16,  3.18s/it] 23%|██▎       | 7/30 [00:33<01:04,  2.79s/it] 27%|██▋       | 8/30 [00:35<00:55,  2.54s/it] 30%|███       | 9/30 [00:38<00:50,  2.40s/it] 33%|███▎      | 10/30 [00:40<00:47,  2.36s/it] 37%|███▋      | 11/30 [00:42<00:43,  2.27s/it] 40%|████      | 12/30 [00:44<00:39,  2.21s/it] 43%|████▎     | 13/30 [00:46<00:36,  2.14s/it] 47%|████▋     | 14/30 [00:48<00:33,  2.10s/it] 50%|█████     | 15/30 [00:50<00:30,  2.06s/it]                                                50%|█████     | 15/30 [00:50<00:30,  2.06s/it] 53%|█████▎    | 16/30 [00:52<00:28,  2.04s/it] 57%|█████▋    | 17/30 [00:54<00:26,  2.03s/it] 60%|██████    | 18/30 [00:56<00:24,  2.04s/it] 63%|██████▎   | 19/30 [00:58<00:22,  2.09s/it] 67%|██████▋   | 20/30 [01:00<00:21,  2.10s/it] 70%|███████   | 21/30 [01:02<00:18,  2.10s/it] 73%|███████▎  | 22/30 [01:04<00:16,  2.08s/it] 77%|███████▋  | 23/30 [01:06<00:14,  2.07s/it] 80%|████████  | 24/30 [01:09<00:12,  2.11s/it] 83%|████████▎ | 25/30 [01:11<00:10,  2.07s/it] 87%|████████▋ | 26/30 [01:13<00:08,  2.06s/it] 90%|█████████ | 27/30 [01:15<00:06,  2.04s/it] 93%|█████████▎| 28/30 [01:17<00:04,  2.03s/it] 97%|█████████▋| 29/30 [01:19<00:02,  2.11s/it]100%|██████████| 30/30 [01:21<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:21<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:21<00:00,  2.10s/it]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5837, 'grad_norm': 0.3418767750263214, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0772, 'grad_norm': 0.28756916522979736, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 81.5171, 'train_samples_per_second': 2.944, 'train_steps_per_second': 0.368, 'train_loss': 1.330431302388509, 'epoch': 0.24}
END_PROFILE: 1747607927.6455724
Run  1: 83.881290057 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.08s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:28, 11.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:50, 19.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 25.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:21, 38.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:20, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:18, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:07<00:18, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:13, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:12<00:13, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:12, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:17<00:08, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:22<00:03, 39.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:02, 39.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:23<00:02, 38.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 38.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 37.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 42.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:25<00:00, 42.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 43.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 38.06 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747608022.5857477
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<10:52, 22.51s/it]  7%|▋         | 2/30 [00:24<04:52, 10.45s/it] 10%|█         | 3/30 [00:26<02:58,  6.60s/it] 13%|█▎        | 4/30 [00:28<02:04,  4.77s/it] 17%|█▋        | 5/30 [00:30<01:35,  3.83s/it] 20%|██        | 6/30 [00:32<01:16,  3.20s/it] 23%|██▎       | 7/30 [00:34<01:04,  2.79s/it] 27%|██▋       | 8/30 [00:36<00:55,  2.53s/it] 30%|███       | 9/30 [00:38<00:49,  2.38s/it] 33%|███▎      | 10/30 [00:40<00:46,  2.34s/it] 37%|███▋      | 11/30 [00:42<00:42,  2.25s/it] 40%|████      | 12/30 [00:44<00:39,  2.18s/it] 43%|████▎     | 13/30 [00:46<00:35,  2.11s/it] 47%|████▋     | 14/30 [00:48<00:32,  2.06s/it] 50%|█████     | 15/30 [00:50<00:30,  2.03s/it]                                                50%|█████     | 15/30 [00:50<00:30,  2.03s/it] 53%|█████▎    | 16/30 [00:52<00:28,  2.01s/it] 57%|█████▋    | 17/30 [00:54<00:25,  1.99s/it] 60%|██████    | 18/30 [00:56<00:23,  2.00s/it] 63%|██████▎   | 19/30 [00:58<00:22,  2.05s/it] 67%|██████▋   | 20/30 [01:00<00:20,  2.07s/it] 70%|███████   | 21/30 [01:03<00:18,  2.07s/it] 73%|███████▎  | 22/30 [01:05<00:16,  2.05s/it] 77%|███████▋  | 23/30 [01:07<00:14,  2.05s/it] 80%|████████  | 24/30 [01:09<00:12,  2.09s/it] 83%|████████▎ | 25/30 [01:11<00:10,  2.04s/it] 87%|████████▋ | 26/30 [01:13<00:08,  2.04s/it] 90%|█████████ | 27/30 [01:15<00:06,  2.02s/it] 93%|█████████▎| 28/30 [01:17<00:03,  1.99s/it] 97%|█████████▋| 29/30 [01:19<00:02,  2.08s/it]100%|██████████| 30/30 [01:21<00:00,  2.06s/it]                                               100%|██████████| 30/30 [01:21<00:00,  2.06s/it]                                               100%|██████████| 30/30 [01:21<00:00,  2.06s/it]100%|██████████| 30/30 [01:21<00:00,  2.72s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5837, 'grad_norm': 0.3413962423801422, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0773, 'grad_norm': 0.2879781126976013, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 81.4585, 'train_samples_per_second': 2.946, 'train_steps_per_second': 0.368, 'train_loss': 1.3304882049560547, 'epoch': 0.24}
END_PROFILE: 1747608104.8305392
Run  2: 83.845566421 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.08s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:29, 11.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:50, 19.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 24.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:21, 38.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:20, 39.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:13, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:17<00:08, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:22<00:03, 39.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:02, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 38.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 37.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 42.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:25<00:00, 42.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 43.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 38.00 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747608204.186445
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:51, 20.41s/it]  7%|▋         | 2/30 [00:22<04:27,  9.57s/it] 10%|█         | 3/30 [00:24<02:44,  6.10s/it] 13%|█▎        | 4/30 [00:26<01:56,  4.47s/it] 17%|█▋        | 5/30 [00:28<01:30,  3.62s/it] 20%|██        | 6/30 [00:30<01:13,  3.07s/it] 23%|██▎       | 7/30 [00:32<01:01,  2.69s/it] 27%|██▋       | 8/30 [00:34<00:54,  2.46s/it] 30%|███       | 9/30 [00:36<00:48,  2.32s/it] 33%|███▎      | 10/30 [00:38<00:45,  2.29s/it] 37%|███▋      | 11/30 [00:40<00:41,  2.21s/it] 40%|████      | 12/30 [00:42<00:38,  2.15s/it] 43%|████▎     | 13/30 [00:44<00:35,  2.08s/it] 47%|████▋     | 14/30 [00:46<00:32,  2.03s/it] 50%|█████     | 15/30 [00:48<00:30,  2.00s/it]                                                50%|█████     | 15/30 [00:48<00:30,  2.00s/it] 53%|█████▎    | 16/30 [00:50<00:27,  1.98s/it] 57%|█████▋    | 17/30 [00:52<00:25,  1.97s/it] 60%|██████    | 18/30 [00:54<00:23,  1.97s/it] 63%|██████▎   | 19/30 [00:56<00:22,  2.03s/it] 67%|██████▋   | 20/30 [00:58<00:20,  2.05s/it] 70%|███████   | 21/30 [01:00<00:18,  2.04s/it] 73%|███████▎  | 22/30 [01:02<00:16,  2.03s/it] 77%|███████▋  | 23/30 [01:04<00:14,  2.02s/it] 80%|████████  | 24/30 [01:06<00:12,  2.06s/it] 83%|████████▎ | 25/30 [01:08<00:10,  2.02s/it] 87%|████████▋ | 26/30 [01:10<00:08,  2.02s/it] 90%|█████████ | 27/30 [01:12<00:05,  1.99s/it] 93%|█████████▎| 28/30 [01:14<00:03,  1.97s/it] 97%|█████████▋| 29/30 [01:16<00:02,  2.05s/it]100%|██████████| 30/30 [01:18<00:00,  2.05s/it]                                               100%|██████████| 30/30 [01:18<00:00,  2.05s/it]                                               100%|██████████| 30/30 [01:18<00:00,  2.05s/it]100%|██████████| 30/30 [01:18<00:00,  2.62s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5835, 'grad_norm': 0.34106868505477905, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.077, 'grad_norm': 0.2883818447589874, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 78.714, 'train_samples_per_second': 3.049, 'train_steps_per_second': 0.381, 'train_loss': 1.3302751541137696, 'epoch': 0.24}
END_PROFILE: 1747608283.6781657
Run  3: 81.004811300 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 81.004811300 seconds
  Max elapsed : 83.881290057 seconds
  Avg elapsed : 82.91055592600000000000 seconds
===================================
--------------------------------------------
TORCH

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.22s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:28, 11.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:50, 19.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 25.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:21, 38.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:20, 38.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 38.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:14, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:13, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:18<00:08, 39.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:08, 39.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:22<00:03, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:02, 39.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 37.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 37.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 41.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:26<00:00, 41.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 42.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 37.94 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747608405.4285839
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<10:39, 22.04s/it]  7%|▋         | 2/30 [00:24<04:48, 10.31s/it] 10%|█         | 3/30 [00:26<02:57,  6.57s/it] 13%|█▎        | 4/30 [00:28<02:04,  4.80s/it] 17%|█▋        | 5/30 [00:30<01:36,  3.88s/it] 20%|██        | 6/30 [00:32<01:18,  3.28s/it] 23%|██▎       | 7/30 [00:34<01:06,  2.89s/it] 27%|██▋       | 8/30 [00:36<00:57,  2.63s/it] 30%|███       | 9/30 [00:39<00:52,  2.48s/it] 33%|███▎      | 10/30 [00:41<00:47,  2.37s/it] 37%|███▋      | 11/30 [00:43<00:43,  2.28s/it] 40%|████      | 12/30 [00:45<00:39,  2.21s/it] 43%|████▎     | 13/30 [00:47<00:36,  2.15s/it] 47%|████▋     | 14/30 [00:49<00:33,  2.12s/it] 50%|█████     | 15/30 [00:51<00:31,  2.11s/it]                                                50%|█████     | 15/30 [00:51<00:31,  2.11s/it] 53%|█████▎    | 16/30 [00:53<00:29,  2.11s/it] 57%|█████▋    | 17/30 [00:55<00:27,  2.10s/it] 60%|██████    | 18/30 [00:57<00:25,  2.12s/it] 63%|██████▎   | 19/30 [00:59<00:23,  2.11s/it] 67%|██████▋   | 20/30 [01:01<00:20,  2.10s/it] 70%|███████   | 21/30 [01:03<00:18,  2.09s/it] 73%|███████▎  | 22/30 [01:06<00:16,  2.09s/it] 77%|███████▋  | 23/30 [01:08<00:14,  2.09s/it] 80%|████████  | 24/30 [01:10<00:12,  2.10s/it] 83%|████████▎ | 25/30 [01:12<00:10,  2.09s/it] 87%|████████▋ | 26/30 [01:14<00:08,  2.10s/it] 90%|█████████ | 27/30 [01:16<00:06,  2.09s/it] 93%|█████████▎| 28/30 [01:18<00:04,  2.09s/it] 97%|█████████▋| 29/30 [01:20<00:02,  2.08s/it]100%|██████████| 30/30 [01:22<00:00,  2.07s/it]                                               100%|██████████| 30/30 [01:22<00:00,  2.07s/it]                                               100%|██████████| 30/30 [01:22<00:00,  2.07s/it]100%|██████████| 30/30 [01:22<00:00,  2.76s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5834, 'grad_norm': 0.34128865599632263, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0771, 'grad_norm': 0.2878531217575073, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 82.761, 'train_samples_per_second': 2.9, 'train_steps_per_second': 0.362, 'train_loss': 1.3302590052286785, 'epoch': 0.24}
END_PROFILE: 1747608504.657291
Run  1: 101.321100310 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.15s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:29, 11.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:50, 19.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:38, 25.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 29.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:28, 32.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 34.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:21, 38.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:20, 38.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:06<00:19, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:13, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:12<00:13, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:12, 39.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 38.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 38.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:17<00:08, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:07, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:22<00:03, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:02, 39.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 38.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 37.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 42.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:25<00:00, 41.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 43.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 38.03 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747608605.5468235
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<10:56, 22.62s/it]  7%|▋         | 2/30 [00:24<04:56, 10.58s/it] 10%|█         | 3/30 [00:26<03:01,  6.72s/it] 13%|█▎        | 4/30 [00:29<02:07,  4.90s/it] 17%|█▋        | 5/30 [00:31<01:38,  3.95s/it] 20%|██        | 6/30 [00:33<01:20,  3.34s/it] 23%|██▎       | 7/30 [00:35<01:07,  2.94s/it] 27%|██▋       | 8/30 [00:37<00:58,  2.68s/it] 30%|███       | 9/30 [00:39<00:52,  2.52s/it] 33%|███▎      | 10/30 [00:41<00:48,  2.41s/it] 37%|███▋      | 11/30 [00:44<00:43,  2.31s/it] 40%|████      | 12/30 [00:46<00:40,  2.24s/it] 43%|████▎     | 13/30 [00:48<00:37,  2.19s/it] 47%|████▋     | 14/30 [00:50<00:34,  2.16s/it] 50%|█████     | 15/30 [00:52<00:32,  2.14s/it]                                                50%|█████     | 15/30 [00:52<00:32,  2.14s/it] 53%|█████▎    | 16/30 [00:54<00:29,  2.13s/it] 57%|█████▋    | 17/30 [00:56<00:27,  2.12s/it] 60%|██████    | 18/30 [00:58<00:25,  2.13s/it] 63%|██████▎   | 19/30 [01:00<00:23,  2.13s/it] 67%|██████▋   | 20/30 [01:03<00:21,  2.12s/it] 70%|███████   | 21/30 [01:05<00:19,  2.12s/it] 73%|███████▎  | 22/30 [01:07<00:16,  2.11s/it] 77%|███████▋  | 23/30 [01:09<00:14,  2.11s/it] 80%|████████  | 24/30 [01:11<00:12,  2.13s/it] 83%|████████▎ | 25/30 [01:13<00:10,  2.12s/it] 87%|████████▋ | 26/30 [01:15<00:08,  2.13s/it] 90%|█████████ | 27/30 [01:17<00:06,  2.12s/it] 93%|█████████▎| 28/30 [01:19<00:04,  2.11s/it] 97%|█████████▋| 29/30 [01:22<00:02,  2.11s/it]100%|██████████| 30/30 [01:24<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.10s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.10s/it]100%|██████████| 30/30 [01:24<00:00,  2.80s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5839, 'grad_norm': 0.3408562242984772, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0774, 'grad_norm': 0.28781670331954956, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 84.1369, 'train_samples_per_second': 2.852, 'train_steps_per_second': 0.357, 'train_loss': 1.3306129455566407, 'epoch': 0.24}
END_PROFILE: 1747608706.4486365
Run  2: 103.054575491 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for gemma-3-4b...
==((====))==  Unsloth 2025.5.3: Fast Gemma3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.30s/it]
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Adding LoRA adapters for gemma-3-4b...
Unsloth: Making `model.base_model.model.language_model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:01<01:30, 10.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:01<00:51, 18.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:02<00:38, 24.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:02<00:32, 28.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:02<00:29, 31.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:03<00:26, 33.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:03<00:25, 35.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:03<00:24, 36.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:04<00:23, 37.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:04<00:22, 38.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:04<00:22, 38.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:05<00:21, 38.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:05<00:21, 38.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:05<00:20, 38.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:06<00:20, 39.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:06<00:19, 38.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:07<00:19, 39.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:07<00:19, 39.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:07<00:18, 39.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:08<00:18, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:08<00:17, 39.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:08<00:17, 39.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:09<00:17, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:09<00:16, 39.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:09<00:16, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:10<00:16, 39.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:10<00:15, 39.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:10<00:15, 39.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:11<00:15, 39.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:11<00:14, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:11<00:14, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:12<00:13, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:12<00:13, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:13<00:13, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:13<00:12, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:13<00:12, 39.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:14<00:12, 39.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:14<00:11, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:14<00:11, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:15<00:11, 39.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:15<00:10, 39.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:15<00:10, 39.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:16<00:10, 39.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:16<00:09, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:16<00:09, 39.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:17<00:09, 39.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:17<00:08, 39.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:18<00:08, 39.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:18<00:08, 39.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:18<00:07, 39.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:19<00:07, 39.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:19<00:06, 39.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:19<00:06, 39.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:20<00:06, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:20<00:05, 39.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:20<00:05, 39.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:21<00:05, 39.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:21<00:04, 39.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:21<00:04, 39.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:22<00:04, 39.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:22<00:03, 39.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:23<00:03, 39.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:23<00:03, 39.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:23<00:02, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:24<00:02, 38.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:24<00:02, 37.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:24<00:01, 37.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:25<00:01, 37.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:25<00:01, 36.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:25<00:00, 42.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:26<00:00, 42.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 42.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:26<00:00, 37.87 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 14,901,248/4,314,980,720 (0.35% trained)
START_PROFILE: 1747608809.4655373
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:24, 19.48s/it]  7%|▋         | 2/30 [00:21<04:20,  9.31s/it] 10%|█         | 3/30 [00:23<02:43,  6.07s/it] 13%|█▎        | 4/30 [00:26<01:57,  4.52s/it] 17%|█▋        | 5/30 [00:28<01:33,  3.74s/it] 20%|██        | 6/30 [00:30<01:17,  3.21s/it] 23%|██▎       | 7/30 [00:32<01:05,  2.87s/it] 27%|██▋       | 8/30 [00:34<00:58,  2.64s/it] 30%|███       | 9/30 [00:37<00:52,  2.51s/it] 33%|███▎      | 10/30 [00:39<00:48,  2.41s/it] 37%|███▋      | 11/30 [00:41<00:44,  2.33s/it] 40%|████      | 12/30 [00:43<00:40,  2.27s/it] 43%|████▎     | 13/30 [00:45<00:37,  2.22s/it] 47%|████▋     | 14/30 [00:47<00:35,  2.20s/it] 50%|█████     | 15/30 [00:50<00:32,  2.19s/it]                                                50%|█████     | 15/30 [00:50<00:32,  2.19s/it] 53%|█████▎    | 16/30 [00:52<00:30,  2.18s/it] 57%|█████▋    | 17/30 [00:54<00:28,  2.18s/it] 60%|██████    | 18/30 [00:56<00:26,  2.19s/it] 63%|██████▎   | 19/30 [00:58<00:23,  2.18s/it] 67%|██████▋   | 20/30 [01:00<00:21,  2.18s/it] 70%|███████   | 21/30 [01:03<00:19,  2.18s/it] 73%|███████▎  | 22/30 [01:05<00:17,  2.17s/it] 77%|███████▋  | 23/30 [01:07<00:15,  2.17s/it] 80%|████████  | 24/30 [01:09<00:13,  2.18s/it] 83%|████████▎ | 25/30 [01:11<00:10,  2.18s/it] 87%|████████▋ | 26/30 [01:13<00:08,  2.19s/it] 90%|█████████ | 27/30 [01:16<00:06,  2.18s/it] 93%|█████████▎| 28/30 [01:18<00:04,  2.17s/it] 97%|█████████▋| 29/30 [01:20<00:02,  2.17s/it]100%|██████████| 30/30 [01:22<00:00,  2.16s/it]                                               100%|██████████| 30/30 [01:22<00:00,  2.16s/it]                                               100%|██████████| 30/30 [01:22<00:00,  2.16s/it]100%|██████████| 30/30 [01:22<00:00,  2.75s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.5839, 'grad_norm': 0.34079164266586304, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0773, 'grad_norm': 0.28773319721221924, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 82.606, 'train_samples_per_second': 2.905, 'train_steps_per_second': 0.363, 'train_loss': 1.3305702209472656, 'epoch': 0.24}
END_PROFILE: 1747608908.44966
Run  3: 101.094716032 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 101.094716032 seconds
  Max elapsed : 103.054575491 seconds
  Avg elapsed : 101.82346394433333333333 seconds
===================================
--------------------------------------------
--------------------------------------------
Timing llama-3.1-8b with NONE

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.47s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:45, 21.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:25, 38.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 51.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 66.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 75.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 77.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 79.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 82.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 83.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 83.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 84.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 84.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 84.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 83.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 83.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 83.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 83.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 83.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 83.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 81.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 78.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 81.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.92 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747609029.0666642
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:16<07:45, 16.06s/it]  7%|▋         | 2/30 [00:17<03:24,  7.31s/it] 10%|█         | 3/30 [00:18<02:03,  4.57s/it] 13%|█▎        | 4/30 [00:19<01:23,  3.22s/it] 17%|█▋        | 5/30 [00:21<01:03,  2.53s/it] 20%|██        | 6/30 [00:22<00:50,  2.10s/it] 23%|██▎       | 7/30 [00:23<00:41,  1.80s/it] 27%|██▋       | 8/30 [00:24<00:35,  1.60s/it] 30%|███       | 9/30 [00:25<00:31,  1.49s/it] 33%|███▎      | 10/30 [00:27<00:27,  1.38s/it] 37%|███▋      | 11/30 [00:28<00:24,  1.31s/it] 40%|████      | 12/30 [00:29<00:22,  1.26s/it] 43%|████▎     | 13/30 [00:30<00:20,  1.23s/it] 47%|████▋     | 14/30 [00:31<00:19,  1.20s/it] 50%|█████     | 15/30 [00:32<00:17,  1.19s/it]                                                50%|█████     | 15/30 [00:32<00:17,  1.19s/it] 53%|█████▎    | 16/30 [00:33<00:16,  1.18s/it] 57%|█████▋    | 17/30 [00:35<00:15,  1.18s/it] 60%|██████    | 18/30 [00:36<00:14,  1.21s/it] 63%|██████▎   | 19/30 [00:37<00:13,  1.19s/it] 67%|██████▋   | 20/30 [00:38<00:11,  1.17s/it] 70%|███████   | 21/30 [00:39<00:10,  1.17s/it] 73%|███████▎  | 22/30 [00:40<00:09,  1.16s/it] 77%|███████▋  | 23/30 [00:42<00:08,  1.15s/it] 80%|████████  | 24/30 [00:43<00:07,  1.19s/it] 83%|████████▎ | 25/30 [00:44<00:05,  1.18s/it] 87%|████████▋ | 26/30 [00:45<00:04,  1.20s/it] 90%|█████████ | 27/30 [00:46<00:03,  1.19s/it] 93%|█████████▎| 28/30 [00:48<00:02,  1.18s/it] 97%|█████████▋| 29/30 [00:49<00:01,  1.17s/it]100%|██████████| 30/30 [00:50<00:00,  1.17s/it]                                               100%|██████████| 30/30 [00:50<00:00,  1.17s/it]                                               100%|██████████| 30/30 [00:50<00:00,  1.17s/it]100%|██████████| 30/30 [00:50<00:00,  1.68s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2542, 'grad_norm': 0.35248956084251404, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.037, 'grad_norm': 0.20221476256847382, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 50.3922, 'train_samples_per_second': 4.763, 'train_steps_per_second': 0.595, 'train_loss': 1.1456193606058755, 'epoch': 0.24}
END_PROFILE: 1747609079.9795384
Run  1: 52.365772825 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.48s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 76.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 76.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:11, 76.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:11, 76.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 75.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:10, 75.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:10, 75.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:03<00:10, 77.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 79.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 80.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 81.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 81.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 82.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:04<00:08, 82.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 82.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 83.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 83.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 83.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 82.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 82.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:05, 64.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 89.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 88.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 86.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 85.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 85.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 84.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 84.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 84.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 84.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 84.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 84.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 81.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 78.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 79.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.38 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747609175.2656496
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:27, 19.56s/it]  7%|▋         | 2/30 [00:20<04:05,  8.78s/it] 10%|█         | 3/30 [00:22<02:24,  5.37s/it] 13%|█▎        | 4/30 [00:23<01:36,  3.73s/it] 17%|█▋        | 5/30 [00:24<01:11,  2.87s/it] 20%|██        | 6/30 [00:25<00:56,  2.34s/it] 23%|██▎       | 7/30 [00:27<00:45,  1.97s/it] 27%|██▋       | 8/30 [00:28<00:38,  1.73s/it] 30%|███       | 9/30 [00:29<00:33,  1.59s/it] 33%|███▎      | 10/30 [00:30<00:29,  1.47s/it] 37%|███▋      | 11/30 [00:32<00:26,  1.39s/it] 40%|████      | 12/30 [00:33<00:23,  1.33s/it] 43%|████▎     | 13/30 [00:34<00:22,  1.30s/it] 47%|████▋     | 14/30 [00:35<00:20,  1.27s/it] 50%|█████     | 15/30 [00:36<00:18,  1.26s/it]                                                50%|█████     | 15/30 [00:36<00:18,  1.26s/it] 53%|█████▎    | 16/30 [00:38<00:17,  1.24s/it] 57%|█████▋    | 17/30 [00:39<00:15,  1.23s/it] 60%|██████    | 18/30 [00:40<00:14,  1.24s/it] 63%|██████▎   | 19/30 [00:41<00:13,  1.23s/it] 67%|██████▋   | 20/30 [00:43<00:12,  1.22s/it] 70%|███████   | 21/30 [00:44<00:10,  1.21s/it] 73%|███████▎  | 22/30 [00:45<00:09,  1.20s/it] 77%|███████▋  | 23/30 [00:46<00:08,  1.20s/it] 80%|████████  | 24/30 [00:47<00:07,  1.23s/it] 83%|████████▎ | 25/30 [00:49<00:06,  1.23s/it] 87%|████████▋ | 26/30 [00:50<00:04,  1.24s/it] 90%|█████████ | 27/30 [00:51<00:03,  1.24s/it] 93%|█████████▎| 28/30 [00:52<00:02,  1.22s/it] 97%|█████████▋| 29/30 [00:53<00:01,  1.21s/it]100%|██████████| 30/30 [00:55<00:00,  1.21s/it]                                               100%|██████████| 30/30 [00:55<00:00,  1.21s/it]                                               100%|██████████| 30/30 [00:55<00:00,  1.21s/it]100%|██████████| 30/30 [00:55<00:00,  1.84s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2541, 'grad_norm': 0.34640607237815857, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0369, 'grad_norm': 0.20216214656829834, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 55.2004, 'train_samples_per_second': 4.348, 'train_steps_per_second': 0.543, 'train_loss': 1.1454986572265624, 'epoch': 0.24}
END_PROFILE: 1747609230.9736323
Run  2: 57.173688030 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.49s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:44, 22.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:24, 39.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 61.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 68.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 72.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 75.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 79.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 82.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 84.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 84.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 84.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 84.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 84.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 83.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 83.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 83.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 84.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 84.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 84.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 79.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 82.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.38 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747609325.0877593
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:25, 19.51s/it]  7%|▋         | 2/30 [00:20<04:04,  8.71s/it] 10%|█         | 3/30 [00:21<02:23,  5.30s/it] 13%|█▎        | 4/30 [00:23<01:35,  3.65s/it] 17%|█▋        | 5/30 [00:24<01:10,  2.80s/it] 20%|██        | 6/30 [00:25<00:54,  2.27s/it] 23%|██▎       | 7/30 [00:26<00:43,  1.90s/it] 27%|██▋       | 8/30 [00:27<00:36,  1.66s/it] 30%|███       | 9/30 [00:29<00:31,  1.52s/it] 33%|███▎      | 10/30 [00:30<00:27,  1.40s/it] 37%|███▋      | 11/30 [00:31<00:25,  1.32s/it] 40%|████      | 12/30 [00:32<00:22,  1.25s/it] 43%|████▎     | 13/30 [00:33<00:20,  1.21s/it] 47%|████▋     | 14/30 [00:34<00:18,  1.18s/it] 50%|█████     | 15/30 [00:35<00:17,  1.17s/it]                                                50%|█████     | 15/30 [00:35<00:17,  1.17s/it] 53%|█████▎    | 16/30 [00:36<00:16,  1.16s/it] 57%|█████▋    | 17/30 [00:38<00:14,  1.15s/it] 60%|██████    | 18/30 [00:39<00:14,  1.17s/it] 63%|██████▎   | 19/30 [00:40<00:12,  1.15s/it] 67%|██████▋   | 20/30 [00:41<00:11,  1.14s/it] 70%|███████   | 21/30 [00:42<00:10,  1.14s/it] 73%|███████▎  | 22/30 [00:43<00:08,  1.12s/it] 77%|███████▋  | 23/30 [00:44<00:07,  1.12s/it] 80%|████████  | 24/30 [00:46<00:06,  1.16s/it] 83%|████████▎ | 25/30 [00:47<00:05,  1.15s/it] 87%|████████▋ | 26/30 [00:48<00:04,  1.17s/it] 90%|█████████ | 27/30 [00:49<00:03,  1.17s/it] 93%|█████████▎| 28/30 [00:50<00:02,  1.16s/it] 97%|█████████▋| 29/30 [00:51<00:01,  1.15s/it]100%|██████████| 30/30 [00:52<00:00,  1.14s/it]                                               100%|██████████| 30/30 [00:52<00:00,  1.14s/it]                                               100%|██████████| 30/30 [00:52<00:00,  1.14s/it]100%|██████████| 30/30 [00:52<00:00,  1.77s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2543, 'grad_norm': 0.34889939427375793, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0369, 'grad_norm': 0.2024800032377243, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 53.0003, 'train_samples_per_second': 4.528, 'train_steps_per_second': 0.566, 'train_loss': 1.145583724975586, 'epoch': 0.24}
END_PROFILE: 1747609378.5584843
Run  3: 54.868515857 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 52.365772825 seconds
  Max elapsed : 57.173688030 seconds
  Avg elapsed : 54.80265890400000000000 seconds
===================================
NSYS

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.48s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 59.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 65.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 70.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 76.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 78.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 79.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 80.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 81.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 81.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 82.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 82.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 82.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 82.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 82.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 82.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 82.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:04<00:08, 82.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 82.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 82.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:08, 82.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 82.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 82.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 83.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 82.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 82.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 82.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 83.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 82.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 82.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 82.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 82.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 82.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 82.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 82.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 82.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 82.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 82.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 83.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 82.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 82.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 82.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 82.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 82.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 82.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 82.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 82.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 82.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 82.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 82.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 82.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 82.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 82.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 82.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 82.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 82.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 82.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 80.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 79.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 78.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 78.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 78.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 83.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 85.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 88.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.45 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747609499.7477882
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:51, 18.32s/it]  7%|▋         | 2/30 [00:19<03:58,  8.52s/it] 10%|█         | 3/30 [00:21<02:26,  5.42s/it] 13%|█▎        | 4/30 [00:23<01:42,  3.94s/it] 17%|█▋        | 5/30 [00:25<01:19,  3.17s/it] 20%|██        | 6/30 [00:26<01:04,  2.68s/it] 23%|██▎       | 7/30 [00:28<00:54,  2.36s/it] 27%|██▋       | 8/30 [00:30<00:47,  2.14s/it] 30%|███       | 9/30 [00:32<00:42,  2.01s/it] 33%|███▎      | 10/30 [00:33<00:37,  1.90s/it] 37%|███▋      | 11/30 [00:35<00:34,  1.82s/it] 40%|████      | 12/30 [00:36<00:31,  1.77s/it] 43%|████▎     | 13/30 [00:38<00:29,  1.73s/it] 47%|████▋     | 14/30 [00:40<00:27,  1.71s/it] 50%|█████     | 15/30 [00:41<00:25,  1.71s/it]                                                50%|█████     | 15/30 [00:41<00:25,  1.71s/it] 53%|█████▎    | 16/30 [00:43<00:23,  1.71s/it] 57%|█████▋    | 17/30 [00:45<00:22,  1.70s/it] 60%|██████    | 18/30 [00:47<00:20,  1.71s/it] 63%|██████▎   | 19/30 [00:48<00:18,  1.69s/it] 67%|██████▋   | 20/30 [00:50<00:16,  1.68s/it] 70%|███████   | 21/30 [00:52<00:15,  1.67s/it] 73%|███████▎  | 22/30 [00:53<00:13,  1.67s/it] 77%|███████▋  | 23/30 [00:55<00:11,  1.67s/it] 80%|████████  | 24/30 [00:57<00:10,  1.69s/it] 83%|████████▎ | 25/30 [00:58<00:08,  1.69s/it] 87%|████████▋ | 26/30 [01:00<00:06,  1.70s/it] 90%|█████████ | 27/30 [01:02<00:05,  1.70s/it] 93%|█████████▎| 28/30 [01:03<00:03,  1.69s/it] 97%|█████████▋| 29/30 [01:05<00:01,  1.68s/it]100%|██████████| 30/30 [01:07<00:00,  1.68s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.68s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.68s/it]100%|██████████| 30/30 [01:07<00:00,  2.24s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2542, 'grad_norm': 0.34652021527290344, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.037, 'grad_norm': 0.20352472364902496, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 67.2297, 'train_samples_per_second': 3.57, 'train_steps_per_second': 0.446, 'train_loss': 1.1456239064534506, 'epoch': 0.24}
END_PROFILE: 1747609567.444972
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/llama-3.1-8b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-47d4.qdstrm'
[1/1] [0%                          ] nsys-report-5883.nsys-rep[1/1] [0%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [5%                          ] nsys-report-5883.nsys-rep[1/1] [8%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [5%                          ] nsys-report-5883.nsys-rep[1/1] [5%                          ] nsys-report-5883.nsys-rep[1/1] [6%                          ] nsys-report-5883.nsys-rep[1/1] [7%                          ] nsys-report-5883.nsys-rep[1/1] [8%                          ] nsys-report-5883.nsys-rep[1/1] [9%                          ] nsys-report-5883.nsys-rep[1/1] [10%                         ] nsys-report-5883.nsys-rep[1/1] [11%                         ] nsys-report-5883.nsys-rep[1/1] [12%                         ] nsys-report-5883.nsys-rep[1/1] [13%                         ] nsys-report-5883.nsys-rep[1/1] [14%                         ] nsys-report-5883.nsys-rep[1/1] [=15%                        ] nsys-report-5883.nsys-rep[1/1] [=16%                        ] nsys-report-5883.nsys-rep[1/1] [=17%                        ] nsys-report-5883.nsys-rep[1/1] [==18%                       ] nsys-report-5883.nsys-rep[1/1] [==19%                       ] nsys-report-5883.nsys-rep[1/1] [==20%                       ] nsys-report-5883.nsys-rep[1/1] [==21%                       ] nsys-report-5883.nsys-rep[1/1] [===22%                      ] nsys-report-5883.nsys-rep[1/1] [===23%                      ] nsys-report-5883.nsys-rep[1/1] [===24%                      ] nsys-report-5883.nsys-rep[1/1] [====25%                     ] nsys-report-5883.nsys-rep[1/1] [====26%                     ] nsys-report-5883.nsys-rep[1/1] [====27%                     ] nsys-report-5883.nsys-rep[1/1] [====28%                     ] nsys-report-5883.nsys-rep[1/1] [=====29%                    ] nsys-report-5883.nsys-rep[1/1] [=====30%                    ] nsys-report-5883.nsys-rep[1/1] [=====31%                    ] nsys-report-5883.nsys-rep[1/1] [=====32%                    ] nsys-report-5883.nsys-rep[1/1] [======33%                   ] nsys-report-5883.nsys-rep[1/1] [======34%                   ] nsys-report-5883.nsys-rep[1/1] [======35%                   ] nsys-report-5883.nsys-rep[1/1] [=======36%                  ] nsys-report-5883.nsys-rep[1/1] [=======37%                  ] nsys-report-5883.nsys-rep[1/1] [=======38%                  ] nsys-report-5883.nsys-rep[1/1] [=======39%                  ] nsys-report-5883.nsys-rep[1/1] [========40%                 ] nsys-report-5883.nsys-rep[1/1] [========41%                 ] nsys-report-5883.nsys-rep[1/1] [========42%                 ] nsys-report-5883.nsys-rep[1/1] [=========43%                ] nsys-report-5883.nsys-rep[1/1] [=========44%                ] nsys-report-5883.nsys-rep[1/1] [=========45%                ] nsys-report-5883.nsys-rep[1/1] [=========46%                ] nsys-report-5883.nsys-rep[1/1] [==========47%               ] nsys-report-5883.nsys-rep[1/1] [==========48%               ] nsys-report-5883.nsys-rep[1/1] [==========49%               ] nsys-report-5883.nsys-rep[1/1] [===========50%              ] nsys-report-5883.nsys-rep[1/1] [===========51%              ] nsys-report-5883.nsys-rep[1/1] [===========52%              ] nsys-report-5883.nsys-rep[1/1] [===========53%              ] nsys-report-5883.nsys-rep[1/1] [============54%             ] nsys-report-5883.nsys-rep[1/1] [============55%             ] nsys-report-5883.nsys-rep[1/1] [============56%             ] nsys-report-5883.nsys-rep[1/1] [============57%             ] nsys-report-5883.nsys-rep[1/1] [=============58%            ] nsys-report-5883.nsys-rep[1/1] [=============59%            ] nsys-report-5883.nsys-rep[1/1] [=============60%            ] nsys-report-5883.nsys-rep[1/1] [==============61%           ] nsys-report-5883.nsys-rep[1/1] [==============62%           ] nsys-report-5883.nsys-rep[1/1] [==============63%           ] nsys-report-5883.nsys-rep[1/1] [==============64%           ] nsys-report-5883.nsys-rep[1/1] [===============65%          ] nsys-report-5883.nsys-rep[1/1] [===============66%          ] nsys-report-5883.nsys-rep[1/1] [===============67%          ] nsys-report-5883.nsys-rep[1/1] [================68%         ] nsys-report-5883.nsys-rep[1/1] [================69%         ] nsys-report-5883.nsys-rep[1/1] [================70%         ] nsys-report-5883.nsys-rep[1/1] [================71%         ] nsys-report-5883.nsys-rep[1/1] [=================72%        ] nsys-report-5883.nsys-rep[1/1] [=================73%        ] nsys-report-5883.nsys-rep[1/1] [=================74%        ] nsys-report-5883.nsys-rep[1/1] [==================75%       ] nsys-report-5883.nsys-rep[1/1] [==================76%       ] nsys-report-5883.nsys-rep[1/1] [==================77%       ] nsys-report-5883.nsys-rep[1/1] [==================78%       ] nsys-report-5883.nsys-rep[1/1] [===================79%      ] nsys-report-5883.nsys-rep[1/1] [===================80%      ] nsys-report-5883.nsys-rep[1/1] [===================81%      ] nsys-report-5883.nsys-rep[1/1] [===================82%      ] nsys-report-5883.nsys-rep[1/1] [========================100%] nsys-report-5883.nsys-rep[1/1] [========================100%] nsys-report-5883.nsys-rep
Generated:
	/tmp/nsys-report-5883.nsys-rep
Run  1: 79.988695584 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:39<00:12, 12.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  9.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.51s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 51.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 59.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 65.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:13, 69.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 73.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 75.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 77.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 79.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 80.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 80.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:10, 81.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 81.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:03<00:09, 81.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 81.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 82.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 82.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 82.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 81.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:04<00:08, 82.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 82.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 82.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:08, 82.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 82.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 82.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 82.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 82.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 82.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:07, 82.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 82.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 82.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 82.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 82.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 81.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 82.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 82.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 82.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 82.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 82.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 82.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:05, 82.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 82.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 82.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 81.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 81.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 82.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 82.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 82.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 82.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 82.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 82.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 82.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 82.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 82.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 82.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 82.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 82.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 82.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 82.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 82.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 82.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 82.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 82.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 80.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 79.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 78.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:12<00:00, 78.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 77.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 81.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 83.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 78.92 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747609677.6529682
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:17<08:41, 18.00s/it]  7%|▋         | 2/30 [00:19<03:54,  8.37s/it] 10%|█         | 3/30 [00:21<02:23,  5.32s/it] 13%|█▎        | 4/30 [00:22<01:40,  3.86s/it] 17%|█▋        | 5/30 [00:24<01:17,  3.11s/it] 20%|██        | 6/30 [00:26<01:02,  2.62s/it] 23%|██▎       | 7/30 [00:28<00:52,  2.30s/it] 27%|██▋       | 8/30 [00:29<00:45,  2.09s/it] 30%|███       | 9/30 [00:31<00:41,  1.96s/it] 33%|███▎      | 10/30 [00:32<00:36,  1.85s/it] 37%|███▋      | 11/30 [00:34<00:34,  1.79s/it] 40%|████      | 12/30 [00:36<00:31,  1.74s/it] 43%|████▎     | 13/30 [00:37<00:28,  1.70s/it] 47%|████▋     | 14/30 [00:39<00:26,  1.67s/it] 50%|█████     | 15/30 [00:41<00:24,  1.66s/it]                                                50%|█████     | 15/30 [00:41<00:24,  1.66s/it] 53%|█████▎    | 16/30 [00:42<00:23,  1.65s/it] 57%|█████▋    | 17/30 [00:44<00:21,  1.64s/it] 60%|██████    | 18/30 [00:45<00:19,  1.65s/it] 63%|██████▎   | 19/30 [00:47<00:18,  1.64s/it] 67%|██████▋   | 20/30 [00:49<00:16,  1.63s/it] 70%|███████   | 21/30 [00:50<00:14,  1.63s/it] 73%|███████▎  | 22/30 [00:52<00:12,  1.62s/it] 77%|███████▋  | 23/30 [00:54<00:11,  1.62s/it] 80%|████████  | 24/30 [00:55<00:09,  1.64s/it] 83%|████████▎ | 25/30 [00:57<00:08,  1.64s/it] 87%|████████▋ | 26/30 [00:59<00:06,  1.65s/it] 90%|█████████ | 27/30 [01:00<00:04,  1.65s/it] 93%|█████████▎| 28/30 [01:02<00:03,  1.64s/it] 97%|█████████▋| 29/30 [01:03<00:01,  1.63s/it]100%|██████████| 30/30 [01:05<00:00,  1.62s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.62s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.62s/it]100%|██████████| 30/30 [01:05<00:00,  2.18s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2541, 'grad_norm': 0.34494030475616455, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0368, 'grad_norm': 0.20207270979881287, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 65.5174, 'train_samples_per_second': 3.663, 'train_steps_per_second': 0.458, 'train_loss': 1.1454635302225749, 'epoch': 0.24}
END_PROFILE: 1747609743.6434636
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/llama-3.1-8b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-c241.qdstrm'
[1/1] [0%                          ] nsys-report-9ae6.nsys-rep[1/1] [0%                          ] nsys-report-9ae6.nsys-rep[1/1] [6%                          ] nsys-report-9ae6.nsys-rep[1/1] [5%                          ] nsys-report-9ae6.nsys-rep[1/1] [8%                          ] nsys-report-9ae6.nsys-rep[1/1] [7%                          ] nsys-report-9ae6.nsys-rep[1/1] [6%                          ] nsys-report-9ae6.nsys-rep[1/1] [5%                          ] nsys-report-9ae6.nsys-rep[1/1] [5%                          ] nsys-report-9ae6.nsys-rep[1/1] [6%                          ] nsys-report-9ae6.nsys-rep[1/1] [7%                          ] nsys-report-9ae6.nsys-rep[1/1] [8%                          ] nsys-report-9ae6.nsys-rep[1/1] [9%                          ] nsys-report-9ae6.nsys-rep[1/1] [10%                         ] nsys-report-9ae6.nsys-rep[1/1] [11%                         ] nsys-report-9ae6.nsys-rep[1/1] [12%                         ] nsys-report-9ae6.nsys-rep[1/1] [13%                         ] nsys-report-9ae6.nsys-rep[1/1] [14%                         ] nsys-report-9ae6.nsys-rep[1/1] [=15%                        ] nsys-report-9ae6.nsys-rep[1/1] [=16%                        ] nsys-report-9ae6.nsys-rep[1/1] [=17%                        ] nsys-report-9ae6.nsys-rep[1/1] [==18%                       ] nsys-report-9ae6.nsys-rep[1/1] [==19%                       ] nsys-report-9ae6.nsys-rep[1/1] [==20%                       ] nsys-report-9ae6.nsys-rep[1/1] [==21%                       ] nsys-report-9ae6.nsys-rep[1/1] [===22%                      ] nsys-report-9ae6.nsys-rep[1/1] [===23%                      ] nsys-report-9ae6.nsys-rep[1/1] [===24%                      ] nsys-report-9ae6.nsys-rep[1/1] [====25%                     ] nsys-report-9ae6.nsys-rep[1/1] [====26%                     ] nsys-report-9ae6.nsys-rep[1/1] [====27%                     ] nsys-report-9ae6.nsys-rep[1/1] [====28%                     ] nsys-report-9ae6.nsys-rep[1/1] [=====29%                    ] nsys-report-9ae6.nsys-rep[1/1] [=====30%                    ] nsys-report-9ae6.nsys-rep[1/1] [=====31%                    ] nsys-report-9ae6.nsys-rep[1/1] [=====32%                    ] nsys-report-9ae6.nsys-rep[1/1] [======33%                   ] nsys-report-9ae6.nsys-rep[1/1] [======34%                   ] nsys-report-9ae6.nsys-rep[1/1] [======35%                   ] nsys-report-9ae6.nsys-rep[1/1] [=======36%                  ] nsys-report-9ae6.nsys-rep[1/1] [=======37%                  ] nsys-report-9ae6.nsys-rep[1/1] [=======38%                  ] nsys-report-9ae6.nsys-rep[1/1] [=======39%                  ] nsys-report-9ae6.nsys-rep[1/1] [========40%                 ] nsys-report-9ae6.nsys-rep[1/1] [========41%                 ] nsys-report-9ae6.nsys-rep[1/1] [========42%                 ] nsys-report-9ae6.nsys-rep[1/1] [=========43%                ] nsys-report-9ae6.nsys-rep[1/1] [=========44%                ] nsys-report-9ae6.nsys-rep[1/1] [=========45%                ] nsys-report-9ae6.nsys-rep[1/1] [=========46%                ] nsys-report-9ae6.nsys-rep[1/1] [==========47%               ] nsys-report-9ae6.nsys-rep[1/1] [==========48%               ] nsys-report-9ae6.nsys-rep[1/1] [==========49%               ] nsys-report-9ae6.nsys-rep[1/1] [===========50%              ] nsys-report-9ae6.nsys-rep[1/1] [===========51%              ] nsys-report-9ae6.nsys-rep[1/1] [===========52%              ] nsys-report-9ae6.nsys-rep[1/1] [===========53%              ] nsys-report-9ae6.nsys-rep[1/1] [============54%             ] nsys-report-9ae6.nsys-rep[1/1] [============55%             ] nsys-report-9ae6.nsys-rep[1/1] [============56%             ] nsys-report-9ae6.nsys-rep[1/1] [============57%             ] nsys-report-9ae6.nsys-rep[1/1] [=============58%            ] nsys-report-9ae6.nsys-rep[1/1] [=============59%            ] nsys-report-9ae6.nsys-rep[1/1] [=============60%            ] nsys-report-9ae6.nsys-rep[1/1] [==============61%           ] nsys-report-9ae6.nsys-rep[1/1] [==============62%           ] nsys-report-9ae6.nsys-rep[1/1] [==============63%           ] nsys-report-9ae6.nsys-rep[1/1] [==============64%           ] nsys-report-9ae6.nsys-rep[1/1] [===============65%          ] nsys-report-9ae6.nsys-rep[1/1] [===============66%          ] nsys-report-9ae6.nsys-rep[1/1] [===============67%          ] nsys-report-9ae6.nsys-rep[1/1] [================68%         ] nsys-report-9ae6.nsys-rep[1/1] [================69%         ] nsys-report-9ae6.nsys-rep[1/1] [================70%         ] nsys-report-9ae6.nsys-rep[1/1] [================71%         ] nsys-report-9ae6.nsys-rep[1/1] [=================72%        ] nsys-report-9ae6.nsys-rep[1/1] [=================73%        ] nsys-report-9ae6.nsys-rep[1/1] [=================74%        ] nsys-report-9ae6.nsys-rep[1/1] [==================75%       ] nsys-report-9ae6.nsys-rep[1/1] [==================76%       ] nsys-report-9ae6.nsys-rep[1/1] [==================77%       ] nsys-report-9ae6.nsys-rep[1/1] [==================78%       ] nsys-report-9ae6.nsys-rep[1/1] [===================79%      ] nsys-report-9ae6.nsys-rep[1/1] [===================80%      ] nsys-report-9ae6.nsys-rep[1/1] [===================81%      ] nsys-report-9ae6.nsys-rep[1/1] [===================82%      ] nsys-report-9ae6.nsys-rep[1/1] [========================100%] nsys-report-9ae6.nsys-rep[1/1] [========================100%] nsys-report-9ae6.nsys-rep
Generated:
	/tmp/nsys-report-9ae6.nsys-rep
Run  2: 78.319876359 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:39, 13.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.38s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 61.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 72.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 75.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 79.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 83.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:09, 83.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 84.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 84.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 84.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 84.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 84.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 85.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 85.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 85.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:06, 85.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 85.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 81.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 79.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 78.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 77.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 77.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:06, 76.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:06, 77.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 79.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 81.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 82.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 83.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 83.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 84.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 86.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 86.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 85.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 85.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 85.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 85.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 85.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 85.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 85.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 84.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 85.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 84.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 84.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 84.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 85.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 82.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 81.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 80.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 79.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 81.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 83.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.33 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747609851.2971256
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:43, 18.06s/it]  7%|▋         | 2/30 [00:19<03:55,  8.41s/it] 10%|█         | 3/30 [00:21<02:24,  5.37s/it] 13%|█▎        | 4/30 [00:23<01:41,  3.90s/it] 17%|█▋        | 5/30 [00:24<01:18,  3.15s/it] 20%|██        | 6/30 [00:26<01:03,  2.67s/it] 23%|██▎       | 7/30 [00:28<00:54,  2.35s/it] 27%|██▋       | 8/30 [00:30<00:47,  2.14s/it] 30%|███       | 9/30 [00:31<00:41,  2.00s/it] 33%|███▎      | 10/30 [00:33<00:37,  1.89s/it] 37%|███▋      | 11/30 [00:35<00:34,  1.82s/it] 40%|████      | 12/30 [00:36<00:31,  1.76s/it] 43%|████▎     | 13/30 [00:38<00:29,  1.72s/it] 47%|████▋     | 14/30 [00:39<00:27,  1.69s/it] 50%|█████     | 15/30 [00:41<00:25,  1.68s/it]                                                50%|█████     | 15/30 [00:41<00:25,  1.68s/it] 53%|█████▎    | 16/30 [00:43<00:23,  1.67s/it] 57%|█████▋    | 17/30 [00:44<00:21,  1.66s/it] 60%|██████    | 18/30 [00:46<00:20,  1.67s/it] 63%|██████▎   | 19/30 [00:48<00:18,  1.65s/it] 67%|██████▋   | 20/30 [00:49<00:16,  1.65s/it] 70%|███████   | 21/30 [00:51<00:14,  1.65s/it] 73%|███████▎  | 22/30 [00:53<00:13,  1.64s/it] 77%|███████▋  | 23/30 [00:54<00:11,  1.64s/it] 80%|████████  | 24/30 [00:56<00:09,  1.66s/it] 83%|████████▎ | 25/30 [00:58<00:08,  1.66s/it] 87%|████████▋ | 26/30 [00:59<00:06,  1.67s/it] 90%|█████████ | 27/30 [01:01<00:05,  1.67s/it] 93%|█████████▎| 28/30 [01:03<00:03,  1.66s/it] 97%|█████████▋| 29/30 [01:04<00:01,  1.65s/it]100%|██████████| 30/30 [01:06<00:00,  1.65s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.65s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.65s/it]100%|██████████| 30/30 [01:06<00:00,  2.21s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2542, 'grad_norm': 0.34460124373435974, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0372, 'grad_norm': 0.20248544216156006, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 66.3972, 'train_samples_per_second': 3.615, 'train_steps_per_second': 0.452, 'train_loss': 1.145705795288086, 'epoch': 0.24}
END_PROFILE: 1747609918.1761982
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/llama-3.1-8b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-c109.qdstrm'
[1/1] [0%                          ] nsys-report-fed3.nsys-rep[1/1] [0%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [5%                          ] nsys-report-fed3.nsys-rep[1/1] [8%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [5%                          ] nsys-report-fed3.nsys-rep[1/1] [5%                          ] nsys-report-fed3.nsys-rep[1/1] [6%                          ] nsys-report-fed3.nsys-rep[1/1] [7%                          ] nsys-report-fed3.nsys-rep[1/1] [8%                          ] nsys-report-fed3.nsys-rep[1/1] [9%                          ] nsys-report-fed3.nsys-rep[1/1] [10%                         ] nsys-report-fed3.nsys-rep[1/1] [11%                         ] nsys-report-fed3.nsys-rep[1/1] [12%                         ] nsys-report-fed3.nsys-rep[1/1] [13%                         ] nsys-report-fed3.nsys-rep[1/1] [14%                         ] nsys-report-fed3.nsys-rep[1/1] [=15%                        ] nsys-report-fed3.nsys-rep[1/1] [=16%                        ] nsys-report-fed3.nsys-rep[1/1] [=17%                        ] nsys-report-fed3.nsys-rep[1/1] [==18%                       ] nsys-report-fed3.nsys-rep[1/1] [==19%                       ] nsys-report-fed3.nsys-rep[1/1] [==20%                       ] nsys-report-fed3.nsys-rep[1/1] [==21%                       ] nsys-report-fed3.nsys-rep[1/1] [===22%                      ] nsys-report-fed3.nsys-rep[1/1] [===23%                      ] nsys-report-fed3.nsys-rep[1/1] [===24%                      ] nsys-report-fed3.nsys-rep[1/1] [====25%                     ] nsys-report-fed3.nsys-rep[1/1] [====26%                     ] nsys-report-fed3.nsys-rep[1/1] [====27%                     ] nsys-report-fed3.nsys-rep[1/1] [====28%                     ] nsys-report-fed3.nsys-rep[1/1] [=====29%                    ] nsys-report-fed3.nsys-rep[1/1] [=====30%                    ] nsys-report-fed3.nsys-rep[1/1] [=====31%                    ] nsys-report-fed3.nsys-rep[1/1] [=====32%                    ] nsys-report-fed3.nsys-rep[1/1] [======33%                   ] nsys-report-fed3.nsys-rep[1/1] [======34%                   ] nsys-report-fed3.nsys-rep[1/1] [======35%                   ] nsys-report-fed3.nsys-rep[1/1] [=======36%                  ] nsys-report-fed3.nsys-rep[1/1] [=======37%                  ] nsys-report-fed3.nsys-rep[1/1] [=======38%                  ] nsys-report-fed3.nsys-rep[1/1] [=======39%                  ] nsys-report-fed3.nsys-rep[1/1] [========40%                 ] nsys-report-fed3.nsys-rep[1/1] [========41%                 ] nsys-report-fed3.nsys-rep[1/1] [========42%                 ] nsys-report-fed3.nsys-rep[1/1] [=========43%                ] nsys-report-fed3.nsys-rep[1/1] [=========44%                ] nsys-report-fed3.nsys-rep[1/1] [=========45%                ] nsys-report-fed3.nsys-rep[1/1] [=========46%                ] nsys-report-fed3.nsys-rep[1/1] [==========47%               ] nsys-report-fed3.nsys-rep[1/1] [==========48%               ] nsys-report-fed3.nsys-rep[1/1] [==========49%               ] nsys-report-fed3.nsys-rep[1/1] [===========50%              ] nsys-report-fed3.nsys-rep[1/1] [===========51%              ] nsys-report-fed3.nsys-rep[1/1] [===========52%              ] nsys-report-fed3.nsys-rep[1/1] [===========53%              ] nsys-report-fed3.nsys-rep[1/1] [============54%             ] nsys-report-fed3.nsys-rep[1/1] [============55%             ] nsys-report-fed3.nsys-rep[1/1] [============56%             ] nsys-report-fed3.nsys-rep[1/1] [============57%             ] nsys-report-fed3.nsys-rep[1/1] [=============58%            ] nsys-report-fed3.nsys-rep[1/1] [=============59%            ] nsys-report-fed3.nsys-rep[1/1] [=============60%            ] nsys-report-fed3.nsys-rep[1/1] [==============61%           ] nsys-report-fed3.nsys-rep[1/1] [==============62%           ] nsys-report-fed3.nsys-rep[1/1] [==============63%           ] nsys-report-fed3.nsys-rep[1/1] [==============64%           ] nsys-report-fed3.nsys-rep[1/1] [===============65%          ] nsys-report-fed3.nsys-rep[1/1] [===============66%          ] nsys-report-fed3.nsys-rep[1/1] [===============67%          ] nsys-report-fed3.nsys-rep[1/1] [================68%         ] nsys-report-fed3.nsys-rep[1/1] [================69%         ] nsys-report-fed3.nsys-rep[1/1] [================70%         ] nsys-report-fed3.nsys-rep[1/1] [================71%         ] nsys-report-fed3.nsys-rep[1/1] [=================72%        ] nsys-report-fed3.nsys-rep[1/1] [=================73%        ] nsys-report-fed3.nsys-rep[1/1] [=================74%        ] nsys-report-fed3.nsys-rep[1/1] [==================75%       ] nsys-report-fed3.nsys-rep[1/1] [==================76%       ] nsys-report-fed3.nsys-rep[1/1] [==================77%       ] nsys-report-fed3.nsys-rep[1/1] [==================78%       ] nsys-report-fed3.nsys-rep[1/1] [===================79%      ] nsys-report-fed3.nsys-rep[1/1] [===================80%      ] nsys-report-fed3.nsys-rep[1/1] [===================81%      ] nsys-report-fed3.nsys-rep[1/1] [===================82%      ] nsys-report-fed3.nsys-rep[1/1] [========================100%] nsys-report-fed3.nsys-rep[1/1] [========================100%] nsys-report-fed3.nsys-rep
Generated:
	/tmp/nsys-report-fed3.nsys-rep
Run  3: 79.097801010 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 78.319876359 seconds
  Max elapsed : 79.988695584 seconds
  Avg elapsed : 79.13545765100000000000 seconds
===================================
--------------------------------------------
PROTON

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:38, 12.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.32s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 66.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 70.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 76.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 78.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 79.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 80.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 81.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:10, 81.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 82.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 82.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 82.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 82.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 82.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 82.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 82.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 82.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 83.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 83.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 82.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 83.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 82.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 82.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 82.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 82.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 82.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 82.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 82.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 82.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 83.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 83.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 83.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 83.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 83.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 83.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 83.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 83.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 83.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 83.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 83.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 83.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 81.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 79.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 78.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 79.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 78.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:01, 77.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 75.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 76.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 76.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 81.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 82.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.27 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747610047.0044224
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:56, 18.51s/it]  7%|▋         | 2/30 [00:20<03:57,  8.50s/it] 10%|█         | 3/30 [00:21<02:23,  5.33s/it] 13%|█▎        | 4/30 [00:23<01:38,  3.80s/it] 17%|█▋        | 5/30 [00:24<01:15,  3.01s/it] 20%|██        | 6/30 [00:26<01:00,  2.51s/it] 23%|██▎       | 7/30 [00:27<00:50,  2.18s/it] 27%|██▋       | 8/30 [00:29<00:42,  1.95s/it] 30%|███       | 9/30 [00:30<00:38,  1.82s/it] 33%|███▎      | 10/30 [00:32<00:34,  1.71s/it] 37%|███▋      | 11/30 [00:33<00:31,  1.65s/it] 40%|████      | 12/30 [00:35<00:29,  1.66s/it] 43%|████▎     | 13/30 [00:36<00:27,  1.61s/it] 47%|████▋     | 14/30 [00:38<00:25,  1.58s/it] 50%|█████     | 15/30 [00:39<00:23,  1.57s/it]                                                50%|█████     | 15/30 [00:39<00:23,  1.57s/it] 53%|█████▎    | 16/30 [00:41<00:21,  1.55s/it] 57%|█████▋    | 17/30 [00:42<00:19,  1.52s/it] 60%|██████    | 18/30 [00:44<00:18,  1.53s/it] 63%|██████▎   | 19/30 [00:45<00:16,  1.50s/it] 67%|██████▋   | 20/30 [00:47<00:14,  1.48s/it] 70%|███████   | 21/30 [00:48<00:13,  1.46s/it] 73%|███████▎  | 22/30 [00:50<00:11,  1.45s/it] 77%|███████▋  | 23/30 [00:51<00:10,  1.45s/it] 80%|████████  | 24/30 [00:53<00:09,  1.55s/it] 83%|████████▎ | 25/30 [00:54<00:07,  1.55s/it] 87%|████████▋ | 26/30 [00:56<00:06,  1.56s/it] 90%|█████████ | 27/30 [00:57<00:04,  1.55s/it] 93%|█████████▎| 28/30 [00:59<00:03,  1.52s/it] 97%|█████████▋| 29/30 [01:01<00:01,  1.54s/it]100%|██████████| 30/30 [01:02<00:00,  1.52s/it]                                               100%|██████████| 30/30 [01:02<00:00,  1.52s/it]                                               100%|██████████| 30/30 [01:02<00:00,  1.52s/it]100%|██████████| 30/30 [01:02<00:00,  2.08s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2541, 'grad_norm': 0.3441183567047119, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0366, 'grad_norm': 0.2041667252779007, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 62.4831, 'train_samples_per_second': 3.841, 'train_steps_per_second': 0.48, 'train_loss': 1.1453821182250976, 'epoch': 0.24}
END_PROFILE: 1747610110.2326062
Run  1: 64.757085990 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:38, 12.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.31s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:44, 22.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:24, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 51.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 61.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 72.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 75.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 79.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 82.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 82.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 82.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 83.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 83.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 83.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 83.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 82.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 82.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 82.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 82.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 82.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 83.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 83.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 83.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 83.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 83.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 83.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 83.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 83.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 83.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 82.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 82.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 82.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 83.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 82.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 83.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 81.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 78.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 78.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 80.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 85.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.71 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747610204.625603
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:16<08:10, 16.90s/it]  7%|▋         | 2/30 [00:18<03:40,  7.88s/it] 10%|█         | 3/30 [00:20<02:15,  5.03s/it] 13%|█▎        | 4/30 [00:21<01:34,  3.65s/it] 17%|█▋        | 5/30 [00:23<01:13,  2.94s/it] 20%|██        | 6/30 [00:24<00:59,  2.49s/it] 23%|██▎       | 7/30 [00:26<00:50,  2.20s/it] 27%|██▋       | 8/30 [00:28<00:43,  1.98s/it] 30%|███       | 9/30 [00:29<00:39,  1.86s/it] 33%|███▎      | 10/30 [00:31<00:35,  1.76s/it] 37%|███▋      | 11/30 [00:32<00:32,  1.69s/it] 40%|████      | 12/30 [00:34<00:30,  1.70s/it] 43%|████▎     | 13/30 [00:36<00:28,  1.67s/it] 47%|████▋     | 14/30 [00:37<00:26,  1.64s/it] 50%|█████     | 15/30 [00:39<00:24,  1.63s/it]                                                50%|█████     | 15/30 [00:39<00:24,  1.63s/it] 53%|█████▎    | 16/30 [00:40<00:22,  1.61s/it] 57%|█████▋    | 17/30 [00:42<00:20,  1.60s/it] 60%|██████    | 18/30 [00:43<00:19,  1.60s/it] 63%|██████▎   | 19/30 [00:45<00:17,  1.57s/it] 67%|██████▋   | 20/30 [00:46<00:15,  1.55s/it] 70%|███████   | 21/30 [00:48<00:13,  1.54s/it] 73%|███████▎  | 22/30 [00:49<00:12,  1.53s/it] 77%|███████▋  | 23/30 [00:51<00:10,  1.53s/it] 80%|████████  | 24/30 [00:53<00:09,  1.63s/it] 83%|████████▎ | 25/30 [00:55<00:08,  1.63s/it] 87%|████████▋ | 26/30 [00:56<00:06,  1.64s/it] 90%|█████████ | 27/30 [00:58<00:04,  1.64s/it] 93%|█████████▎| 28/30 [00:59<00:03,  1.61s/it] 97%|█████████▋| 29/30 [01:01<00:01,  1.64s/it]100%|██████████| 30/30 [01:03<00:00,  1.61s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.61s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.61s/it]100%|██████████| 30/30 [01:03<00:00,  2.10s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2542, 'grad_norm': 0.3573744595050812, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0373, 'grad_norm': 0.20210397243499756, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 63.1067, 'train_samples_per_second': 3.803, 'train_steps_per_second': 0.475, 'train_loss': 1.1457708358764649, 'epoch': 0.24}
END_PROFILE: 1747610268.504513
Run  2: 65.361672025 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:38, 12.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.29s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:56, 17.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 66.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 76.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 78.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 82.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 84.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 84.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 84.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 83.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 83.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 83.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 83.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 83.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 83.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 83.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 84.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 83.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 83.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 83.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 81.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 79.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 79.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 88.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.16 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747610363.5809846
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:14, 19.14s/it]  7%|▋         | 2/30 [00:20<04:05,  8.78s/it] 10%|█         | 3/30 [00:22<02:28,  5.50s/it] 13%|█▎        | 4/30 [00:23<01:41,  3.92s/it] 17%|█▋        | 5/30 [00:25<01:17,  3.10s/it] 20%|██        | 6/30 [00:26<01:02,  2.59s/it] 23%|██▎       | 7/30 [00:28<00:51,  2.25s/it] 27%|██▋       | 8/30 [00:30<00:44,  2.01s/it] 30%|███       | 9/30 [00:31<00:39,  1.87s/it] 33%|███▎      | 10/30 [00:33<00:35,  1.76s/it] 37%|███▋      | 11/30 [00:34<00:31,  1.68s/it] 40%|████      | 12/30 [00:36<00:30,  1.68s/it] 43%|████▎     | 13/30 [00:37<00:27,  1.64s/it] 47%|████▋     | 14/30 [00:39<00:25,  1.60s/it] 50%|█████     | 15/30 [00:40<00:23,  1.59s/it]                                                50%|█████     | 15/30 [00:40<00:23,  1.59s/it] 53%|█████▎    | 16/30 [00:42<00:21,  1.57s/it] 57%|█████▋    | 17/30 [00:43<00:20,  1.55s/it] 60%|██████    | 18/30 [00:45<00:18,  1.55s/it] 63%|██████▎   | 19/30 [00:46<00:16,  1.53s/it] 67%|██████▋   | 20/30 [00:48<00:15,  1.52s/it] 70%|███████   | 21/30 [00:49<00:13,  1.51s/it] 73%|███████▎  | 22/30 [00:51<00:11,  1.50s/it] 77%|███████▋  | 23/30 [00:52<00:10,  1.49s/it] 80%|████████  | 24/30 [00:54<00:09,  1.58s/it] 83%|████████▎ | 25/30 [00:56<00:07,  1.58s/it] 87%|████████▋ | 26/30 [00:57<00:06,  1.59s/it] 90%|█████████ | 27/30 [00:59<00:04,  1.59s/it] 93%|█████████▎| 28/30 [01:00<00:03,  1.56s/it] 97%|█████████▋| 29/30 [01:02<00:01,  1.58s/it]100%|██████████| 30/30 [01:04<00:00,  1.56s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.56s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.56s/it]100%|██████████| 30/30 [01:04<00:00,  2.14s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2543, 'grad_norm': 0.34182268381118774, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0373, 'grad_norm': 0.20326179265975952, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 64.1224, 'train_samples_per_second': 3.743, 'train_steps_per_second': 0.468, 'train_loss': 1.1457786560058594, 'epoch': 0.24}
END_PROFILE: 1747610428.450344
Run  3: 66.343471526 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 64.757085990 seconds
  Max elapsed : 66.343471526 seconds
  Avg elapsed : 65.48740984700000000000 seconds
===================================
--------------------------------------------
TORCH

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  9.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.50s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:45, 21.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:24, 39.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 51.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 61.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 72.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 75.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 80.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 82.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 83.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 84.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 84.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 83.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 84.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 84.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 83.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 83.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 83.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 84.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 83.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 83.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 83.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 80.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 77.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 77.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 85.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.18 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747610545.3040352
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:21<10:10, 21.07s/it]  7%|▋         | 2/30 [00:22<04:30,  9.67s/it] 10%|█         | 3/30 [00:24<02:43,  6.06s/it] 13%|█▎        | 4/30 [00:26<01:52,  4.32s/it] 17%|█▋        | 5/30 [00:27<01:25,  3.42s/it] 20%|██        | 6/30 [00:29<01:08,  2.85s/it] 23%|██▎       | 7/30 [00:31<00:57,  2.48s/it] 27%|██▋       | 8/30 [00:33<00:49,  2.24s/it] 30%|███       | 9/30 [00:34<00:43,  2.07s/it] 33%|███▎      | 10/30 [00:36<00:38,  1.94s/it] 37%|███▋      | 11/30 [00:38<00:35,  1.85s/it] 40%|████      | 12/30 [00:39<00:31,  1.78s/it] 43%|████▎     | 13/30 [00:41<00:29,  1.73s/it] 47%|████▋     | 14/30 [00:43<00:27,  1.70s/it] 50%|█████     | 15/30 [00:44<00:25,  1.68s/it]                                                50%|█████     | 15/30 [00:44<00:25,  1.68s/it] 53%|█████▎    | 16/30 [00:46<00:23,  1.67s/it] 57%|█████▋    | 17/30 [00:47<00:21,  1.65s/it] 60%|██████    | 18/30 [00:49<00:20,  1.67s/it] 63%|██████▎   | 19/30 [00:51<00:18,  1.65s/it] 67%|██████▋   | 20/30 [00:52<00:16,  1.64s/it] 70%|███████   | 21/30 [00:54<00:14,  1.64s/it] 73%|███████▎  | 22/30 [00:56<00:13,  1.63s/it] 77%|███████▋  | 23/30 [00:57<00:11,  1.63s/it] 80%|████████  | 24/30 [00:59<00:09,  1.65s/it] 83%|████████▎ | 25/30 [01:01<00:08,  1.65s/it] 87%|████████▋ | 26/30 [01:02<00:06,  1.66s/it] 90%|█████████ | 27/30 [01:04<00:04,  1.66s/it] 93%|█████████▎| 28/30 [01:06<00:03,  1.65s/it] 97%|█████████▋| 29/30 [01:07<00:01,  1.64s/it]100%|██████████| 30/30 [01:09<00:00,  1.64s/it]                                               100%|██████████| 30/30 [01:09<00:00,  1.64s/it]                                               100%|██████████| 30/30 [01:09<00:00,  1.64s/it]100%|██████████| 30/30 [01:09<00:00,  2.31s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2543, 'grad_norm': 0.3916000425815582, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0374, 'grad_norm': 0.20223772525787354, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 69.345, 'train_samples_per_second': 3.461, 'train_steps_per_second': 0.433, 'train_loss': 1.145851453145345, 'epoch': 0.24}
END_PROFILE: 1747610628.2375143
Run  1: 84.816105804 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.49s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:45, 21.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:25, 38.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:19, 50.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 59.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 65.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 70.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 73.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 76.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:02<00:11, 77.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 78.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 79.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 78.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:10, 77.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:10, 76.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:03<00:10, 75.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:10, 75.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:10, 75.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 75.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:09, 75.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:04<00:09, 77.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:04<00:08, 78.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 79.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 80.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:08, 80.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:08, 81.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:05<00:07, 81.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 81.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 81.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 81.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:07, 81.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 81.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:06<00:06, 81.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 81.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 81.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 81.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 81.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 81.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:07<00:05, 81.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 81.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 81.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 81.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:05, 82.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 82.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:08<00:04, 82.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 82.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 82.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 82.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 82.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 82.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:09<00:03, 82.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 82.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 82.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 82.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 82.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 82.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:10<00:02, 82.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 82.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 82.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 82.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 82.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:11<00:01, 82.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:11<00:01, 82.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 82.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 82.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 80.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 79.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:12<00:00, 78.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:12<00:00, 77.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 77.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 81.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 77.78 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747610723.1362572
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:17<08:35, 17.78s/it]  7%|▋         | 2/30 [00:19<03:52,  8.31s/it] 10%|█         | 3/30 [00:21<02:23,  5.32s/it] 13%|█▎        | 4/30 [00:22<01:40,  3.88s/it] 17%|█▋        | 5/30 [00:24<01:18,  3.15s/it] 20%|██        | 6/30 [00:26<01:04,  2.69s/it] 23%|██▎       | 7/30 [00:28<00:54,  2.36s/it] 27%|██▋       | 8/30 [00:29<00:47,  2.14s/it] 30%|███       | 9/30 [00:31<00:42,  2.01s/it] 33%|███▎      | 10/30 [00:33<00:38,  1.91s/it] 37%|███▋      | 11/30 [00:34<00:34,  1.83s/it] 40%|████      | 12/30 [00:36<00:31,  1.78s/it] 43%|████▎     | 13/30 [00:38<00:29,  1.74s/it] 47%|████▋     | 14/30 [00:39<00:27,  1.71s/it] 50%|█████     | 15/30 [00:41<00:25,  1.70s/it]                                                50%|█████     | 15/30 [00:41<00:25,  1.70s/it] 53%|█████▎    | 16/30 [00:43<00:23,  1.69s/it] 57%|█████▋    | 17/30 [00:44<00:21,  1.68s/it] 60%|██████    | 18/30 [00:46<00:20,  1.69s/it] 63%|██████▎   | 19/30 [00:48<00:18,  1.68s/it] 67%|██████▋   | 20/30 [00:49<00:16,  1.67s/it] 70%|███████   | 21/30 [00:51<00:15,  1.67s/it] 73%|███████▎  | 22/30 [00:53<00:13,  1.66s/it] 77%|███████▋  | 23/30 [00:54<00:11,  1.66s/it] 80%|████████  | 24/30 [00:56<00:10,  1.68s/it] 83%|████████▎ | 25/30 [00:58<00:08,  1.68s/it] 87%|████████▋ | 26/30 [01:00<00:06,  1.70s/it] 90%|█████████ | 27/30 [01:01<00:05,  1.70s/it] 93%|█████████▎| 28/30 [01:03<00:03,  1.68s/it] 97%|█████████▋| 29/30 [01:05<00:01,  1.67s/it]100%|██████████| 30/30 [01:06<00:00,  1.67s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.67s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.67s/it]100%|██████████| 30/30 [01:06<00:00,  2.22s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2541, 'grad_norm': 0.3538573384284973, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0371, 'grad_norm': 0.20263728499412537, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 66.6896, 'train_samples_per_second': 3.599, 'train_steps_per_second': 0.45, 'train_loss': 1.14562037785848, 'epoch': 0.24}
END_PROFILE: 1747610803.573234
Run  2: 82.304833304 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.1-8b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  8.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.46s/it]
Adding LoRA adapters for llama-3.1-8b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:29, 32.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:14, 66.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 70.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 74.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 77.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 80.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 82.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 82.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 82.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 82.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 82.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 82.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 82.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 82.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 82.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 82.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 83.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 83.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 82.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 82.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 83.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 83.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 82.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 82.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 83.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 83.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 83.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 83.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 83.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 82.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 83.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 83.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 83.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 83.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 83.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 83.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 81.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 78.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 78.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 81.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.78 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/8,051,232,768 (0.26% trained)
START_PROFILE: 1747610897.923533
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:45, 20.19s/it]  7%|▋         | 2/30 [00:21<04:19,  9.25s/it] 10%|█         | 3/30 [00:23<02:36,  5.81s/it] 13%|█▎        | 4/30 [00:25<01:47,  4.14s/it] 17%|█▋        | 5/30 [00:26<01:21,  3.27s/it] 20%|██        | 6/30 [00:28<01:05,  2.73s/it] 23%|██▎       | 7/30 [00:30<00:54,  2.36s/it] 27%|██▋       | 8/30 [00:31<00:46,  2.12s/it] 30%|███       | 9/30 [00:33<00:41,  1.97s/it] 33%|███▎      | 10/30 [00:34<00:37,  1.85s/it] 37%|███▋      | 11/30 [00:36<00:33,  1.77s/it] 40%|████      | 12/30 [00:38<00:30,  1.71s/it] 43%|████▎     | 13/30 [00:39<00:28,  1.67s/it] 47%|████▋     | 14/30 [00:41<00:26,  1.64s/it] 50%|█████     | 15/30 [00:42<00:24,  1.62s/it]                                                50%|█████     | 15/30 [00:42<00:24,  1.62s/it] 53%|█████▎    | 16/30 [00:44<00:22,  1.61s/it] 57%|█████▋    | 17/30 [00:45<00:20,  1.60s/it] 60%|██████    | 18/30 [00:47<00:19,  1.62s/it] 63%|██████▎   | 19/30 [00:49<00:17,  1.60s/it] 67%|██████▋   | 20/30 [00:50<00:16,  1.60s/it] 70%|███████   | 21/30 [00:52<00:14,  1.60s/it] 73%|███████▎  | 22/30 [00:53<00:12,  1.59s/it] 77%|███████▋  | 23/30 [00:55<00:11,  1.58s/it] 80%|████████  | 24/30 [00:57<00:09,  1.61s/it] 83%|████████▎ | 25/30 [00:58<00:08,  1.60s/it] 87%|████████▋ | 26/30 [01:00<00:06,  1.62s/it] 90%|█████████ | 27/30 [01:02<00:04,  1.61s/it] 93%|█████████▎| 28/30 [01:03<00:03,  1.60s/it] 97%|█████████▋| 29/30 [01:05<00:01,  1.59s/it]100%|██████████| 30/30 [01:06<00:00,  1.60s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.60s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.60s/it]100%|██████████| 30/30 [01:06<00:00,  2.23s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.2545, 'grad_norm': 0.34946712851524353, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.0368, 'grad_norm': 0.20272770524024963, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 66.7856, 'train_samples_per_second': 3.594, 'train_steps_per_second': 0.449, 'train_loss': 1.1456667900085449, 'epoch': 0.24}
END_PROFILE: 1747610978.384025
Run  3: 82.482096832 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 82.304833304 seconds
  Max elapsed : 84.816105804 seconds
  Avg elapsed : 83.20101198000000000000 seconds
===================================
--------------------------------------------
--------------------------------------------
Timing llama-3.2-3b with NONE

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:54, 17.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 75.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 79.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 82.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 84.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 83.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 83.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 84.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 84.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 84.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 83.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 83.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 84.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 84.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 84.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 83.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 84.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 84.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 84.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 81.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 80.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 79.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 82.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 88.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.56 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747611071.2812023
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:53, 18.38s/it]  7%|▋         | 2/30 [00:19<03:50,  8.22s/it] 10%|█         | 3/30 [00:20<02:15,  5.01s/it] 13%|█▎        | 4/30 [00:21<01:30,  3.49s/it] 17%|█▋        | 5/30 [00:23<01:06,  2.67s/it] 20%|██        | 6/30 [00:24<00:51,  2.16s/it] 23%|██▎       | 7/30 [00:25<00:41,  1.80s/it] 27%|██▋       | 8/30 [00:26<00:34,  1.57s/it] 30%|███       | 9/30 [00:27<00:29,  1.42s/it] 33%|███▎      | 10/30 [00:28<00:26,  1.32s/it] 37%|███▋      | 11/30 [00:29<00:23,  1.24s/it] 40%|████      | 12/30 [00:30<00:21,  1.19s/it] 43%|████▎     | 13/30 [00:31<00:19,  1.15s/it] 47%|████▋     | 14/30 [00:32<00:17,  1.12s/it] 50%|█████     | 15/30 [00:33<00:16,  1.10s/it]                                                50%|█████     | 15/30 [00:33<00:16,  1.10s/it] 53%|█████▎    | 16/30 [00:34<00:15,  1.10s/it] 57%|█████▋    | 17/30 [00:35<00:14,  1.09s/it] 60%|██████    | 18/30 [00:37<00:13,  1.10s/it] 63%|██████▎   | 19/30 [00:38<00:11,  1.08s/it] 67%|██████▋   | 20/30 [00:39<00:10,  1.08s/it] 70%|███████   | 21/30 [00:40<00:09,  1.07s/it] 73%|███████▎  | 22/30 [00:41<00:08,  1.06s/it] 77%|███████▋  | 23/30 [00:42<00:07,  1.07s/it] 80%|████████  | 24/30 [00:43<00:06,  1.09s/it] 83%|████████▎ | 25/30 [00:44<00:05,  1.08s/it] 87%|████████▋ | 26/30 [00:45<00:04,  1.09s/it] 90%|█████████ | 27/30 [00:46<00:03,  1.08s/it] 93%|█████████▎| 28/30 [00:47<00:02,  1.07s/it] 97%|█████████▋| 29/30 [00:48<00:01,  1.07s/it]100%|██████████| 30/30 [00:49<00:00,  1.07s/it]                                               100%|██████████| 30/30 [00:49<00:00,  1.07s/it]                                               100%|██████████| 30/30 [00:49<00:00,  1.07s/it]100%|██████████| 30/30 [00:49<00:00,  1.67s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3604, 'grad_norm': 0.360908180475235, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.173, 'grad_norm': 0.19790935516357422, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 49.9819, 'train_samples_per_second': 4.802, 'train_steps_per_second': 0.6, 'train_loss': 1.2666877110799153, 'epoch': 0.24}
END_PROFILE: 1747611121.7426004
Run  1: 51.949942354 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 66.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 77.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 79.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 82.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 83.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 84.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 84.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 84.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 84.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 84.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 84.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 84.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 84.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 84.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 84.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 83.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 83.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 84.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 84.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 84.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 83.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 83.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 84.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 81.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 80.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 79.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 82.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 89.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.56 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747611196.384653
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:51, 20.38s/it]  7%|▋         | 2/30 [00:21<04:11,  9.00s/it] 10%|█         | 3/30 [00:22<02:25,  5.40s/it] 13%|█▎        | 4/30 [00:23<01:35,  3.69s/it] 17%|█▋        | 5/30 [00:24<01:09,  2.77s/it] 20%|██        | 6/30 [00:25<00:52,  2.19s/it] 23%|██▎       | 7/30 [00:26<00:41,  1.80s/it] 27%|██▋       | 8/30 [00:27<00:33,  1.54s/it] 30%|███       | 9/30 [00:28<00:29,  1.39s/it] 33%|███▎      | 10/30 [00:29<00:25,  1.27s/it] 37%|███▋      | 11/30 [00:30<00:22,  1.19s/it] 40%|████      | 12/30 [00:31<00:20,  1.13s/it] 43%|████▎     | 13/30 [00:32<00:18,  1.09s/it] 47%|████▋     | 14/30 [00:33<00:16,  1.06s/it] 50%|█████     | 15/30 [00:34<00:15,  1.04s/it]                                                50%|█████     | 15/30 [00:34<00:15,  1.04s/it] 53%|█████▎    | 16/30 [00:35<00:14,  1.03s/it] 57%|█████▋    | 17/30 [00:36<00:13,  1.02s/it] 60%|██████    | 18/30 [00:37<00:12,  1.03s/it] 63%|██████▎   | 19/30 [00:38<00:11,  1.01s/it] 67%|██████▋   | 20/30 [00:39<00:10,  1.01s/it] 70%|███████   | 21/30 [00:40<00:08,  1.00it/s] 73%|███████▎  | 22/30 [00:41<00:07,  1.01it/s] 77%|███████▋  | 23/30 [00:42<00:06,  1.01it/s] 80%|████████  | 24/30 [00:43<00:06,  1.00s/it] 83%|████████▎ | 25/30 [00:44<00:04,  1.00it/s] 87%|████████▋ | 26/30 [00:45<00:04,  1.01s/it] 90%|█████████ | 27/30 [00:46<00:03,  1.01s/it] 93%|█████████▎| 28/30 [00:47<00:01,  1.00it/s] 97%|█████████▋| 29/30 [00:48<00:00,  1.01it/s]100%|██████████| 30/30 [00:49<00:00,  1.01it/s]                                               100%|██████████| 30/30 [00:49<00:00,  1.01it/s]                                               100%|██████████| 30/30 [00:49<00:00,  1.01it/s]100%|██████████| 30/30 [00:49<00:00,  1.66s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3602, 'grad_norm': 0.3494498133659363, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1729, 'grad_norm': 0.19230078160762787, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 49.7791, 'train_samples_per_second': 4.821, 'train_steps_per_second': 0.603, 'train_loss': 1.2665803909301758, 'epoch': 0.24}
END_PROFILE: 1747611246.6341033
Run  2: 51.740104533 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:54, 18.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:17, 53.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 61.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 72.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 75.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 80.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 83.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:09, 83.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 84.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 84.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 84.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 84.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:08, 85.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 84.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 84.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:07, 85.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 85.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 84.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 84.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 84.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 84.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 84.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 84.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 84.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 84.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 84.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 85.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 85.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 85.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 85.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 85.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 84.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 85.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 85.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 84.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 85.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 85.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 85.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 82.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 80.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 79.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 78.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 77.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:02, 78.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 80.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 81.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 61.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 86.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 84.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 82.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 81.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 80.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 82.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 87.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 90.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.53 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747611315.792875
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:27, 19.56s/it]  7%|▋         | 2/30 [00:20<04:03,  8.68s/it] 10%|█         | 3/30 [00:21<02:21,  5.24s/it] 13%|█▎        | 4/30 [00:22<01:33,  3.60s/it] 17%|█▋        | 5/30 [00:24<01:08,  2.74s/it] 20%|██        | 6/30 [00:25<00:52,  2.19s/it] 23%|██▎       | 7/30 [00:26<00:41,  1.81s/it] 27%|██▋       | 8/30 [00:27<00:34,  1.58s/it] 30%|███       | 9/30 [00:28<00:29,  1.43s/it] 33%|███▎      | 10/30 [00:29<00:26,  1.31s/it] 37%|███▋      | 11/30 [00:30<00:23,  1.24s/it] 40%|████      | 12/30 [00:31<00:21,  1.18s/it] 43%|████▎     | 13/30 [00:32<00:19,  1.16s/it] 47%|████▋     | 14/30 [00:33<00:17,  1.12s/it] 50%|█████     | 15/30 [00:34<00:16,  1.10s/it]                                                50%|█████     | 15/30 [00:34<00:16,  1.10s/it] 53%|█████▎    | 16/30 [00:35<00:15,  1.10s/it] 57%|█████▋    | 17/30 [00:36<00:14,  1.08s/it] 60%|██████    | 18/30 [00:38<00:13,  1.09s/it] 63%|██████▎   | 19/30 [00:39<00:11,  1.08s/it] 67%|██████▋   | 20/30 [00:40<00:10,  1.07s/it] 70%|███████   | 21/30 [00:41<00:09,  1.06s/it] 73%|███████▎  | 22/30 [00:42<00:08,  1.05s/it] 77%|███████▋  | 23/30 [00:43<00:07,  1.05s/it] 80%|████████  | 24/30 [00:44<00:06,  1.06s/it] 83%|████████▎ | 25/30 [00:45<00:05,  1.06s/it] 87%|████████▋ | 26/30 [00:46<00:04,  1.07s/it] 90%|█████████ | 27/30 [00:47<00:03,  1.07s/it] 93%|█████████▎| 28/30 [00:48<00:02,  1.06s/it] 97%|█████████▋| 29/30 [00:49<00:01,  1.05s/it]100%|██████████| 30/30 [00:50<00:00,  1.05s/it]                                               100%|██████████| 30/30 [00:50<00:00,  1.05s/it]                                               100%|██████████| 30/30 [00:50<00:00,  1.05s/it]100%|██████████| 30/30 [00:50<00:00,  1.69s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3602, 'grad_norm': 0.35360845923423767, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1731, 'grad_norm': 0.19458450376987457, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 50.6575, 'train_samples_per_second': 4.738, 'train_steps_per_second': 0.592, 'train_loss': 1.2666379928588867, 'epoch': 0.24}
END_PROFILE: 1747611366.9044492
Run  3: 52.514110495 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 51.740104533 seconds
  Max elapsed : 52.514110495 seconds
  Avg elapsed : 52.06805246066666666666 seconds
===================================
NSYS

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:49, 20.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:27, 35.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:20, 47.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:16, 56.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 64.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:13, 70.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 73.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 74.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:02<00:11, 75.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:11, 77.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 79.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 81.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:10, 79.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:10, 78.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:03<00:09, 79.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 81.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 81.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 82.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:04<00:08, 84.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 84.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 84.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 84.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 84.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 84.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 84.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 84.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 84.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 84.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 84.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 84.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 84.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 84.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 84.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 84.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 84.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 84.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 84.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 84.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 81.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 80.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 80.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 79.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 82.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 86.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 90.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.68 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747611460.4221683
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:37, 19.92s/it]  7%|▋         | 2/30 [00:21<04:13,  9.07s/it] 10%|█         | 3/30 [00:22<02:31,  5.63s/it] 13%|█▎        | 4/30 [00:24<01:44,  4.00s/it] 17%|█▋        | 5/30 [00:26<01:18,  3.13s/it] 20%|██        | 6/30 [00:27<01:01,  2.58s/it] 23%|██▎       | 7/30 [00:28<00:50,  2.20s/it] 27%|██▋       | 8/30 [00:30<00:43,  1.96s/it] 30%|███       | 9/30 [00:31<00:37,  1.80s/it] 33%|███▎      | 10/30 [00:33<00:33,  1.68s/it] 37%|███▋      | 11/30 [00:34<00:30,  1.61s/it] 40%|████      | 12/30 [00:36<00:27,  1.55s/it] 43%|████▎     | 13/30 [00:37<00:25,  1.51s/it] 47%|████▋     | 14/30 [00:38<00:23,  1.48s/it] 50%|█████     | 15/30 [00:40<00:21,  1.46s/it]                                                50%|█████     | 15/30 [00:40<00:21,  1.46s/it] 53%|█████▎    | 16/30 [00:41<00:20,  1.45s/it] 57%|█████▋    | 17/30 [00:43<00:18,  1.44s/it] 60%|██████    | 18/30 [00:44<00:17,  1.46s/it] 63%|██████▎   | 19/30 [00:46<00:15,  1.44s/it] 67%|██████▋   | 20/30 [00:47<00:14,  1.44s/it] 70%|███████   | 21/30 [00:48<00:12,  1.42s/it] 73%|███████▎  | 22/30 [00:50<00:11,  1.42s/it] 77%|███████▋  | 23/30 [00:51<00:09,  1.42s/it] 80%|████████  | 24/30 [00:53<00:08,  1.43s/it] 83%|████████▎ | 25/30 [00:54<00:07,  1.43s/it] 87%|████████▋ | 26/30 [00:56<00:05,  1.44s/it] 90%|█████████ | 27/30 [00:57<00:04,  1.44s/it] 93%|█████████▎| 28/30 [00:58<00:02,  1.43s/it] 97%|█████████▋| 29/30 [01:00<00:01,  1.43s/it]100%|██████████| 30/30 [01:01<00:00,  1.42s/it]                                               100%|██████████| 30/30 [01:01<00:00,  1.42s/it]                                               100%|██████████| 30/30 [01:01<00:00,  1.42s/it]100%|██████████| 30/30 [01:01<00:00,  2.06s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3603, 'grad_norm': 0.35606589913368225, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1731, 'grad_norm': 0.20047029852867126, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 61.8051, 'train_samples_per_second': 3.883, 'train_steps_per_second': 0.485, 'train_loss': 1.2666728337605795, 'epoch': 0.24}
END_PROFILE: 1747611522.6959276
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/llama-3.2-3b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-a47e.qdstrm'
[1/1] [0%                          ] nsys-report-dd5b.nsys-rep[1/1] [0%                          ] nsys-report-dd5b.nsys-rep[1/1] [0%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [5%                          ] nsys-report-dd5b.nsys-rep[1/1] [8%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [5%                          ] nsys-report-dd5b.nsys-rep[1/1] [5%                          ] nsys-report-dd5b.nsys-rep[1/1] [6%                          ] nsys-report-dd5b.nsys-rep[1/1] [7%                          ] nsys-report-dd5b.nsys-rep[1/1] [8%                          ] nsys-report-dd5b.nsys-rep[1/1] [9%                          ] nsys-report-dd5b.nsys-rep[1/1] [10%                         ] nsys-report-dd5b.nsys-rep[1/1] [11%                         ] nsys-report-dd5b.nsys-rep[1/1] [12%                         ] nsys-report-dd5b.nsys-rep[1/1] [13%                         ] nsys-report-dd5b.nsys-rep[1/1] [14%                         ] nsys-report-dd5b.nsys-rep[1/1] [=15%                        ] nsys-report-dd5b.nsys-rep[1/1] [=16%                        ] nsys-report-dd5b.nsys-rep[1/1] [=17%                        ] nsys-report-dd5b.nsys-rep[1/1] [==18%                       ] nsys-report-dd5b.nsys-rep[1/1] [==19%                       ] nsys-report-dd5b.nsys-rep[1/1] [==20%                       ] nsys-report-dd5b.nsys-rep[1/1] [==21%                       ] nsys-report-dd5b.nsys-rep[1/1] [===22%                      ] nsys-report-dd5b.nsys-rep[1/1] [===23%                      ] nsys-report-dd5b.nsys-rep[1/1] [===24%                      ] nsys-report-dd5b.nsys-rep[1/1] [====25%                     ] nsys-report-dd5b.nsys-rep[1/1] [====26%                     ] nsys-report-dd5b.nsys-rep[1/1] [====27%                     ] nsys-report-dd5b.nsys-rep[1/1] [====28%                     ] nsys-report-dd5b.nsys-rep[1/1] [=====29%                    ] nsys-report-dd5b.nsys-rep[1/1] [=====30%                    ] nsys-report-dd5b.nsys-rep[1/1] [=====31%                    ] nsys-report-dd5b.nsys-rep[1/1] [=====32%                    ] nsys-report-dd5b.nsys-rep[1/1] [======33%                   ] nsys-report-dd5b.nsys-rep[1/1] [======34%                   ] nsys-report-dd5b.nsys-rep[1/1] [======35%                   ] nsys-report-dd5b.nsys-rep[1/1] [=======36%                  ] nsys-report-dd5b.nsys-rep[1/1] [=======37%                  ] nsys-report-dd5b.nsys-rep[1/1] [=======38%                  ] nsys-report-dd5b.nsys-rep[1/1] [=======39%                  ] nsys-report-dd5b.nsys-rep[1/1] [========40%                 ] nsys-report-dd5b.nsys-rep[1/1] [========41%                 ] nsys-report-dd5b.nsys-rep[1/1] [========42%                 ] nsys-report-dd5b.nsys-rep[1/1] [=========43%                ] nsys-report-dd5b.nsys-rep[1/1] [=========44%                ] nsys-report-dd5b.nsys-rep[1/1] [=========45%                ] nsys-report-dd5b.nsys-rep[1/1] [=========46%                ] nsys-report-dd5b.nsys-rep[1/1] [==========47%               ] nsys-report-dd5b.nsys-rep[1/1] [==========48%               ] nsys-report-dd5b.nsys-rep[1/1] [==========49%               ] nsys-report-dd5b.nsys-rep[1/1] [===========50%              ] nsys-report-dd5b.nsys-rep[1/1] [===========51%              ] nsys-report-dd5b.nsys-rep[1/1] [===========52%              ] nsys-report-dd5b.nsys-rep[1/1] [===========53%              ] nsys-report-dd5b.nsys-rep[1/1] [============54%             ] nsys-report-dd5b.nsys-rep[1/1] [============55%             ] nsys-report-dd5b.nsys-rep[1/1] [============56%             ] nsys-report-dd5b.nsys-rep[1/1] [============57%             ] nsys-report-dd5b.nsys-rep[1/1] [=============58%            ] nsys-report-dd5b.nsys-rep[1/1] [=============59%            ] nsys-report-dd5b.nsys-rep[1/1] [=============60%            ] nsys-report-dd5b.nsys-rep[1/1] [==============61%           ] nsys-report-dd5b.nsys-rep[1/1] [==============62%           ] nsys-report-dd5b.nsys-rep[1/1] [==============63%           ] nsys-report-dd5b.nsys-rep[1/1] [==============64%           ] nsys-report-dd5b.nsys-rep[1/1] [===============65%          ] nsys-report-dd5b.nsys-rep[1/1] [===============66%          ] nsys-report-dd5b.nsys-rep[1/1] [===============67%          ] nsys-report-dd5b.nsys-rep[1/1] [================68%         ] nsys-report-dd5b.nsys-rep[1/1] [================69%         ] nsys-report-dd5b.nsys-rep[1/1] [================70%         ] nsys-report-dd5b.nsys-rep[1/1] [================71%         ] nsys-report-dd5b.nsys-rep[1/1] [=================72%        ] nsys-report-dd5b.nsys-rep[1/1] [=================73%        ] nsys-report-dd5b.nsys-rep[1/1] [=================74%        ] nsys-report-dd5b.nsys-rep[1/1] [==================75%       ] nsys-report-dd5b.nsys-rep[1/1] [==================76%       ] nsys-report-dd5b.nsys-rep[1/1] [==================77%       ] nsys-report-dd5b.nsys-rep[1/1] [==================78%       ] nsys-report-dd5b.nsys-rep[1/1] [===================79%      ] nsys-report-dd5b.nsys-rep[1/1] [===================80%      ] nsys-report-dd5b.nsys-rep[1/1] [===================81%      ] nsys-report-dd5b.nsys-rep[1/1] [===================82%      ] nsys-report-dd5b.nsys-rep[1/1] [========================100%] nsys-report-dd5b.nsys-rep[1/1] [========================100%] nsys-report-dd5b.nsys-rep
Generated:
	/tmp/nsys-report-dd5b.nsys-rep
Run  1: 73.074333373 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 72.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 76.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 78.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 80.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 82.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:09, 83.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 84.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:08, 84.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 85.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 85.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 85.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:07, 85.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 84.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 85.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 85.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 85.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 85.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:06, 85.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 85.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 85.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 85.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 85.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 85.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:05, 85.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 85.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 84.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 84.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 84.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 84.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 84.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 85.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 85.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 85.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 85.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 85.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 85.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 85.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 85.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 85.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 84.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 84.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 84.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 84.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 84.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 84.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 84.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:10<00:01, 84.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 81.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 80.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 80.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 79.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:11<00:00, 82.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 86.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 81.27 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747611600.1974938
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:44, 18.08s/it]  7%|▋         | 2/30 [00:19<03:52,  8.29s/it] 10%|█         | 3/30 [00:21<02:19,  5.18s/it] 13%|█▎        | 4/30 [00:22<01:36,  3.71s/it] 17%|█▋        | 5/30 [00:24<01:13,  2.93s/it] 20%|██        | 6/30 [00:25<00:58,  2.44s/it] 23%|██▎       | 7/30 [00:26<00:48,  2.09s/it] 27%|██▋       | 8/30 [00:28<00:41,  1.88s/it] 30%|███       | 9/30 [00:29<00:36,  1.75s/it] 33%|███▎      | 10/30 [00:31<00:32,  1.64s/it] 37%|███▋      | 11/30 [00:32<00:29,  1.57s/it] 40%|████      | 12/30 [00:33<00:27,  1.52s/it] 43%|████▎     | 13/30 [00:35<00:25,  1.48s/it] 47%|████▋     | 14/30 [00:36<00:23,  1.45s/it] 50%|█████     | 15/30 [00:38<00:21,  1.44s/it]                                                50%|█████     | 15/30 [00:38<00:21,  1.44s/it] 53%|█████▎    | 16/30 [00:39<00:20,  1.43s/it] 57%|█████▋    | 17/30 [00:40<00:18,  1.42s/it] 60%|██████    | 18/30 [00:42<00:17,  1.44s/it] 63%|██████▎   | 19/30 [00:43<00:15,  1.42s/it] 67%|██████▋   | 20/30 [00:45<00:14,  1.42s/it] 70%|███████   | 21/30 [00:46<00:12,  1.40s/it] 73%|███████▎  | 22/30 [00:47<00:11,  1.40s/it] 77%|███████▋  | 23/30 [00:49<00:09,  1.40s/it] 80%|████████  | 24/30 [00:50<00:08,  1.42s/it] 83%|████████▎ | 25/30 [00:52<00:07,  1.42s/it] 87%|████████▋ | 26/30 [00:53<00:05,  1.42s/it] 90%|█████████ | 27/30 [00:55<00:04,  1.42s/it] 93%|█████████▎| 28/30 [00:56<00:02,  1.41s/it] 97%|█████████▋| 29/30 [00:57<00:01,  1.40s/it]100%|██████████| 30/30 [00:59<00:00,  1.40s/it]                                               100%|██████████| 30/30 [00:59<00:00,  1.40s/it]                                               100%|██████████| 30/30 [00:59<00:00,  1.40s/it]100%|██████████| 30/30 [00:59<00:00,  1.98s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3602, 'grad_norm': 0.3545737564563751, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1731, 'grad_norm': 0.20156513154506683, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 59.261, 'train_samples_per_second': 4.05, 'train_steps_per_second': 0.506, 'train_loss': 1.2666381200154622, 'epoch': 0.24}
END_PROFILE: 1747611659.9333243
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/llama-3.2-3b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-875c.qdstrm'
[1/1] [0%                          ] nsys-report-6db0.nsys-rep[1/1] [0%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [5%                          ] nsys-report-6db0.nsys-rep[1/1] [8%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [5%                          ] nsys-report-6db0.nsys-rep[1/1] [5%                          ] nsys-report-6db0.nsys-rep[1/1] [6%                          ] nsys-report-6db0.nsys-rep[1/1] [7%                          ] nsys-report-6db0.nsys-rep[1/1] [8%                          ] nsys-report-6db0.nsys-rep[1/1] [9%                          ] nsys-report-6db0.nsys-rep[1/1] [10%                         ] nsys-report-6db0.nsys-rep[1/1] [11%                         ] nsys-report-6db0.nsys-rep[1/1] [12%                         ] nsys-report-6db0.nsys-rep[1/1] [13%                         ] nsys-report-6db0.nsys-rep[1/1] [14%                         ] nsys-report-6db0.nsys-rep[1/1] [=15%                        ] nsys-report-6db0.nsys-rep[1/1] [=16%                        ] nsys-report-6db0.nsys-rep[1/1] [=17%                        ] nsys-report-6db0.nsys-rep[1/1] [==18%                       ] nsys-report-6db0.nsys-rep[1/1] [==19%                       ] nsys-report-6db0.nsys-rep[1/1] [==20%                       ] nsys-report-6db0.nsys-rep[1/1] [==21%                       ] nsys-report-6db0.nsys-rep[1/1] [===22%                      ] nsys-report-6db0.nsys-rep[1/1] [===23%                      ] nsys-report-6db0.nsys-rep[1/1] [===24%                      ] nsys-report-6db0.nsys-rep[1/1] [====25%                     ] nsys-report-6db0.nsys-rep[1/1] [====26%                     ] nsys-report-6db0.nsys-rep[1/1] [====27%                     ] nsys-report-6db0.nsys-rep[1/1] [====28%                     ] nsys-report-6db0.nsys-rep[1/1] [=====29%                    ] nsys-report-6db0.nsys-rep[1/1] [=====30%                    ] nsys-report-6db0.nsys-rep[1/1] [=====31%                    ] nsys-report-6db0.nsys-rep[1/1] [=====32%                    ] nsys-report-6db0.nsys-rep[1/1] [======33%                   ] nsys-report-6db0.nsys-rep[1/1] [======34%                   ] nsys-report-6db0.nsys-rep[1/1] [======35%                   ] nsys-report-6db0.nsys-rep[1/1] [=======36%                  ] nsys-report-6db0.nsys-rep[1/1] [=======37%                  ] nsys-report-6db0.nsys-rep[1/1] [=======38%                  ] nsys-report-6db0.nsys-rep[1/1] [=======39%                  ] nsys-report-6db0.nsys-rep[1/1] [========40%                 ] nsys-report-6db0.nsys-rep[1/1] [========41%                 ] nsys-report-6db0.nsys-rep[1/1] [========42%                 ] nsys-report-6db0.nsys-rep[1/1] [=========43%                ] nsys-report-6db0.nsys-rep[1/1] [=========44%                ] nsys-report-6db0.nsys-rep[1/1] [=========45%                ] nsys-report-6db0.nsys-rep[1/1] [=========46%                ] nsys-report-6db0.nsys-rep[1/1] [==========47%               ] nsys-report-6db0.nsys-rep[1/1] [==========48%               ] nsys-report-6db0.nsys-rep[1/1] [==========49%               ] nsys-report-6db0.nsys-rep[1/1] [===========50%              ] nsys-report-6db0.nsys-rep[1/1] [===========51%              ] nsys-report-6db0.nsys-rep[1/1] [===========52%              ] nsys-report-6db0.nsys-rep[1/1] [===========53%              ] nsys-report-6db0.nsys-rep[1/1] [============54%             ] nsys-report-6db0.nsys-rep[1/1] [============55%             ] nsys-report-6db0.nsys-rep[1/1] [============56%             ] nsys-report-6db0.nsys-rep[1/1] [============57%             ] nsys-report-6db0.nsys-rep[1/1] [=============58%            ] nsys-report-6db0.nsys-rep[1/1] [=============59%            ] nsys-report-6db0.nsys-rep[1/1] [=============60%            ] nsys-report-6db0.nsys-rep[1/1] [==============61%           ] nsys-report-6db0.nsys-rep[1/1] [==============62%           ] nsys-report-6db0.nsys-rep[1/1] [==============63%           ] nsys-report-6db0.nsys-rep[1/1] [==============64%           ] nsys-report-6db0.nsys-rep[1/1] [===============65%          ] nsys-report-6db0.nsys-rep[1/1] [===============66%          ] nsys-report-6db0.nsys-rep[1/1] [===============67%          ] nsys-report-6db0.nsys-rep[1/1] [================68%         ] nsys-report-6db0.nsys-rep[1/1] [================69%         ] nsys-report-6db0.nsys-rep[1/1] [================70%         ] nsys-report-6db0.nsys-rep[1/1] [================71%         ] nsys-report-6db0.nsys-rep[1/1] [=================72%        ] nsys-report-6db0.nsys-rep[1/1] [=================73%        ] nsys-report-6db0.nsys-rep[1/1] [=================74%        ] nsys-report-6db0.nsys-rep[1/1] [==================75%       ] nsys-report-6db0.nsys-rep[1/1] [==================76%       ] nsys-report-6db0.nsys-rep[1/1] [==================77%       ] nsys-report-6db0.nsys-rep[1/1] [==================78%       ] nsys-report-6db0.nsys-rep[1/1] [===================79%      ] nsys-report-6db0.nsys-rep[1/1] [===================80%      ] nsys-report-6db0.nsys-rep[1/1] [===================81%      ] nsys-report-6db0.nsys-rep[1/1] [===================82%      ] nsys-report-6db0.nsys-rep[1/1] [========================100%] nsys-report-6db0.nsys-rep[1/1] [========================100%] nsys-report-6db0.nsys-rep
Generated:
	/tmp/nsys-report-6db0.nsys-rep
Run  2: 70.631186519 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:29, 33.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:20, 45.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:16, 55.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 63.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:13, 69.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 73.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 76.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:02<00:11, 78.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 82.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:03<00:09, 82.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 83.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:04<00:08, 83.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:05, 101.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 78.97 examples/s] Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 80.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 81.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:04, 98.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 78.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 79.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:05, 80.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 81.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 82.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:03, 101.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:04, 78.27 examples/s] Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 79.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 81.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 81.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 82.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 82.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 83.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 83.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 84.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 100.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 95.11 examples/s] Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 89.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 86.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 87.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 92.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.21 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747611741.6902492
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:16<07:58, 16.51s/it]  7%|▋         | 2/30 [00:17<03:34,  7.67s/it] 10%|█         | 3/30 [00:19<02:12,  4.90s/it] 13%|█▎        | 4/30 [00:21<01:32,  3.57s/it] 17%|█▋        | 5/30 [00:22<01:11,  2.86s/it] 20%|██        | 6/30 [00:24<00:57,  2.41s/it] 23%|██▎       | 7/30 [00:25<00:48,  2.09s/it] 27%|██▋       | 8/30 [00:27<00:41,  1.89s/it] 30%|███       | 9/30 [00:28<00:37,  1.77s/it] 33%|███▎      | 10/30 [00:30<00:33,  1.67s/it] 37%|███▋      | 11/30 [00:31<00:30,  1.61s/it] 40%|████      | 12/30 [00:33<00:28,  1.56s/it] 43%|████▎     | 13/30 [00:34<00:25,  1.52s/it] 47%|████▋     | 14/30 [00:35<00:23,  1.50s/it] 50%|█████     | 15/30 [00:37<00:22,  1.49s/it]                                                50%|█████     | 15/30 [00:37<00:22,  1.49s/it] 53%|█████▎    | 16/30 [00:38<00:20,  1.48s/it] 57%|█████▋    | 17/30 [00:40<00:19,  1.47s/it] 60%|██████    | 18/30 [00:41<00:17,  1.49s/it] 63%|██████▎   | 19/30 [00:43<00:16,  1.48s/it] 67%|██████▋   | 20/30 [00:44<00:14,  1.47s/it] 70%|███████   | 21/30 [00:46<00:13,  1.46s/it] 73%|███████▎  | 22/30 [00:47<00:11,  1.45s/it] 77%|███████▋  | 23/30 [00:49<00:10,  1.45s/it] 80%|████████  | 24/30 [00:50<00:08,  1.47s/it] 83%|████████▎ | 25/30 [00:51<00:07,  1.46s/it] 87%|████████▋ | 26/30 [00:53<00:05,  1.47s/it] 90%|█████████ | 27/30 [00:54<00:04,  1.47s/it] 93%|█████████▎| 28/30 [00:56<00:02,  1.46s/it] 97%|█████████▋| 29/30 [00:57<00:01,  1.45s/it]100%|██████████| 30/30 [00:59<00:00,  1.45s/it]                                               100%|██████████| 30/30 [00:59<00:00,  1.45s/it]                                               100%|██████████| 30/30 [00:59<00:00,  1.45s/it]100%|██████████| 30/30 [00:59<00:00,  1.98s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3602, 'grad_norm': 0.3545711934566498, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.173, 'grad_norm': 0.20725473761558533, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 59.2587, 'train_samples_per_second': 4.05, 'train_steps_per_second': 0.506, 'train_loss': 1.2666048049926757, 'epoch': 0.24}
END_PROFILE: 1747611801.4120555
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/llama-3.2-3b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-f05d.qdstrm'
[1/1] [0%                          ] nsys-report-2c63.nsys-rep[1/1] [0%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [5%                          ] nsys-report-2c63.nsys-rep[1/1] [8%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [5%                          ] nsys-report-2c63.nsys-rep[1/1] [5%                          ] nsys-report-2c63.nsys-rep[1/1] [6%                          ] nsys-report-2c63.nsys-rep[1/1] [7%                          ] nsys-report-2c63.nsys-rep[1/1] [8%                          ] nsys-report-2c63.nsys-rep[1/1] [9%                          ] nsys-report-2c63.nsys-rep[1/1] [10%                         ] nsys-report-2c63.nsys-rep[1/1] [11%                         ] nsys-report-2c63.nsys-rep[1/1] [12%                         ] nsys-report-2c63.nsys-rep[1/1] [13%                         ] nsys-report-2c63.nsys-rep[1/1] [14%                         ] nsys-report-2c63.nsys-rep[1/1] [=15%                        ] nsys-report-2c63.nsys-rep[1/1] [=16%                        ] nsys-report-2c63.nsys-rep[1/1] [=17%                        ] nsys-report-2c63.nsys-rep[1/1] [==18%                       ] nsys-report-2c63.nsys-rep[1/1] [==19%                       ] nsys-report-2c63.nsys-rep[1/1] [==20%                       ] nsys-report-2c63.nsys-rep[1/1] [==21%                       ] nsys-report-2c63.nsys-rep[1/1] [===22%                      ] nsys-report-2c63.nsys-rep[1/1] [===23%                      ] nsys-report-2c63.nsys-rep[1/1] [===24%                      ] nsys-report-2c63.nsys-rep[1/1] [====25%                     ] nsys-report-2c63.nsys-rep[1/1] [====26%                     ] nsys-report-2c63.nsys-rep[1/1] [====27%                     ] nsys-report-2c63.nsys-rep[1/1] [====28%                     ] nsys-report-2c63.nsys-rep[1/1] [=====29%                    ] nsys-report-2c63.nsys-rep[1/1] [=====30%                    ] nsys-report-2c63.nsys-rep[1/1] [=====31%                    ] nsys-report-2c63.nsys-rep[1/1] [=====32%                    ] nsys-report-2c63.nsys-rep[1/1] [======33%                   ] nsys-report-2c63.nsys-rep[1/1] [======34%                   ] nsys-report-2c63.nsys-rep[1/1] [======35%                   ] nsys-report-2c63.nsys-rep[1/1] [=======36%                  ] nsys-report-2c63.nsys-rep[1/1] [=======37%                  ] nsys-report-2c63.nsys-rep[1/1] [=======38%                  ] nsys-report-2c63.nsys-rep[1/1] [=======39%                  ] nsys-report-2c63.nsys-rep[1/1] [========40%                 ] nsys-report-2c63.nsys-rep[1/1] [========41%                 ] nsys-report-2c63.nsys-rep[1/1] [========42%                 ] nsys-report-2c63.nsys-rep[1/1] [=========43%                ] nsys-report-2c63.nsys-rep[1/1] [=========44%                ] nsys-report-2c63.nsys-rep[1/1] [=========45%                ] nsys-report-2c63.nsys-rep[1/1] [=========46%                ] nsys-report-2c63.nsys-rep[1/1] [==========47%               ] nsys-report-2c63.nsys-rep[1/1] [==========48%               ] nsys-report-2c63.nsys-rep[1/1] [==========49%               ] nsys-report-2c63.nsys-rep[1/1] [===========50%              ] nsys-report-2c63.nsys-rep[1/1] [===========51%              ] nsys-report-2c63.nsys-rep[1/1] [===========52%              ] nsys-report-2c63.nsys-rep[1/1] [===========53%              ] nsys-report-2c63.nsys-rep[1/1] [============54%             ] nsys-report-2c63.nsys-rep[1/1] [============55%             ] nsys-report-2c63.nsys-rep[1/1] [============56%             ] nsys-report-2c63.nsys-rep[1/1] [============57%             ] nsys-report-2c63.nsys-rep[1/1] [=============58%            ] nsys-report-2c63.nsys-rep[1/1] [=============59%            ] nsys-report-2c63.nsys-rep[1/1] [=============60%            ] nsys-report-2c63.nsys-rep[1/1] [==============61%           ] nsys-report-2c63.nsys-rep[1/1] [==============62%           ] nsys-report-2c63.nsys-rep[1/1] [==============63%           ] nsys-report-2c63.nsys-rep[1/1] [==============64%           ] nsys-report-2c63.nsys-rep[1/1] [===============65%          ] nsys-report-2c63.nsys-rep[1/1] [===============66%          ] nsys-report-2c63.nsys-rep[1/1] [===============67%          ] nsys-report-2c63.nsys-rep[1/1] [================68%         ] nsys-report-2c63.nsys-rep[1/1] [================69%         ] nsys-report-2c63.nsys-rep[1/1] [================70%         ] nsys-report-2c63.nsys-rep[1/1] [================71%         ] nsys-report-2c63.nsys-rep[1/1] [=================72%        ] nsys-report-2c63.nsys-rep[1/1] [=================73%        ] nsys-report-2c63.nsys-rep[1/1] [=================74%        ] nsys-report-2c63.nsys-rep[1/1] [==================75%       ] nsys-report-2c63.nsys-rep[1/1] [==================76%       ] nsys-report-2c63.nsys-rep[1/1] [==================77%       ] nsys-report-2c63.nsys-rep[1/1] [==================78%       ] nsys-report-2c63.nsys-rep[1/1] [===================79%      ] nsys-report-2c63.nsys-rep[1/1] [===================80%      ] nsys-report-2c63.nsys-rep[1/1] [===================81%      ] nsys-report-2c63.nsys-rep[1/1] [===================82%      ] nsys-report-2c63.nsys-rep[1/1] [========================100%] nsys-report-2c63.nsys-rep[1/1] [========================100%] nsys-report-2c63.nsys-rep
Generated:
	/tmp/nsys-report-2c63.nsys-rep
Run  3: 70.475235877 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 70.475235877 seconds
  Max elapsed : 73.074333373 seconds
  Avg elapsed : 71.39358525633333333333 seconds
===================================
--------------------------------------------
PROTON

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:29, 33.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:01<00:20, 45.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:16, 55.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 63.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:13, 69.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 77.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:02<00:11, 79.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:03<00:09, 84.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:08, 84.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 84.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:04<00:08, 84.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 84.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 84.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 84.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:06, 85.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 85.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 85.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:04, 109.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:05, 100.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 75.31 examples/s] Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 77.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:06, 79.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 81.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 82.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 83.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 83.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 84.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:02, 105.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 79.75 examples/s] Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 80.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 101.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 77.95 examples/s] Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 79.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 80.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 81.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 82.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 82.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 84.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 81.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 81.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 99.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 94.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 92.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 96.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.90 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747611900.7651818
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:15<07:27, 15.43s/it]  7%|▋         | 2/30 [00:16<03:20,  7.17s/it] 10%|█         | 3/30 [00:18<02:02,  4.54s/it] 13%|█▎        | 4/30 [00:19<01:25,  3.29s/it] 17%|█▋        | 5/30 [00:21<01:05,  2.62s/it] 20%|██        | 6/30 [00:22<00:52,  2.20s/it] 23%|██▎       | 7/30 [00:23<00:43,  1.91s/it] 27%|██▋       | 8/30 [00:25<00:37,  1.72s/it] 30%|███       | 9/30 [00:26<00:33,  1.60s/it] 33%|███▎      | 10/30 [00:27<00:30,  1.51s/it] 37%|███▋      | 11/30 [00:29<00:27,  1.46s/it] 40%|████      | 12/30 [00:30<00:25,  1.41s/it] 43%|████▎     | 13/30 [00:31<00:23,  1.38s/it] 47%|████▋     | 14/30 [00:33<00:22,  1.42s/it] 50%|█████     | 15/30 [00:34<00:21,  1.40s/it]                                                50%|█████     | 15/30 [00:34<00:21,  1.40s/it] 53%|█████▎    | 16/30 [00:35<00:19,  1.39s/it] 57%|█████▋    | 17/30 [00:37<00:18,  1.39s/it] 60%|██████    | 18/30 [00:38<00:16,  1.39s/it] 63%|██████▎   | 19/30 [00:39<00:14,  1.36s/it] 67%|██████▋   | 20/30 [00:41<00:13,  1.35s/it] 70%|███████   | 21/30 [00:42<00:11,  1.33s/it] 73%|███████▎  | 22/30 [00:43<00:10,  1.31s/it] 77%|███████▋  | 23/30 [00:45<00:09,  1.31s/it] 80%|████████  | 24/30 [00:46<00:07,  1.31s/it] 83%|████████▎ | 25/30 [00:47<00:06,  1.31s/it] 87%|████████▋ | 26/30 [00:49<00:05,  1.32s/it] 90%|█████████ | 27/30 [00:50<00:04,  1.37s/it] 93%|█████████▎| 28/30 [00:51<00:02,  1.38s/it] 97%|█████████▋| 29/30 [00:53<00:01,  1.38s/it]100%|██████████| 30/30 [00:54<00:00,  1.37s/it]                                               100%|██████████| 30/30 [00:54<00:00,  1.37s/it]                                               100%|██████████| 30/30 [00:54<00:00,  1.37s/it]100%|██████████| 30/30 [00:54<00:00,  1.82s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3604, 'grad_norm': 0.35593369603157043, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1731, 'grad_norm': 0.20517221093177795, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 54.7228, 'train_samples_per_second': 4.386, 'train_steps_per_second': 0.548, 'train_loss': 1.2667316436767577, 'epoch': 0.24}
END_PROFILE: 1747611956.2463758
Run  1: 56.948787777 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 59.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 66.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 77.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 79.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 82.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 82.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 82.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 84.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 84.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 84.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 84.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 82.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 84.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 84.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 84.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 84.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 84.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 84.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 84.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 84.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:05, 80.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 79.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 78.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 77.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 77.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 76.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:04, 77.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 79.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 80.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 80.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 78.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 77.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:03, 79.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 80.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 81.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 81.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 82.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 82.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 82.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 82.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 83.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 84.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 81.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 78.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 78.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 86.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.16 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747612024.6833053
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:16<07:55, 16.40s/it]  7%|▋         | 2/30 [00:17<03:31,  7.56s/it] 10%|█         | 3/30 [00:19<02:08,  4.76s/it] 13%|█▎        | 4/30 [00:20<01:29,  3.43s/it] 17%|█▋        | 5/30 [00:22<01:08,  2.72s/it] 20%|██        | 6/30 [00:23<00:54,  2.29s/it] 23%|██▎       | 7/30 [00:24<00:45,  1.98s/it] 27%|██▋       | 8/30 [00:26<00:39,  1.78s/it] 30%|███       | 9/30 [00:27<00:34,  1.65s/it] 33%|███▎      | 10/30 [00:28<00:30,  1.54s/it] 37%|███▋      | 11/30 [00:30<00:28,  1.50s/it] 40%|████      | 12/30 [00:31<00:26,  1.45s/it] 43%|████▎     | 13/30 [00:32<00:24,  1.42s/it] 47%|████▋     | 14/30 [00:34<00:23,  1.45s/it] 50%|█████     | 15/30 [00:35<00:21,  1.44s/it]                                                50%|█████     | 15/30 [00:35<00:21,  1.44s/it] 53%|█████▎    | 16/30 [00:37<00:20,  1.43s/it] 57%|█████▋    | 17/30 [00:38<00:18,  1.42s/it] 60%|██████    | 18/30 [00:40<00:17,  1.42s/it] 63%|██████▎   | 19/30 [00:41<00:15,  1.39s/it] 67%|██████▋   | 20/30 [00:42<00:13,  1.38s/it] 70%|███████   | 21/30 [00:44<00:12,  1.36s/it] 73%|███████▎  | 22/30 [00:45<00:10,  1.34s/it] 77%|███████▋  | 23/30 [00:46<00:09,  1.33s/it] 80%|████████  | 24/30 [00:48<00:08,  1.33s/it] 83%|████████▎ | 25/30 [00:49<00:06,  1.33s/it] 87%|████████▋ | 26/30 [00:50<00:05,  1.34s/it] 90%|█████████ | 27/30 [00:52<00:04,  1.41s/it] 93%|█████████▎| 28/30 [00:53<00:02,  1.40s/it] 97%|█████████▋| 29/30 [00:55<00:01,  1.40s/it]100%|██████████| 30/30 [00:56<00:00,  1.39s/it]                                               100%|██████████| 30/30 [00:56<00:00,  1.39s/it]                                               100%|██████████| 30/30 [00:56<00:00,  1.39s/it]100%|██████████| 30/30 [00:56<00:00,  1.88s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3601, 'grad_norm': 0.3442866802215576, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1728, 'grad_norm': 0.1977422535419464, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 56.4455, 'train_samples_per_second': 4.252, 'train_steps_per_second': 0.531, 'train_loss': 1.2664637883504233, 'epoch': 0.24}
END_PROFILE: 1747612081.872163
Run  2: 58.690301687 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 67.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:11, 75.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 77.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:10, 79.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 81.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 82.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:09, 83.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 84.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 84.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 84.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:08, 84.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 84.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 84.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 84.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:07, 87.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:10, 66.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 92.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 90.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:06, 89.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:06, 87.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:06, 87.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 86.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 86.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 85.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 85.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 85.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 84.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 84.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 84.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 84.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 84.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 84.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 84.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 85.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 85.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 84.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 84.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 84.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 84.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 84.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 84.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 84.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 84.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 84.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 84.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 84.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 84.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 63.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 87.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 84.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 83.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:11<00:00, 83.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 85.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 88.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 81.14 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747612148.042581
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:17<08:35, 17.78s/it]  7%|▋         | 2/30 [00:19<03:47,  8.12s/it] 10%|█         | 3/30 [00:20<02:16,  5.06s/it] 13%|█▎        | 4/30 [00:21<01:33,  3.61s/it] 17%|█▋        | 5/30 [00:23<01:10,  2.83s/it] 20%|██        | 6/30 [00:24<00:56,  2.35s/it] 23%|██▎       | 7/30 [00:26<00:46,  2.01s/it] 27%|██▋       | 8/30 [00:27<00:39,  1.80s/it] 30%|███       | 9/30 [00:28<00:34,  1.66s/it] 33%|███▎      | 10/30 [00:30<00:30,  1.55s/it] 37%|███▋      | 11/30 [00:31<00:28,  1.49s/it] 40%|████      | 12/30 [00:32<00:25,  1.44s/it] 43%|████▎     | 13/30 [00:34<00:23,  1.40s/it] 47%|████▋     | 14/30 [00:35<00:23,  1.44s/it] 50%|█████     | 15/30 [00:37<00:21,  1.42s/it]                                                50%|█████     | 15/30 [00:37<00:21,  1.42s/it] 53%|█████▎    | 16/30 [00:38<00:19,  1.41s/it] 57%|█████▋    | 17/30 [00:39<00:18,  1.41s/it] 60%|██████    | 18/30 [00:41<00:16,  1.41s/it] 63%|██████▎   | 19/30 [00:42<00:15,  1.38s/it] 67%|██████▋   | 20/30 [00:43<00:13,  1.37s/it] 70%|███████   | 21/30 [00:45<00:12,  1.35s/it] 73%|███████▎  | 22/30 [00:46<00:10,  1.33s/it] 77%|███████▋  | 23/30 [00:47<00:09,  1.33s/it] 80%|████████  | 24/30 [00:49<00:08,  1.33s/it] 83%|████████▎ | 25/30 [00:50<00:06,  1.33s/it] 87%|████████▋ | 26/30 [00:51<00:05,  1.35s/it] 90%|█████████ | 27/30 [00:53<00:04,  1.40s/it] 93%|█████████▎| 28/30 [00:54<00:02,  1.41s/it] 97%|█████████▋| 29/30 [00:56<00:01,  1.40s/it]100%|██████████| 30/30 [00:57<00:00,  1.39s/it]                                               100%|██████████| 30/30 [00:57<00:00,  1.39s/it]                                               100%|██████████| 30/30 [00:57<00:00,  1.39s/it]100%|██████████| 30/30 [00:57<00:00,  1.92s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3604, 'grad_norm': 0.3527488112449646, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1729, 'grad_norm': 0.19789017736911774, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 57.555, 'train_samples_per_second': 4.17, 'train_steps_per_second': 0.521, 'train_loss': 1.2666457494099934, 'epoch': 0.24}
END_PROFILE: 1747612206.3495095
Run  3: 59.718976957 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 56.948787777 seconds
  Max elapsed : 59.718976957 seconds
  Avg elapsed : 58.45268880700000000000 seconds
===================================
--------------------------------------------
TORCH

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:13, 66.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 77.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 79.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 83.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 83.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 80.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 79.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 78.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 77.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:08, 77.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:08, 76.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:08, 79.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:05<00:07, 80.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 81.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 82.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 82.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 83.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 83.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 84.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 84.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 84.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 84.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 84.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 84.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 83.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 84.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 81.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 80.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 79.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 80.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 87.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.78 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747612296.5696118
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:16, 19.19s/it]  7%|▋         | 2/30 [00:20<04:04,  8.75s/it] 10%|█         | 3/30 [00:22<02:27,  5.45s/it] 13%|█▎        | 4/30 [00:23<01:40,  3.87s/it] 17%|█▋        | 5/30 [00:25<01:15,  3.04s/it] 20%|██        | 6/30 [00:26<01:00,  2.53s/it] 23%|██▎       | 7/30 [00:28<00:49,  2.16s/it] 27%|██▋       | 8/30 [00:29<00:42,  1.92s/it] 30%|███       | 9/30 [00:30<00:37,  1.78s/it] 33%|███▎      | 10/30 [00:32<00:33,  1.66s/it] 37%|███▋      | 11/30 [00:33<00:30,  1.59s/it] 40%|████      | 12/30 [00:35<00:27,  1.54s/it] 43%|████▎     | 13/30 [00:36<00:25,  1.50s/it] 47%|████▋     | 14/30 [00:38<00:23,  1.47s/it] 50%|█████     | 15/30 [00:39<00:21,  1.45s/it]                                                50%|█████     | 15/30 [00:39<00:21,  1.45s/it] 53%|█████▎    | 16/30 [00:40<00:20,  1.45s/it] 57%|█████▋    | 17/30 [00:42<00:18,  1.44s/it] 60%|██████    | 18/30 [00:43<00:17,  1.45s/it] 63%|██████▎   | 19/30 [00:45<00:15,  1.43s/it] 67%|██████▋   | 20/30 [00:46<00:14,  1.42s/it] 70%|███████   | 21/30 [00:47<00:12,  1.42s/it] 73%|███████▎  | 22/30 [00:49<00:11,  1.40s/it] 77%|███████▋  | 23/30 [00:50<00:09,  1.41s/it] 80%|████████  | 24/30 [00:52<00:08,  1.42s/it] 83%|████████▎ | 25/30 [00:53<00:07,  1.42s/it] 87%|████████▋ | 26/30 [00:55<00:05,  1.43s/it] 90%|█████████ | 27/30 [00:56<00:04,  1.42s/it] 93%|█████████▎| 28/30 [00:57<00:02,  1.41s/it] 97%|█████████▋| 29/30 [00:59<00:01,  1.41s/it]100%|██████████| 30/30 [01:00<00:00,  1.42s/it]                                               100%|██████████| 30/30 [01:00<00:00,  1.42s/it]                                               100%|██████████| 30/30 [01:00<00:00,  1.42s/it]100%|██████████| 30/30 [01:00<00:00,  2.02s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3603, 'grad_norm': 0.3590201437473297, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1731, 'grad_norm': 0.2009200155735016, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 60.7124, 'train_samples_per_second': 3.953, 'train_steps_per_second': 0.494, 'train_loss': 1.266684341430664, 'epoch': 0.24}
END_PROFILE: 1747612369.0249233
Run  1: 74.225298822 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 59.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 65.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 70.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 76.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 78.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 80.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 81.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 81.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 82.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 82.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 82.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 82.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:09, 82.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 83.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 83.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 83.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 83.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 83.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:06<00:06, 83.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 82.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 82.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:06, 82.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 82.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 82.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:07<00:05, 82.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 82.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 82.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 82.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 82.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 82.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:08<00:04, 82.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 82.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 83.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 83.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 83.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 83.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:09<00:03, 83.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 83.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 82.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:10<00:02, 82.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 82.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 82.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:11<00:01, 83.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 82.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 81.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 79.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 78.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:12<00:00, 78.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 82.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 85.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 88.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 79.62 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747612442.1029134
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:14<07:03, 14.60s/it]  7%|▋         | 2/30 [00:16<03:11,  6.84s/it] 10%|█         | 3/30 [00:17<01:57,  4.37s/it] 13%|█▎        | 4/30 [00:18<01:23,  3.20s/it] 17%|█▋        | 5/30 [00:20<01:04,  2.57s/it] 20%|██        | 6/30 [00:21<00:52,  2.18s/it] 23%|██▎       | 7/30 [00:23<00:43,  1.90s/it] 27%|██▋       | 8/30 [00:24<00:37,  1.72s/it] 30%|███       | 9/30 [00:25<00:33,  1.61s/it] 33%|███▎      | 10/30 [00:27<00:30,  1.52s/it] 37%|███▋      | 11/30 [00:28<00:27,  1.46s/it] 40%|████      | 12/30 [00:29<00:25,  1.42s/it] 43%|████▎     | 13/30 [00:31<00:23,  1.39s/it] 47%|████▋     | 14/30 [00:32<00:21,  1.37s/it] 50%|█████     | 15/30 [00:33<00:20,  1.35s/it]                                                50%|█████     | 15/30 [00:33<00:20,  1.35s/it] 53%|█████▎    | 16/30 [00:35<00:18,  1.35s/it] 57%|█████▋    | 17/30 [00:36<00:17,  1.34s/it] 60%|██████    | 18/30 [00:37<00:16,  1.36s/it] 63%|██████▎   | 19/30 [00:39<00:14,  1.34s/it] 67%|██████▋   | 20/30 [00:40<00:13,  1.33s/it] 70%|███████   | 21/30 [00:41<00:11,  1.31s/it] 73%|███████▎  | 22/30 [00:42<00:10,  1.30s/it] 77%|███████▋  | 23/30 [00:44<00:09,  1.31s/it] 80%|████████  | 24/30 [00:45<00:07,  1.32s/it] 83%|████████▎ | 25/30 [00:46<00:06,  1.31s/it] 87%|████████▋ | 26/30 [00:48<00:05,  1.33s/it] 90%|█████████ | 27/30 [00:49<00:03,  1.32s/it] 93%|█████████▎| 28/30 [00:50<00:02,  1.32s/it] 97%|█████████▋| 29/30 [00:52<00:01,  1.32s/it]100%|██████████| 30/30 [00:53<00:00,  1.31s/it]                                               100%|██████████| 30/30 [00:53<00:00,  1.31s/it]                                               100%|██████████| 30/30 [00:53<00:00,  1.31s/it]100%|██████████| 30/30 [00:53<00:00,  1.78s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3603, 'grad_norm': 0.34853094816207886, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1731, 'grad_norm': 0.19573664665222168, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 53.4315, 'train_samples_per_second': 4.492, 'train_steps_per_second': 0.561, 'train_loss': 1.2666878382364908, 'epoch': 0.24}
END_PROFILE: 1747612507.308936
Run  2: 66.972724366 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for llama-3.2-3b...
==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Adding LoRA adapters for llama-3.2-3b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:55, 17.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:18, 52.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:01<00:15, 60.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:01<00:14, 66.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:12, 71.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:12, 74.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:11, 77.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:11, 79.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:02<00:10, 80.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:02<00:10, 81.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:02<00:10, 82.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:02<00:09, 82.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:02<00:09, 83.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:09, 83.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:03<00:09, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:03<00:09, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:03<00:08, 84.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:03<00:08, 83.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:03<00:08, 83.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:03<00:08, 83.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:04<00:08, 83.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:04<00:08, 83.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:04<00:07, 83.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:04<00:07, 83.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:04<00:07, 83.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:04<00:07, 83.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:05<00:07, 83.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:05<00:07, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:05<00:06, 83.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:05<00:06, 84.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:05<00:06, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:05<00:06, 83.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:06<00:06, 83.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:06<00:06, 83.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:06<00:05, 83.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:06<00:05, 83.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:06<00:05, 83.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:06<00:05, 83.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:07<00:05, 83.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:07<00:05, 83.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:07<00:04, 84.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:07<00:04, 84.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:07<00:04, 84.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:07<00:04, 84.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:08<00:04, 84.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:08<00:04, 84.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:08<00:03, 84.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:08<00:03, 84.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:08<00:03, 83.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:08<00:03, 84.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:09<00:03, 83.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:09<00:03, 83.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:09<00:02, 83.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:09<00:02, 83.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:09<00:02, 83.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:09<00:02, 83.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:10<00:02, 83.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:10<00:02, 83.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:10<00:01, 83.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:10<00:01, 83.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:10<00:01, 83.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:10<00:01, 83.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:11<00:01, 83.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:11<00:01, 82.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:11<00:00, 80.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:11<00:00, 80.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:11<00:00, 79.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:11<00:00, 78.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:12<00:00, 82.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  99%|█████████▊| 987/1000 [00:12<00:00, 84.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 88.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:12<00:00, 80.25 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 12,156,928/3,224,906,752 (0.38% trained)
START_PROFILE: 1747612581.0609894
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:26<12:47, 26.46s/it]  7%|▋         | 2/30 [00:27<05:29, 11.76s/it] 10%|█         | 3/30 [00:29<03:11,  7.10s/it] 13%|█▎        | 4/30 [00:30<02:07,  4.89s/it] 17%|█▋        | 5/30 [00:32<01:32,  3.71s/it] 20%|██        | 6/30 [00:34<01:11,  2.96s/it] 23%|██▎       | 7/30 [00:35<00:56,  2.47s/it] 27%|██▋       | 8/30 [00:37<00:47,  2.15s/it] 30%|███       | 9/30 [00:38<00:40,  1.95s/it] 33%|███▎      | 10/30 [00:40<00:35,  1.80s/it] 37%|███▋      | 11/30 [00:41<00:32,  1.70s/it] 40%|████      | 12/30 [00:42<00:29,  1.63s/it] 43%|████▎     | 13/30 [00:44<00:26,  1.58s/it] 47%|████▋     | 14/30 [00:45<00:24,  1.55s/it] 50%|█████     | 15/30 [00:47<00:22,  1.53s/it]                                                50%|█████     | 15/30 [00:47<00:22,  1.53s/it] 53%|█████▎    | 16/30 [00:48<00:21,  1.51s/it] 57%|█████▋    | 17/30 [00:50<00:19,  1.50s/it] 60%|██████    | 18/30 [00:51<00:18,  1.50s/it] 63%|██████▎   | 19/30 [00:53<00:16,  1.49s/it] 67%|██████▋   | 20/30 [00:54<00:14,  1.48s/it] 70%|███████   | 21/30 [00:56<00:13,  1.47s/it] 73%|███████▎  | 22/30 [00:57<00:11,  1.46s/it] 77%|███████▋  | 23/30 [00:59<00:10,  1.46s/it] 80%|████████  | 24/30 [01:00<00:08,  1.47s/it] 83%|████████▎ | 25/30 [01:02<00:07,  1.47s/it] 87%|████████▋ | 26/30 [01:03<00:05,  1.48s/it] 90%|█████████ | 27/30 [01:05<00:04,  1.47s/it] 93%|█████████▎| 28/30 [01:06<00:02,  1.46s/it] 97%|█████████▋| 29/30 [01:07<00:01,  1.46s/it]100%|██████████| 30/30 [01:09<00:00,  1.46s/it]                                               100%|██████████| 30/30 [01:09<00:00,  1.46s/it]                                               100%|██████████| 30/30 [01:09<00:00,  1.46s/it]100%|██████████| 30/30 [01:09<00:00,  2.31s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.3602, 'grad_norm': 0.3598931133747101, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 1.1731, 'grad_norm': 0.201741561293602, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 69.3619, 'train_samples_per_second': 3.46, 'train_steps_per_second': 0.433, 'train_loss': 1.2666504542032877, 'epoch': 0.24}
END_PROFILE: 1747612662.7186642
Run  3: 83.532179030 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 66.972724366 seconds
  Max elapsed : 83.532179030 seconds
  Avg elapsed : 74.91006740600000000000 seconds
===================================
--------------------------------------------
--------------------------------------------
Timing qwen3-14b with NONE

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.32s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:39, 13.00s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.90s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.81s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.58s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.84s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 27.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 70.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:11, 85.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 97.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 106.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:07, 112.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 117.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 121.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 123.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 125.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 126.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 127.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 127.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 128.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 128.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 128.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 128.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 128.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 128.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 128.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 128.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 128.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 128.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 128.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 128.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 129.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 129.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 129.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 129.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 129.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 129.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 129.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 129.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 129.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 128.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 128.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 128.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 127.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 127.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 128.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 128.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 127.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 128.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 128.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 128.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 128.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 128.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 128.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 128.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 128.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 128.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:02, 128.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 128.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 128.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 128.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 128.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 128.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 128.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 128.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 128.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 128.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 128.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 128.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 125.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 123.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 122.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 121.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 139.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 146.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 122.29 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747612818.7783983
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:34, 19.80s/it]  7%|▋         | 2/30 [00:21<04:16,  9.16s/it] 10%|█         | 3/30 [00:23<02:35,  5.76s/it] 13%|█▎        | 4/30 [00:24<01:47,  4.15s/it] 17%|█▋        | 5/30 [00:26<01:23,  3.35s/it] 20%|██        | 6/30 [00:28<01:07,  2.83s/it] 23%|██▎       | 7/30 [00:30<00:56,  2.47s/it] 27%|██▋       | 8/30 [00:32<00:48,  2.21s/it] 30%|███       | 9/30 [00:34<00:44,  2.14s/it] 33%|███▎      | 10/30 [00:35<00:40,  2.00s/it] 37%|███▋      | 11/30 [00:37<00:36,  1.90s/it] 40%|████      | 12/30 [00:39<00:32,  1.83s/it] 43%|████▎     | 13/30 [00:40<00:30,  1.80s/it] 47%|████▋     | 14/30 [00:42<00:28,  1.80s/it] 50%|█████     | 15/30 [00:44<00:26,  1.77s/it]                                                50%|█████     | 15/30 [00:44<00:26,  1.77s/it] 53%|█████▎    | 16/30 [00:46<00:24,  1.75s/it] 57%|█████▋    | 17/30 [00:47<00:22,  1.72s/it] 60%|██████    | 18/30 [00:49<00:21,  1.78s/it] 63%|██████▎   | 19/30 [00:51<00:19,  1.75s/it] 67%|██████▋   | 20/30 [00:52<00:17,  1.73s/it] 70%|███████   | 21/30 [00:54<00:15,  1.71s/it] 73%|███████▎  | 22/30 [00:56<00:13,  1.69s/it] 77%|███████▋  | 23/30 [00:57<00:11,  1.68s/it] 80%|████████  | 24/30 [00:59<00:10,  1.76s/it] 83%|████████▎ | 25/30 [01:01<00:08,  1.73s/it] 87%|████████▋ | 26/30 [01:03<00:07,  1.79s/it] 90%|█████████ | 27/30 [01:05<00:05,  1.76s/it] 93%|█████████▎| 28/30 [01:06<00:03,  1.74s/it] 97%|█████████▋| 29/30 [01:08<00:01,  1.73s/it]100%|██████████| 30/30 [01:10<00:00,  1.72s/it]                                               100%|██████████| 30/30 [01:10<00:00,  1.72s/it]                                               100%|██████████| 30/30 [01:10<00:00,  1.72s/it]100%|██████████| 30/30 [01:10<00:00,  2.34s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0414, 'grad_norm': 0.2576189339160919, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8184, 'grad_norm': 0.12985233962535858, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 70.2335, 'train_samples_per_second': 3.417, 'train_steps_per_second': 0.427, 'train_loss': 0.9299384117126465, 'epoch': 0.24}
END_PROFILE: 1747612889.5014296
Run  1: 73.465202323 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.16s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.33s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:39, 13.00s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.90s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.75s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.55s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.82s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 27.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 69.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:11, 85.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 97.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 105.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:08, 112.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 117.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 120.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:07, 122.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 123.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 125.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 126.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 127.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 127.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 127.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 128.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 128.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 128.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 128.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 128.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 129.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 128.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 129.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 129.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 129.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 128.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 129.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 128.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 128.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 129.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 129.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 129.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 129.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 128.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 128.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 128.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 129.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 129.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 127.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 128.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 128.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 129.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 129.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 129.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 129.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 129.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 129.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 129.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 129.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 129.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 129.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 129.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 128.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 128.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 128.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 128.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 128.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 128.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 128.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 128.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 128.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 128.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 128.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 125.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 123.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 122.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 121.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 137.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 144.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 122.21 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747613021.0957236
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<09:09, 18.96s/it]  7%|▋         | 2/30 [00:20<04:06,  8.79s/it] 10%|█         | 3/30 [00:22<02:29,  5.54s/it] 13%|█▎        | 4/30 [00:23<01:44,  4.00s/it] 17%|█▋        | 5/30 [00:25<01:20,  3.24s/it] 20%|██        | 6/30 [00:27<01:05,  2.75s/it] 23%|██▎       | 7/30 [00:29<00:55,  2.40s/it] 27%|██▋       | 8/30 [00:30<00:47,  2.16s/it] 30%|███       | 9/30 [00:32<00:43,  2.09s/it] 33%|███▎      | 10/30 [00:34<00:39,  1.96s/it] 37%|███▋      | 11/30 [00:36<00:35,  1.86s/it] 40%|████      | 12/30 [00:37<00:32,  1.79s/it] 43%|████▎     | 13/30 [00:39<00:29,  1.76s/it] 47%|████▋     | 14/30 [00:41<00:28,  1.76s/it] 50%|█████     | 15/30 [00:42<00:26,  1.74s/it]                                                50%|█████     | 15/30 [00:42<00:26,  1.74s/it] 53%|█████▎    | 16/30 [00:44<00:24,  1.71s/it] 57%|█████▋    | 17/30 [00:46<00:21,  1.69s/it] 60%|██████    | 18/30 [00:48<00:20,  1.74s/it] 63%|██████▎   | 19/30 [00:49<00:18,  1.71s/it] 67%|██████▋   | 20/30 [00:51<00:16,  1.68s/it] 70%|███████   | 21/30 [00:52<00:14,  1.66s/it] 73%|███████▎  | 22/30 [00:54<00:13,  1.65s/it] 77%|███████▋  | 23/30 [00:56<00:11,  1.64s/it] 80%|████████  | 24/30 [00:58<00:10,  1.71s/it] 83%|████████▎ | 25/30 [00:59<00:08,  1.68s/it] 87%|████████▋ | 26/30 [01:01<00:06,  1.74s/it] 90%|█████████ | 27/30 [01:03<00:05,  1.72s/it] 93%|█████████▎| 28/30 [01:04<00:03,  1.70s/it] 97%|█████████▋| 29/30 [01:06<00:01,  1.68s/it]100%|██████████| 30/30 [01:08<00:00,  1.67s/it]                                               100%|██████████| 30/30 [01:08<00:00,  1.67s/it]                                               100%|██████████| 30/30 [01:08<00:00,  1.67s/it]100%|██████████| 30/30 [01:08<00:00,  2.27s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0416, 'grad_norm': 0.2569372355937958, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8185, 'grad_norm': 0.1302301585674286, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 68.2088, 'train_samples_per_second': 3.519, 'train_steps_per_second': 0.44, 'train_loss': 0.9300537427266439, 'epoch': 0.24}
END_PROFILE: 1747613089.7973561
Run  2: 70.184031509 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.14s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.34s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:39, 13.04s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.94s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:05<00:12, 12.84s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.61s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.87s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 26.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 69.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:11, 84.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 96.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 105.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:08, 112.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 116.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 120.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 123.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 123.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 125.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 125.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 126.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 126.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 126.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 127.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 127.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 128.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 127.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 128.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 128.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 128.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 128.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 128.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 128.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 127.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 128.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 127.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 127.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 127.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 127.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:04<00:04, 127.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 127.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:04, 127.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 127.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 127.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 127.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 127.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 126.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 126.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:05<00:03, 127.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 127.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:03, 127.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 127.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 128.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 128.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 128.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 128.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 128.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 128.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 128.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:02, 128.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 128.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 128.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 128.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 128.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 128.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 128.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 128.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 127.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 128.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 127.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 128.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 124.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 122.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 121.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 121.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:08<00:00, 134.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 143.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 121.36 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747613223.266673
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<10:48, 22.37s/it]  7%|▋         | 2/30 [00:24<04:45, 10.21s/it] 10%|█         | 3/30 [00:25<02:50,  6.32s/it] 13%|█▎        | 4/30 [00:27<01:56,  4.49s/it] 17%|█▋        | 5/30 [00:29<01:28,  3.56s/it] 20%|██        | 6/30 [00:31<01:11,  2.96s/it] 23%|██▎       | 7/30 [00:32<00:58,  2.55s/it] 27%|██▋       | 8/30 [00:34<00:49,  2.25s/it] 30%|███       | 9/30 [00:36<00:45,  2.15s/it] 33%|███▎      | 10/30 [00:38<00:39,  2.00s/it] 37%|███▋      | 11/30 [00:39<00:36,  1.90s/it] 40%|████      | 12/30 [00:41<00:33,  1.84s/it] 43%|████▎     | 13/30 [00:43<00:30,  1.78s/it] 47%|████▋     | 14/30 [00:44<00:28,  1.78s/it] 50%|█████     | 15/30 [00:46<00:26,  1.75s/it]                                                50%|█████     | 15/30 [00:46<00:26,  1.75s/it] 53%|█████▎    | 16/30 [00:48<00:24,  1.72s/it] 57%|█████▋    | 17/30 [00:49<00:21,  1.69s/it] 60%|██████    | 18/30 [00:51<00:20,  1.75s/it] 63%|██████▎   | 19/30 [00:53<00:18,  1.71s/it] 67%|██████▋   | 20/30 [00:54<00:16,  1.69s/it] 70%|███████   | 21/30 [00:56<00:15,  1.67s/it] 73%|███████▎  | 22/30 [00:58<00:13,  1.65s/it] 77%|███████▋  | 23/30 [00:59<00:11,  1.64s/it] 80%|████████  | 24/30 [01:01<00:10,  1.72s/it] 83%|████████▎ | 25/30 [01:03<00:08,  1.69s/it] 87%|████████▋ | 26/30 [01:05<00:06,  1.75s/it] 90%|█████████ | 27/30 [01:06<00:05,  1.73s/it] 93%|█████████▎| 28/30 [01:08<00:03,  1.71s/it] 97%|█████████▋| 29/30 [01:10<00:01,  1.69s/it]100%|██████████| 30/30 [01:11<00:00,  1.68s/it]                                               100%|██████████| 30/30 [01:11<00:00,  1.68s/it]                                               100%|██████████| 30/30 [01:11<00:00,  1.68s/it]100%|██████████| 30/30 [01:11<00:00,  2.40s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0414, 'grad_norm': 0.25482019782066345, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8184, 'grad_norm': 0.13045449554920197, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 71.8601, 'train_samples_per_second': 3.34, 'train_steps_per_second': 0.417, 'train_loss': 0.9298895517985026, 'epoch': 0.24}
END_PROFILE: 1747613295.6299841
Run  3: 73.900281058 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 70.184031509 seconds
  Max elapsed : 73.900281058 seconds
  Avg elapsed : 72.51650496333333333333 seconds
===================================
NSYS

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:11, 14.21s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:27<00:53, 13.40s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:39, 13.06s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.96s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:05<00:12, 12.84s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.62s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.89s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 27.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 70.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:10, 85.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 98.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 106.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:07, 114.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 118.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 121.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 124.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 126.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 127.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 128.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 129.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 130.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:05, 130.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 130.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 130.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 131.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 129.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 130.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 129.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 129.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:02<00:05, 130.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 129.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 129.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 129.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 129.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 130.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 129.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 130.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 129.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 130.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 128.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 129.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 129.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 129.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 129.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 130.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 130.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 130.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 129.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 129.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 130.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 129.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 130.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 129.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 129.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 129.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 129.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 130.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 129.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 129.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 130.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 130.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 129.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 130.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 129.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 130.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 129.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:06<00:01, 129.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 130.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 129.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 129.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 126.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 125.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 124.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 123.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 141.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 147.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 123.39 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747613450.8804526
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:44, 20.17s/it]  7%|▋         | 2/30 [00:22<04:28,  9.58s/it] 10%|█         | 3/30 [00:24<02:47,  6.21s/it] 13%|█▎        | 4/30 [00:26<01:59,  4.62s/it] 17%|█▋        | 5/30 [00:29<01:34,  3.80s/it] 20%|██        | 6/30 [00:31<01:18,  3.27s/it] 23%|██▎       | 7/30 [00:33<01:07,  2.92s/it] 27%|██▋       | 8/30 [00:35<00:58,  2.68s/it] 30%|███       | 9/30 [00:38<00:54,  2.59s/it] 33%|███▎      | 10/30 [00:40<00:49,  2.46s/it] 37%|███▋      | 11/30 [00:42<00:44,  2.37s/it] 40%|████      | 12/30 [00:44<00:41,  2.30s/it] 43%|████▎     | 13/30 [00:46<00:38,  2.27s/it] 47%|████▋     | 14/30 [00:49<00:36,  2.27s/it] 50%|█████     | 15/30 [00:51<00:33,  2.26s/it]                                                50%|█████     | 15/30 [00:51<00:33,  2.26s/it] 53%|█████▎    | 16/30 [00:53<00:31,  2.24s/it] 57%|█████▋    | 17/30 [00:55<00:28,  2.21s/it] 60%|██████    | 18/30 [00:57<00:26,  2.25s/it] 63%|██████▎   | 19/30 [01:00<00:24,  2.22s/it] 67%|██████▋   | 20/30 [01:02<00:22,  2.21s/it] 70%|███████   | 21/30 [01:04<00:19,  2.19s/it] 73%|███████▎  | 22/30 [01:06<00:17,  2.18s/it] 77%|███████▋  | 23/30 [01:08<00:15,  2.17s/it] 80%|████████  | 24/30 [01:11<00:13,  2.22s/it] 83%|████████▎ | 25/30 [01:13<00:10,  2.20s/it] 87%|████████▋ | 26/30 [01:15<00:08,  2.24s/it] 90%|█████████ | 27/30 [01:17<00:06,  2.23s/it] 93%|█████████▎| 28/30 [01:19<00:04,  2.21s/it] 97%|█████████▋| 29/30 [01:22<00:02,  2.20s/it]100%|██████████| 30/30 [01:24<00:00,  2.21s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.21s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.21s/it]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0415, 'grad_norm': 0.2559984624385834, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8183, 'grad_norm': 0.1302247792482376, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 84.2765, 'train_samples_per_second': 2.848, 'train_steps_per_second': 0.356, 'train_loss': 0.929914919535319, 'epoch': 0.24}
END_PROFILE: 1747613535.6587417
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/qwen3-14b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-89ed.qdstrm'
[1/1] [0%                          ] nsys-report-1062.nsys-rep[1/1] [0%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [8%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [5%                          ] nsys-report-1062.nsys-rep[1/1] [5%                          ] nsys-report-1062.nsys-rep[1/1] [6%                          ] nsys-report-1062.nsys-rep[1/1] [7%                          ] nsys-report-1062.nsys-rep[1/1] [8%                          ] nsys-report-1062.nsys-rep[1/1] [9%                          ] nsys-report-1062.nsys-rep[1/1] [10%                         ] nsys-report-1062.nsys-rep[1/1] [11%                         ] nsys-report-1062.nsys-rep[1/1] [12%                         ] nsys-report-1062.nsys-rep[1/1] [13%                         ] nsys-report-1062.nsys-rep[1/1] [14%                         ] nsys-report-1062.nsys-rep[1/1] [=15%                        ] nsys-report-1062.nsys-rep[1/1] [=16%                        ] nsys-report-1062.nsys-rep[1/1] [=17%                        ] nsys-report-1062.nsys-rep[1/1] [==18%                       ] nsys-report-1062.nsys-rep[1/1] [==19%                       ] nsys-report-1062.nsys-rep[1/1] [==20%                       ] nsys-report-1062.nsys-rep[1/1] [==21%                       ] nsys-report-1062.nsys-rep[1/1] [===22%                      ] nsys-report-1062.nsys-rep[1/1] [===23%                      ] nsys-report-1062.nsys-rep[1/1] [===24%                      ] nsys-report-1062.nsys-rep[1/1] [====25%                     ] nsys-report-1062.nsys-rep[1/1] [====26%                     ] nsys-report-1062.nsys-rep[1/1] [====27%                     ] nsys-report-1062.nsys-rep[1/1] [====28%                     ] nsys-report-1062.nsys-rep[1/1] [=====29%                    ] nsys-report-1062.nsys-rep[1/1] [=====30%                    ] nsys-report-1062.nsys-rep[1/1] [=====31%                    ] nsys-report-1062.nsys-rep[1/1] [=====32%                    ] nsys-report-1062.nsys-rep[1/1] [======33%                   ] nsys-report-1062.nsys-rep[1/1] [======34%                   ] nsys-report-1062.nsys-rep[1/1] [======35%                   ] nsys-report-1062.nsys-rep[1/1] [=======36%                  ] nsys-report-1062.nsys-rep[1/1] [=======37%                  ] nsys-report-1062.nsys-rep[1/1] [=======38%                  ] nsys-report-1062.nsys-rep[1/1] [=======39%                  ] nsys-report-1062.nsys-rep[1/1] [========40%                 ] nsys-report-1062.nsys-rep[1/1] [========41%                 ] nsys-report-1062.nsys-rep[1/1] [========42%                 ] nsys-report-1062.nsys-rep[1/1] [=========43%                ] nsys-report-1062.nsys-rep[1/1] [=========44%                ] nsys-report-1062.nsys-rep[1/1] [=========45%                ] nsys-report-1062.nsys-rep[1/1] [=========46%                ] nsys-report-1062.nsys-rep[1/1] [==========47%               ] nsys-report-1062.nsys-rep[1/1] [==========48%               ] nsys-report-1062.nsys-rep[1/1] [==========49%               ] nsys-report-1062.nsys-rep[1/1] [===========50%              ] nsys-report-1062.nsys-rep[1/1] [===========51%              ] nsys-report-1062.nsys-rep[1/1] [===========52%              ] nsys-report-1062.nsys-rep[1/1] [===========53%              ] nsys-report-1062.nsys-rep[1/1] [============54%             ] nsys-report-1062.nsys-rep[1/1] [============55%             ] nsys-report-1062.nsys-rep[1/1] [============56%             ] nsys-report-1062.nsys-rep[1/1] [============57%             ] nsys-report-1062.nsys-rep[1/1] [=============58%            ] nsys-report-1062.nsys-rep[1/1] [=============59%            ] nsys-report-1062.nsys-rep[1/1] [=============60%            ] nsys-report-1062.nsys-rep[1/1] [==============61%           ] nsys-report-1062.nsys-rep[1/1] [==============62%           ] nsys-report-1062.nsys-rep[1/1] [==============63%           ] nsys-report-1062.nsys-rep[1/1] [==============64%           ] nsys-report-1062.nsys-rep[1/1] [===============65%          ] nsys-report-1062.nsys-rep[1/1] [===============66%          ] nsys-report-1062.nsys-rep[1/1] [===============67%          ] nsys-report-1062.nsys-rep[1/1] [================68%         ] nsys-report-1062.nsys-rep[1/1] [================69%         ] nsys-report-1062.nsys-rep[1/1] [================70%         ] nsys-report-1062.nsys-rep[1/1] [================71%         ] nsys-report-1062.nsys-rep[1/1] [=================72%        ] nsys-report-1062.nsys-rep[1/1] [=================73%        ] nsys-report-1062.nsys-rep[1/1] [=================74%        ] nsys-report-1062.nsys-rep[1/1] [==================75%       ] nsys-report-1062.nsys-rep[1/1] [==================76%       ] nsys-report-1062.nsys-rep[1/1] [==================77%       ] nsys-report-1062.nsys-rep[1/1] [==================78%       ] nsys-report-1062.nsys-rep[1/1] [===================79%      ] nsys-report-1062.nsys-rep[1/1] [===================80%      ] nsys-report-1062.nsys-rep[1/1] [===================81%      ] nsys-report-1062.nsys-rep[1/1] [========================100%] nsys-report-1062.nsys-rep[1/1] [========================100%] nsys-report-1062.nsys-rep
Generated:
	/tmp/nsys-report-1062.nsys-rep
Run  1: 99.711985988 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.17s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.37s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:39, 13.07s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.98s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:05<00:12, 12.83s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.62s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.88s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 26.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 69.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:11, 85.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 96.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 105.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:08, 112.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 117.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 120.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 123.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 125.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 126.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 127.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 127.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 128.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 128.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 128.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 128.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 128.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 128.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 129.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 129.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 129.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 129.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 129.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 129.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 129.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 129.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 129.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 129.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 129.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 129.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 129.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 129.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 129.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 129.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 129.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 129.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 129.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 128.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 128.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 128.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 128.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 129.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 129.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 128.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 128.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 128.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 128.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 128.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 128.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 129.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 129.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 129.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 129.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 129.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 129.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 129.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 129.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 128.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 129.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 129.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 129.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 129.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 126.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 124.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 123.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 122.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 136.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 148.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 122.50 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747613688.2463245
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:55, 20.55s/it]  7%|▋         | 2/30 [00:22<04:32,  9.74s/it] 10%|█         | 3/30 [00:24<02:50,  6.30s/it] 13%|█▎        | 4/30 [00:27<02:01,  4.67s/it] 17%|█▋        | 5/30 [00:29<01:35,  3.83s/it] 20%|██        | 6/30 [00:31<01:19,  3.30s/it] 23%|██▎       | 7/30 [00:33<01:07,  2.94s/it] 27%|██▋       | 8/30 [00:36<00:59,  2.69s/it] 30%|███       | 9/30 [00:38<00:54,  2.59s/it] 33%|███▎      | 10/30 [00:40<00:49,  2.46s/it] 37%|███▋      | 11/30 [00:42<00:45,  2.37s/it] 40%|████      | 12/30 [00:44<00:41,  2.32s/it] 43%|████▎     | 13/30 [00:47<00:38,  2.27s/it] 47%|████▋     | 14/30 [00:49<00:36,  2.26s/it] 50%|█████     | 15/30 [00:51<00:33,  2.24s/it]                                                50%|█████     | 15/30 [00:51<00:33,  2.24s/it] 53%|█████▎    | 16/30 [00:53<00:31,  2.22s/it] 57%|█████▋    | 17/30 [00:55<00:28,  2.19s/it] 60%|██████    | 18/30 [00:58<00:26,  2.23s/it] 63%|██████▎   | 19/30 [01:00<00:24,  2.20s/it] 67%|██████▋   | 20/30 [01:02<00:21,  2.19s/it] 70%|███████   | 21/30 [01:04<00:19,  2.18s/it] 73%|███████▎  | 22/30 [01:06<00:17,  2.17s/it] 77%|███████▋  | 23/30 [01:08<00:15,  2.16s/it] 80%|████████  | 24/30 [01:11<00:13,  2.21s/it] 83%|████████▎ | 25/30 [01:13<00:10,  2.19s/it] 87%|████████▋ | 26/30 [01:15<00:08,  2.23s/it] 90%|█████████ | 27/30 [01:17<00:06,  2.22s/it] 93%|█████████▎| 28/30 [01:20<00:04,  2.21s/it] 97%|█████████▋| 29/30 [01:22<00:02,  2.20s/it]100%|██████████| 30/30 [01:24<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.20s/it]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0414, 'grad_norm': 0.2558608651161194, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8186, 'grad_norm': 0.1300828754901886, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 84.4457, 'train_samples_per_second': 2.842, 'train_steps_per_second': 0.355, 'train_loss': 0.9299871762593587, 'epoch': 0.24}
END_PROFILE: 1747613773.1828337
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/qwen3-14b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-c74f.qdstrm'
[1/1] [0%                          ] nsys-report-fc5f.nsys-rep[1/1] [0%                          ] nsys-report-fc5f.nsys-rep[1/1] [0%                          ] nsys-report-fc5f.nsys-rep[1/1] [7%                          ] nsys-report-fc5f.nsys-rep[1/1] [6%                          ] nsys-report-fc5f.nsys-rep[1/1] [8%                          ] nsys-report-fc5f.nsys-rep[1/1] [7%                          ] nsys-report-fc5f.nsys-rep[1/1] [6%                          ] nsys-report-fc5f.nsys-rep[1/1] [5%                          ] nsys-report-fc5f.nsys-rep[1/1] [5%                          ] nsys-report-fc5f.nsys-rep[1/1] [6%                          ] nsys-report-fc5f.nsys-rep[1/1] [7%                          ] nsys-report-fc5f.nsys-rep[1/1] [8%                          ] nsys-report-fc5f.nsys-rep[1/1] [9%                          ] nsys-report-fc5f.nsys-rep[1/1] [10%                         ] nsys-report-fc5f.nsys-rep[1/1] [11%                         ] nsys-report-fc5f.nsys-rep[1/1] [12%                         ] nsys-report-fc5f.nsys-rep[1/1] [13%                         ] nsys-report-fc5f.nsys-rep[1/1] [14%                         ] nsys-report-fc5f.nsys-rep[1/1] [=15%                        ] nsys-report-fc5f.nsys-rep[1/1] [=16%                        ] nsys-report-fc5f.nsys-rep[1/1] [=17%                        ] nsys-report-fc5f.nsys-rep[1/1] [==18%                       ] nsys-report-fc5f.nsys-rep[1/1] [==19%                       ] nsys-report-fc5f.nsys-rep[1/1] [==20%                       ] nsys-report-fc5f.nsys-rep[1/1] [==21%                       ] nsys-report-fc5f.nsys-rep[1/1] [===22%                      ] nsys-report-fc5f.nsys-rep[1/1] [===23%                      ] nsys-report-fc5f.nsys-rep[1/1] [===24%                      ] nsys-report-fc5f.nsys-rep[1/1] [====25%                     ] nsys-report-fc5f.nsys-rep[1/1] [====26%                     ] nsys-report-fc5f.nsys-rep[1/1] [====27%                     ] nsys-report-fc5f.nsys-rep[1/1] [====28%                     ] nsys-report-fc5f.nsys-rep[1/1] [=====29%                    ] nsys-report-fc5f.nsys-rep[1/1] [=====30%                    ] nsys-report-fc5f.nsys-rep[1/1] [=====31%                    ] nsys-report-fc5f.nsys-rep[1/1] [=====32%                    ] nsys-report-fc5f.nsys-rep[1/1] [======33%                   ] nsys-report-fc5f.nsys-rep[1/1] [======34%                   ] nsys-report-fc5f.nsys-rep[1/1] [======35%                   ] nsys-report-fc5f.nsys-rep[1/1] [=======36%                  ] nsys-report-fc5f.nsys-rep[1/1] [=======37%                  ] nsys-report-fc5f.nsys-rep[1/1] [=======38%                  ] nsys-report-fc5f.nsys-rep[1/1] [=======39%                  ] nsys-report-fc5f.nsys-rep[1/1] [========40%                 ] nsys-report-fc5f.nsys-rep[1/1] [========41%                 ] nsys-report-fc5f.nsys-rep[1/1] [========42%                 ] nsys-report-fc5f.nsys-rep[1/1] [=========43%                ] nsys-report-fc5f.nsys-rep[1/1] [=========44%                ] nsys-report-fc5f.nsys-rep[1/1] [=========45%                ] nsys-report-fc5f.nsys-rep[1/1] [=========46%                ] nsys-report-fc5f.nsys-rep[1/1] [==========47%               ] nsys-report-fc5f.nsys-rep[1/1] [==========48%               ] nsys-report-fc5f.nsys-rep[1/1] [==========49%               ] nsys-report-fc5f.nsys-rep[1/1] [===========50%              ] nsys-report-fc5f.nsys-rep[1/1] [===========51%              ] nsys-report-fc5f.nsys-rep[1/1] [===========52%              ] nsys-report-fc5f.nsys-rep[1/1] [===========53%              ] nsys-report-fc5f.nsys-rep[1/1] [============54%             ] nsys-report-fc5f.nsys-rep[1/1] [============55%             ] nsys-report-fc5f.nsys-rep[1/1] [============56%             ] nsys-report-fc5f.nsys-rep[1/1] [============57%             ] nsys-report-fc5f.nsys-rep[1/1] [=============58%            ] nsys-report-fc5f.nsys-rep[1/1] [=============59%            ] nsys-report-fc5f.nsys-rep[1/1] [=============60%            ] nsys-report-fc5f.nsys-rep[1/1] [==============61%           ] nsys-report-fc5f.nsys-rep[1/1] [==============62%           ] nsys-report-fc5f.nsys-rep[1/1] [==============63%           ] nsys-report-fc5f.nsys-rep[1/1] [==============64%           ] nsys-report-fc5f.nsys-rep[1/1] [===============65%          ] nsys-report-fc5f.nsys-rep[1/1] [===============66%          ] nsys-report-fc5f.nsys-rep[1/1] [===============67%          ] nsys-report-fc5f.nsys-rep[1/1] [================68%         ] nsys-report-fc5f.nsys-rep[1/1] [================69%         ] nsys-report-fc5f.nsys-rep[1/1] [================70%         ] nsys-report-fc5f.nsys-rep[1/1] [================71%         ] nsys-report-fc5f.nsys-rep[1/1] [=================72%        ] nsys-report-fc5f.nsys-rep[1/1] [=================73%        ] nsys-report-fc5f.nsys-rep[1/1] [=================74%        ] nsys-report-fc5f.nsys-rep[1/1] [==================75%       ] nsys-report-fc5f.nsys-rep[1/1] [==================76%       ] nsys-report-fc5f.nsys-rep[1/1] [==================77%       ] nsys-report-fc5f.nsys-rep[1/1] [==================78%       ] nsys-report-fc5f.nsys-rep[1/1] [===================79%      ] nsys-report-fc5f.nsys-rep[1/1] [===================80%      ] nsys-report-fc5f.nsys-rep[1/1] [===================81%      ] nsys-report-fc5f.nsys-rep[1/1] [========================100%] nsys-report-fc5f.nsys-rep[1/1] [========================100%] nsys-report-fc5f.nsys-rep
Generated:
	/tmp/nsys-report-fc5f.nsys-rep
Run  2: 99.730553736 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.32s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:39, 13.02s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.93s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.83s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.60s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.86s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 27.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 70.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:10, 86.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 97.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 107.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:07, 114.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 119.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 122.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 124.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 126.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 128.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 129.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 129.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 130.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:05, 130.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 130.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 129.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 130.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 129.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 130.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 130.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 130.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:02<00:05, 130.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:04, 131.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 131.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 130.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 130.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 131.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 130.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 130.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 130.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 131.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 130.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 130.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 130.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 130.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 130.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 130.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 129.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 130.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 130.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 130.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 130.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 131.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 130.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 130.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 131.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 131.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 131.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 130.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:05<00:02, 132.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 131.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 131.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 131.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 131.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 131.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 131.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 131.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 130.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:06<00:01, 130.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 130.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 130.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 130.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 126.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 124.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 124.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 138.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 144.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 123.76 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747613918.2601674
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:17, 19.22s/it]  7%|▋         | 2/30 [00:21<04:17,  9.19s/it] 10%|█         | 3/30 [00:23<02:41,  5.99s/it] 13%|█▎        | 4/30 [00:25<01:56,  4.48s/it] 17%|█▋        | 5/30 [00:28<01:32,  3.70s/it] 20%|██        | 6/30 [00:30<01:16,  3.20s/it] 23%|██▎       | 7/30 [00:32<01:06,  2.88s/it] 27%|██▋       | 8/30 [00:34<00:58,  2.65s/it] 30%|███       | 9/30 [00:37<00:54,  2.57s/it] 33%|███▎      | 10/30 [00:39<00:49,  2.46s/it] 37%|███▋      | 11/30 [00:41<00:45,  2.37s/it] 40%|████      | 12/30 [00:43<00:41,  2.30s/it] 43%|████▎     | 13/30 [00:45<00:38,  2.26s/it] 47%|████▋     | 14/30 [00:48<00:36,  2.30s/it] 50%|█████     | 15/30 [00:50<00:34,  2.28s/it]                                                50%|█████     | 15/30 [00:50<00:34,  2.28s/it] 53%|█████▎    | 16/30 [00:52<00:31,  2.24s/it] 57%|█████▋    | 17/30 [00:54<00:28,  2.21s/it] 60%|██████    | 18/30 [00:56<00:26,  2.24s/it] 63%|██████▎   | 19/30 [00:59<00:24,  2.22s/it] 67%|██████▋   | 20/30 [01:01<00:21,  2.20s/it] 70%|███████   | 21/30 [01:03<00:19,  2.18s/it] 73%|███████▎  | 22/30 [01:05<00:17,  2.17s/it] 77%|███████▋  | 23/30 [01:07<00:15,  2.16s/it] 80%|████████  | 24/30 [01:10<00:13,  2.21s/it] 83%|████████▎ | 25/30 [01:12<00:10,  2.19s/it] 87%|████████▋ | 26/30 [01:14<00:08,  2.23s/it] 90%|█████████ | 27/30 [01:16<00:06,  2.21s/it] 93%|█████████▎| 28/30 [01:18<00:04,  2.20s/it] 97%|█████████▋| 29/30 [01:21<00:02,  2.18s/it]100%|██████████| 30/30 [01:23<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:23<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:23<00:00,  2.20s/it]100%|██████████| 30/30 [01:23<00:00,  2.77s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0414, 'grad_norm': 0.25635844469070435, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8184, 'grad_norm': 0.1298394352197647, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 83.2436, 'train_samples_per_second': 2.883, 'train_steps_per_second': 0.36, 'train_loss': 0.9299102147420247, 'epoch': 0.24}
END_PROFILE: 1747614002.0031974
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/qwen3-14b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-917e.qdstrm'
[1/1] [0%                          ] nsys-report-3ac9.nsys-rep[1/1] [0%                          ] nsys-report-3ac9.nsys-rep[1/1] [0%                          ] nsys-report-3ac9.nsys-rep[1/1] [7%                          ] nsys-report-3ac9.nsys-rep[1/1] [6%                          ] nsys-report-3ac9.nsys-rep[1/1] [9%                          ] nsys-report-3ac9.nsys-rep[1/1] [8%                          ] nsys-report-3ac9.nsys-rep[1/1] [7%                          ] nsys-report-3ac9.nsys-rep[1/1] [6%                          ] nsys-report-3ac9.nsys-rep[1/1] [5%                          ] nsys-report-3ac9.nsys-rep[1/1] [5%                          ] nsys-report-3ac9.nsys-rep[1/1] [6%                          ] nsys-report-3ac9.nsys-rep[1/1] [7%                          ] nsys-report-3ac9.nsys-rep[1/1] [8%                          ] nsys-report-3ac9.nsys-rep[1/1] [9%                          ] nsys-report-3ac9.nsys-rep[1/1] [10%                         ] nsys-report-3ac9.nsys-rep[1/1] [11%                         ] nsys-report-3ac9.nsys-rep[1/1] [12%                         ] nsys-report-3ac9.nsys-rep[1/1] [13%                         ] nsys-report-3ac9.nsys-rep[1/1] [14%                         ] nsys-report-3ac9.nsys-rep[1/1] [=15%                        ] nsys-report-3ac9.nsys-rep[1/1] [=16%                        ] nsys-report-3ac9.nsys-rep[1/1] [=17%                        ] nsys-report-3ac9.nsys-rep[1/1] [==18%                       ] nsys-report-3ac9.nsys-rep[1/1] [==19%                       ] nsys-report-3ac9.nsys-rep[1/1] [==20%                       ] nsys-report-3ac9.nsys-rep[1/1] [==21%                       ] nsys-report-3ac9.nsys-rep[1/1] [===22%                      ] nsys-report-3ac9.nsys-rep[1/1] [===23%                      ] nsys-report-3ac9.nsys-rep[1/1] [===24%                      ] nsys-report-3ac9.nsys-rep[1/1] [====25%                     ] nsys-report-3ac9.nsys-rep[1/1] [====26%                     ] nsys-report-3ac9.nsys-rep[1/1] [====27%                     ] nsys-report-3ac9.nsys-rep[1/1] [====28%                     ] nsys-report-3ac9.nsys-rep[1/1] [=====29%                    ] nsys-report-3ac9.nsys-rep[1/1] [=====30%                    ] nsys-report-3ac9.nsys-rep[1/1] [=====31%                    ] nsys-report-3ac9.nsys-rep[1/1] [=====32%                    ] nsys-report-3ac9.nsys-rep[1/1] [======33%                   ] nsys-report-3ac9.nsys-rep[1/1] [======34%                   ] nsys-report-3ac9.nsys-rep[1/1] [======35%                   ] nsys-report-3ac9.nsys-rep[1/1] [=======36%                  ] nsys-report-3ac9.nsys-rep[1/1] [=======37%                  ] nsys-report-3ac9.nsys-rep[1/1] [=======38%                  ] nsys-report-3ac9.nsys-rep[1/1] [=======39%                  ] nsys-report-3ac9.nsys-rep[1/1] [========40%                 ] nsys-report-3ac9.nsys-rep[1/1] [========41%                 ] nsys-report-3ac9.nsys-rep[1/1] [========42%                 ] nsys-report-3ac9.nsys-rep[1/1] [=========43%                ] nsys-report-3ac9.nsys-rep[1/1] [=========44%                ] nsys-report-3ac9.nsys-rep[1/1] [=========45%                ] nsys-report-3ac9.nsys-rep[1/1] [=========46%                ] nsys-report-3ac9.nsys-rep[1/1] [==========47%               ] nsys-report-3ac9.nsys-rep[1/1] [==========48%               ] nsys-report-3ac9.nsys-rep[1/1] [==========49%               ] nsys-report-3ac9.nsys-rep[1/1] [===========50%              ] nsys-report-3ac9.nsys-rep[1/1] [===========51%              ] nsys-report-3ac9.nsys-rep[1/1] [===========52%              ] nsys-report-3ac9.nsys-rep[1/1] [===========53%              ] nsys-report-3ac9.nsys-rep[1/1] [============54%             ] nsys-report-3ac9.nsys-rep[1/1] [============55%             ] nsys-report-3ac9.nsys-rep[1/1] [============56%             ] nsys-report-3ac9.nsys-rep[1/1] [============57%             ] nsys-report-3ac9.nsys-rep[1/1] [=============58%            ] nsys-report-3ac9.nsys-rep[1/1] [=============59%            ] nsys-report-3ac9.nsys-rep[1/1] [=============60%            ] nsys-report-3ac9.nsys-rep[1/1] [==============61%           ] nsys-report-3ac9.nsys-rep[1/1] [==============62%           ] nsys-report-3ac9.nsys-rep[1/1] [==============63%           ] nsys-report-3ac9.nsys-rep[1/1] [==============64%           ] nsys-report-3ac9.nsys-rep[1/1] [===============65%          ] nsys-report-3ac9.nsys-rep[1/1] [===============66%          ] nsys-report-3ac9.nsys-rep[1/1] [===============67%          ] nsys-report-3ac9.nsys-rep[1/1] [================68%         ] nsys-report-3ac9.nsys-rep[1/1] [================69%         ] nsys-report-3ac9.nsys-rep[1/1] [================70%         ] nsys-report-3ac9.nsys-rep[1/1] [================71%         ] nsys-report-3ac9.nsys-rep[1/1] [=================72%        ] nsys-report-3ac9.nsys-rep[1/1] [=================73%        ] nsys-report-3ac9.nsys-rep[1/1] [=================74%        ] nsys-report-3ac9.nsys-rep[1/1] [==================75%       ] nsys-report-3ac9.nsys-rep[1/1] [==================76%       ] nsys-report-3ac9.nsys-rep[1/1] [==================77%       ] nsys-report-3ac9.nsys-rep[1/1] [==================78%       ] nsys-report-3ac9.nsys-rep[1/1] [===================79%      ] nsys-report-3ac9.nsys-rep[1/1] [===================80%      ] nsys-report-3ac9.nsys-rep[1/1] [===================81%      ] nsys-report-3ac9.nsys-rep[1/1] [========================100%] nsys-report-3ac9.nsys-rep[1/1] [========================100%] nsys-report-3ac9.nsys-rep
Generated:
	/tmp/nsys-report-3ac9.nsys-rep
Run  3: 98.639140971 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 98.639140971 seconds
  Max elapsed : 99.730553736 seconds
  Avg elapsed : 99.36056023166666666666 seconds
===================================
--------------------------------------------
PROTON

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:12<01:04, 12.85s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:25<00:51, 12.80s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:38<00:38, 12.71s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:50<00:25, 12.74s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:03<00:12, 12.70s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.36s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.56s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 27.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 70.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:10, 85.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 97.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 105.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:07, 112.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 117.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 120.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:07, 122.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 124.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 125.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 126.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 126.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 127.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 128.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 128.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 128.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 128.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 128.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 129.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 128.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 128.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 128.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 128.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 128.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 129.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 128.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 129.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 129.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 129.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 129.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 129.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 129.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 129.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 128.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 129.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 128.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 127.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 126.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 128.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 128.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 129.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 128.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 129.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 129.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 129.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 129.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 129.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 129.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 129.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 129.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 129.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 129.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 129.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 129.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 129.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 125.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 122.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 120.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 119.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 118.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 119.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 122.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 122.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 120.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 120.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 120.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:08<00:00, 135.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 145.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 121.44 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747614167.2065492
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<11:05, 22.94s/it]  7%|▋         | 2/30 [00:25<04:59, 10.69s/it] 10%|█         | 3/30 [00:27<03:02,  6.78s/it] 13%|█▎        | 4/30 [00:29<02:08,  4.93s/it] 17%|█▋        | 5/30 [00:31<01:40,  4.01s/it] 20%|██        | 6/30 [00:33<01:21,  3.39s/it] 23%|██▎       | 7/30 [00:35<01:08,  2.98s/it] 27%|██▋       | 8/30 [00:38<00:59,  2.70s/it] 30%|███       | 9/30 [00:40<00:54,  2.61s/it] 33%|███▎      | 10/30 [00:42<00:50,  2.53s/it] 37%|███▋      | 11/30 [00:45<00:46,  2.43s/it] 40%|████      | 12/30 [00:47<00:42,  2.34s/it] 43%|████▎     | 13/30 [00:49<00:38,  2.27s/it] 47%|████▋     | 14/30 [00:51<00:36,  2.27s/it] 50%|█████     | 15/30 [00:53<00:33,  2.23s/it]                                                50%|█████     | 15/30 [00:53<00:33,  2.23s/it] 53%|█████▎    | 16/30 [00:55<00:30,  2.18s/it] 57%|█████▋    | 17/30 [00:57<00:27,  2.15s/it] 60%|██████    | 18/30 [01:00<00:26,  2.18s/it] 63%|██████▎   | 19/30 [01:02<00:24,  2.24s/it] 67%|██████▋   | 20/30 [01:04<00:22,  2.23s/it] 70%|███████   | 21/30 [01:06<00:19,  2.21s/it] 73%|███████▎  | 22/30 [01:09<00:17,  2.19s/it] 77%|███████▋  | 23/30 [01:11<00:15,  2.21s/it] 80%|████████  | 24/30 [01:13<00:13,  2.23s/it] 83%|████████▎ | 25/30 [01:15<00:10,  2.17s/it] 87%|████████▋ | 26/30 [01:17<00:08,  2.20s/it] 90%|█████████ | 27/30 [01:19<00:06,  2.17s/it] 93%|█████████▎| 28/30 [01:22<00:04,  2.22s/it] 97%|█████████▋| 29/30 [01:24<00:02,  2.22s/it]100%|██████████| 30/30 [01:26<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:26<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:26<00:00,  2.20s/it]100%|██████████| 30/30 [01:26<00:00,  2.89s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0416, 'grad_norm': 0.25483274459838867, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8185, 'grad_norm': 0.12933491170406342, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 86.6654, 'train_samples_per_second': 2.769, 'train_steps_per_second': 0.346, 'train_loss': 0.9300360679626465, 'epoch': 0.24}
END_PROFILE: 1747614254.628441
Run  1: 89.079330402 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:12<01:04, 12.88s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:25<00:51, 12.82s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:38<00:38, 12.73s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:51<00:25, 12.76s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:03<00:12, 12.72s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.52s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.65s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 27.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 69.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:11, 84.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 96.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 105.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:08, 111.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 116.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 119.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:07, 121.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 123.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 124.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 125.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 126.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 126.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 127.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 127.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 127.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 127.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 127.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 127.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 128.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 127.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 128.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 128.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 128.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 127.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 128.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 128.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 128.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 128.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 128.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:04<00:04, 128.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 127.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:04, 127.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 127.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 127.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 127.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 127.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 126.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 127.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:05<00:03, 126.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 126.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:03, 127.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 126.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 127.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 128.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 128.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 127.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 127.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 127.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 128.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:02, 128.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 127.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 128.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 128.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 127.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 127.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 127.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 127.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 127.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 127.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 127.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 127.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 124.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 122.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 121.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 121.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:08<00:00, 137.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 145.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 121.33 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747614387.54082
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:21<10:23, 21.52s/it]  7%|▋         | 2/30 [00:23<04:44, 10.15s/it] 10%|█         | 3/30 [00:25<02:55,  6.52s/it] 13%|█▎        | 4/30 [00:28<02:04,  4.79s/it] 17%|█▋        | 5/30 [00:30<01:38,  3.92s/it] 20%|██        | 6/30 [00:32<01:20,  3.34s/it] 23%|██▎       | 7/30 [00:34<01:08,  2.96s/it] 27%|██▋       | 8/30 [00:36<00:59,  2.70s/it] 30%|███       | 9/30 [00:39<00:54,  2.61s/it] 33%|███▎      | 10/30 [00:41<00:51,  2.56s/it] 37%|███▋      | 11/30 [00:44<00:47,  2.48s/it] 40%|████      | 12/30 [00:46<00:43,  2.39s/it] 43%|████▎     | 13/30 [00:48<00:39,  2.34s/it] 47%|████▋     | 14/30 [00:50<00:37,  2.33s/it] 50%|█████     | 15/30 [00:53<00:34,  2.29s/it]                                                50%|█████     | 15/30 [00:53<00:34,  2.29s/it] 53%|█████▎    | 16/30 [00:55<00:31,  2.24s/it] 57%|█████▋    | 17/30 [00:57<00:28,  2.21s/it] 60%|██████    | 18/30 [00:59<00:26,  2.24s/it] 63%|██████▎   | 19/30 [01:02<00:25,  2.31s/it] 67%|██████▋   | 20/30 [01:04<00:22,  2.29s/it] 70%|███████   | 21/30 [01:06<00:20,  2.27s/it] 73%|███████▎  | 22/30 [01:08<00:17,  2.25s/it] 77%|███████▋  | 23/30 [01:11<00:15,  2.26s/it] 80%|████████  | 24/30 [01:13<00:13,  2.28s/it] 83%|████████▎ | 25/30 [01:15<00:11,  2.23s/it] 87%|████████▋ | 26/30 [01:17<00:09,  2.26s/it] 90%|█████████ | 27/30 [01:19<00:06,  2.23s/it] 93%|█████████▎| 28/30 [01:22<00:04,  2.30s/it] 97%|█████████▋| 29/30 [01:24<00:02,  2.29s/it]100%|██████████| 30/30 [01:26<00:00,  2.28s/it]                                               100%|██████████| 30/30 [01:26<00:00,  2.28s/it]                                               100%|██████████| 30/30 [01:26<00:00,  2.28s/it]100%|██████████| 30/30 [01:26<00:00,  2.90s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0414, 'grad_norm': 0.25526922941207886, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8183, 'grad_norm': 0.12999169528484344, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 86.9681, 'train_samples_per_second': 2.76, 'train_steps_per_second': 0.345, 'train_loss': 0.9298542022705079, 'epoch': 0.24}
END_PROFILE: 1747614475.2877884
Run  2: 89.359779422 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:12<01:03, 12.77s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:25<00:51, 12.77s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:38<00:38, 12.71s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:50<00:25, 12.75s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:03<00:12, 12.71s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.52s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:15<00:00, 12.63s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:36, 27.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:18, 51.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 70.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:10, 86.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 99.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 107.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:07, 114.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 119.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 122.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 124.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 126.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 128.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 129.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 129.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 129.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:05, 130.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:05, 130.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:05, 130.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 131.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 131.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 131.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 131.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 130.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:02<00:05, 130.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:04, 130.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 131.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 130.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 130.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 130.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 131.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 131.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 131.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 131.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:03, 131.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 131.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 131.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 131.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 131.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 131.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 129.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 130.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 131.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:04<00:03, 131.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 132.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 131.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 131.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 131.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 131.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 131.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 131.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 131.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:05<00:02, 131.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 129.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 126.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 124.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 123.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 122.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 121.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 120.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 120.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:06<00:01, 123.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 125.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 127.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 128.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:07<00:00, 126.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 125.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 124.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 123.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 139.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 144.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 123.09 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747614608.0454578
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:22<10:50, 22.42s/it]  7%|▋         | 2/30 [00:24<04:53, 10.49s/it] 10%|█         | 3/30 [00:26<03:00,  6.68s/it] 13%|█▎        | 4/30 [00:28<02:06,  4.87s/it] 17%|█▋        | 5/30 [00:31<01:38,  3.95s/it] 20%|██        | 6/30 [00:33<01:20,  3.34s/it] 23%|██▎       | 7/30 [00:35<01:07,  2.94s/it] 27%|██▋       | 8/30 [00:37<00:58,  2.66s/it] 30%|███       | 9/30 [00:39<00:53,  2.55s/it] 33%|███▎      | 10/30 [00:42<00:49,  2.47s/it] 37%|███▋      | 11/30 [00:44<00:45,  2.38s/it] 40%|████      | 12/30 [00:46<00:41,  2.28s/it] 43%|████▎     | 13/30 [00:48<00:37,  2.22s/it] 47%|████▋     | 14/30 [00:50<00:35,  2.21s/it] 50%|█████     | 15/30 [00:52<00:32,  2.17s/it]                                                50%|█████     | 15/30 [00:52<00:32,  2.17s/it] 53%|█████▎    | 16/30 [00:54<00:29,  2.13s/it] 57%|█████▋    | 17/30 [00:56<00:27,  2.10s/it] 60%|██████    | 18/30 [00:58<00:25,  2.13s/it] 63%|██████▎   | 19/30 [01:01<00:24,  2.19s/it] 67%|██████▋   | 20/30 [01:03<00:21,  2.18s/it] 70%|███████   | 21/30 [01:05<00:19,  2.16s/it] 73%|███████▎  | 22/30 [01:07<00:17,  2.14s/it] 77%|███████▋  | 23/30 [01:09<00:15,  2.16s/it] 80%|████████  | 24/30 [01:12<00:13,  2.18s/it] 83%|████████▎ | 25/30 [01:14<00:10,  2.13s/it] 87%|████████▋ | 26/30 [01:16<00:08,  2.16s/it] 90%|█████████ | 27/30 [01:18<00:06,  2.13s/it] 93%|█████████▎| 28/30 [01:20<00:04,  2.18s/it] 97%|█████████▋| 29/30 [01:22<00:02,  2.17s/it]100%|██████████| 30/30 [01:24<00:00,  2.17s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.17s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.17s/it]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0414, 'grad_norm': 0.25465038418769836, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8185, 'grad_norm': 0.13011953234672546, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 84.9466, 'train_samples_per_second': 2.825, 'train_steps_per_second': 0.353, 'train_loss': 0.9299306233723958, 'epoch': 0.24}
END_PROFILE: 1747614693.808241
Run  3: 87.397578205 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 87.397578205 seconds
  Max elapsed : 89.359779422 seconds
  Avg elapsed : 88.61222934300000000000 seconds
===================================
--------------------------------------------
TORCH

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.35s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:39, 13.02s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.90s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.80s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.58s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:17<00:00, 12.84s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:35, 27.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:18, 51.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 71.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:10, 87.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 99.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 108.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:07, 114.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 119.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 122.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 125.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 125.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 124.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 123.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 122.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 122.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 121.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:06, 121.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:06, 121.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 122.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 124.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 126.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 127.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 128.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 128.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 129.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 129.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 130.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 130.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 130.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 131.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 131.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 130.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:03<00:04, 130.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 130.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 130.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 131.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 130.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 130.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 130.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 128.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 128.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 130.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 130.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 130.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 129.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 130.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 130.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 129.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 129.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 129.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 129.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 130.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 130.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 130.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 130.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 130.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 130.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 130.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 130.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 129.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 129.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 130.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 130.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 130.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 126.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 125.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 124.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 138.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 147.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 122.90 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747614850.0840063
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:55, 20.54s/it]  7%|▋         | 2/30 [00:22<04:35,  9.84s/it] 10%|█         | 3/30 [00:25<02:53,  6.41s/it] 13%|█▎        | 4/30 [00:27<02:04,  4.80s/it] 17%|█▋        | 5/30 [00:30<01:39,  3.99s/it] 20%|██        | 6/30 [00:32<01:22,  3.45s/it] 23%|██▎       | 7/30 [00:34<01:11,  3.09s/it] 27%|██▋       | 8/30 [00:37<01:02,  2.84s/it] 30%|███       | 9/30 [00:39<00:57,  2.74s/it] 33%|███▎      | 10/30 [00:42<00:52,  2.61s/it] 37%|███▋      | 11/30 [00:44<00:47,  2.52s/it] 40%|████      | 12/30 [00:46<00:44,  2.45s/it] 43%|████▎     | 13/30 [00:48<00:40,  2.40s/it] 47%|████▋     | 14/30 [00:51<00:38,  2.40s/it] 50%|█████     | 15/30 [00:53<00:35,  2.38s/it]                                                50%|█████     | 15/30 [00:53<00:35,  2.38s/it] 53%|█████▎    | 16/30 [00:55<00:33,  2.36s/it] 57%|█████▋    | 17/30 [00:58<00:30,  2.34s/it] 60%|██████    | 18/30 [01:00<00:28,  2.37s/it] 63%|██████▎   | 19/30 [01:02<00:25,  2.35s/it] 67%|██████▋   | 20/30 [01:05<00:23,  2.34s/it] 70%|███████   | 21/30 [01:07<00:20,  2.33s/it] 73%|███████▎  | 22/30 [01:09<00:18,  2.32s/it] 77%|███████▋  | 23/30 [01:12<00:16,  2.30s/it] 80%|████████  | 24/30 [01:14<00:14,  2.34s/it] 83%|████████▎ | 25/30 [01:16<00:11,  2.33s/it] 87%|████████▋ | 26/30 [01:19<00:09,  2.36s/it] 90%|█████████ | 27/30 [01:21<00:07,  2.35s/it] 93%|█████████▎| 28/30 [01:23<00:04,  2.34s/it] 97%|█████████▋| 29/30 [01:26<00:02,  2.33s/it]100%|██████████| 30/30 [01:28<00:00,  2.33s/it]                                               100%|██████████| 30/30 [01:28<00:00,  2.33s/it]                                               100%|██████████| 30/30 [01:28<00:00,  2.33s/it]100%|██████████| 30/30 [01:28<00:00,  2.95s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0417, 'grad_norm': 0.257609099149704, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8184, 'grad_norm': 0.13045205175876617, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 88.6263, 'train_samples_per_second': 2.708, 'train_steps_per_second': 0.338, 'train_loss': 0.9300467491149902, 'epoch': 0.24}
END_PROFILE: 1747614956.0324962
Run  1: 108.192935163 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:14<01:10, 14.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:53, 13.32s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.97s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:52<00:25, 12.88s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.79s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.57s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.83s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:34, 28.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:18, 52.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 72.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:10, 88.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 99.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 108.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:07, 115.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 120.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 122.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:06, 125.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:06, 126.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 124.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 123.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 122.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 120.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 120.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:06, 120.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:06, 123.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:05, 126.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 127.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 128.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 130.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 124.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 122.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:05, 120.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:05, 119.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:05, 123.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 125.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 126.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 128.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 128.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 124.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:04<00:04, 122.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:04, 120.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:04, 122.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 125.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 127.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 127.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 129.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 129.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 129.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:05<00:03, 129.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 130.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 130.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 130.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 132.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 131.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 131.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 130.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 131.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 131.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 131.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 131.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 131.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 131.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 130.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 130.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 130.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 130.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 130.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:07<00:01, 130.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:01, 130.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 131.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 131.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 126.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 125.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 124.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 138.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 145.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 122.04 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747615091.7805667
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<10:02, 20.77s/it]  7%|▋         | 2/30 [00:22<04:34,  9.81s/it] 10%|█         | 3/30 [00:25<02:50,  6.31s/it] 13%|█▎        | 4/30 [00:27<02:01,  4.66s/it] 17%|█▋        | 5/30 [00:29<01:36,  3.86s/it] 20%|██        | 6/30 [00:31<01:19,  3.32s/it] 23%|██▎       | 7/30 [00:34<01:08,  2.96s/it] 27%|██▋       | 8/30 [00:36<00:59,  2.71s/it] 30%|███       | 9/30 [00:38<00:54,  2.62s/it] 33%|███▎      | 10/30 [00:40<00:49,  2.49s/it] 37%|███▋      | 11/30 [00:43<00:45,  2.39s/it] 40%|████      | 12/30 [00:45<00:41,  2.31s/it] 43%|████▎     | 13/30 [00:47<00:38,  2.27s/it] 47%|████▋     | 14/30 [00:49<00:36,  2.27s/it] 50%|█████     | 15/30 [00:51<00:33,  2.24s/it]                                                50%|█████     | 15/30 [00:51<00:33,  2.24s/it] 53%|█████▎    | 16/30 [00:54<00:31,  2.22s/it] 57%|█████▋    | 17/30 [00:56<00:28,  2.20s/it] 60%|██████    | 18/30 [00:58<00:26,  2.24s/it] 63%|██████▎   | 19/30 [01:00<00:24,  2.22s/it] 67%|██████▋   | 20/30 [01:02<00:22,  2.20s/it] 70%|███████   | 21/30 [01:04<00:19,  2.19s/it] 73%|███████▎  | 22/30 [01:07<00:17,  2.18s/it] 77%|███████▋  | 23/30 [01:09<00:15,  2.17s/it] 80%|████████  | 24/30 [01:11<00:13,  2.22s/it] 83%|████████▎ | 25/30 [01:13<00:11,  2.21s/it] 87%|████████▋ | 26/30 [01:16<00:08,  2.24s/it] 90%|█████████ | 27/30 [01:18<00:06,  2.23s/it] 93%|█████████▎| 28/30 [01:20<00:04,  2.21s/it] 97%|█████████▋| 29/30 [01:22<00:02,  2.20s/it]100%|██████████| 30/30 [01:24<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.20s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.20s/it]100%|██████████| 30/30 [01:24<00:00,  2.83s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0416, 'grad_norm': 0.2579704821109772, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8185, 'grad_norm': 0.12980443239212036, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 84.8772, 'train_samples_per_second': 2.828, 'train_steps_per_second': 0.353, 'train_loss': 0.9300565401713053, 'epoch': 0.24}
END_PROFILE: 1747615193.931258
Run  2: 104.442577228 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for qwen3-14b...
==((====))==  Unsloth 2025.5.3: Fast Qwen3 patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:13<01:08, 13.68s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:26<00:52, 13.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:39<00:38, 12.92s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:51<00:25, 12.88s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [01:04<00:12, 12.80s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.60s/it]Loading checkpoint shards: 100%|██████████| 6/6 [01:16<00:00, 12.80s/it]
Adding LoRA adapters for qwen3-14b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:35, 27.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   3%|▎         | 28/1000 [00:00<00:19, 50.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   4%|▍         | 42/1000 [00:00<00:13, 69.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:11, 84.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:09, 95.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   8%|▊         | 84/1000 [00:01<00:08, 103.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:01<00:08, 108.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:01<00:07, 112.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:01<00:07, 115.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:01<00:07, 117.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:01<00:07, 120.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:01<00:06, 123.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:01<00:06, 125.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:01<00:06, 127.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:02<00:06, 128.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:02<00:06, 126.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:02<00:06, 124.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:02<00:06, 123.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:02<00:06, 122.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:02<00:05, 124.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:02<00:05, 126.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:02<00:05, 128.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:02<00:05, 129.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:03<00:05, 129.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:03<00:04, 130.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:03<00:04, 130.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:03<00:04, 130.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:03<00:04, 130.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:03<00:04, 131.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:03<00:04, 131.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  43%|████▎     | 434/1000 [00:03<00:04, 131.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:03<00:04, 131.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:04<00:04, 131.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:04<00:03, 131.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:04<00:03, 130.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:04<00:03, 131.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:04<00:03, 131.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:04<00:03, 131.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  55%|█████▍    | 546/1000 [00:04<00:03, 131.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:04<00:03, 130.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:04<00:03, 131.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:04<00:03, 131.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  60%|██████    | 602/1000 [00:05<00:03, 131.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:05<00:02, 131.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:05<00:02, 131.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:05<00:02, 133.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  66%|██████▌   | 658/1000 [00:05<00:02, 134.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:05<00:02, 131.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:05<00:02, 131.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:05<00:02, 131.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  71%|███████▏  | 714/1000 [00:05<00:02, 131.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:06<00:02, 132.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:06<00:01, 132.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:06<00:01, 132.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  77%|███████▋  | 770/1000 [00:06<00:01, 132.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:06<00:01, 131.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:06<00:01, 131.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:06<00:01, 131.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  83%|████████▎ | 826/1000 [00:06<00:01, 132.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:06<00:01, 132.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:06<00:01, 132.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:07<00:00, 132.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  88%|████████▊ | 882/1000 [00:07<00:00, 132.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:07<00:00, 132.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:07<00:00, 127.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  94%|█████████▎| 935/1000 [00:07<00:00, 126.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:07<00:00, 125.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:07<00:00, 139.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 148.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:08<00:00, 123.29 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 32,112,640/14,800,419,840 (0.22% trained)
START_PROFILE: 1747615332.3093495
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:34, 19.80s/it]  7%|▋         | 2/30 [00:21<04:24,  9.44s/it] 10%|█         | 3/30 [00:24<02:45,  6.13s/it] 13%|█▎        | 4/30 [00:26<01:58,  4.57s/it] 17%|█▋        | 5/30 [00:28<01:34,  3.79s/it] 20%|██        | 6/30 [00:31<01:18,  3.27s/it] 23%|██▎       | 7/30 [00:33<01:07,  2.94s/it] 27%|██▋       | 8/30 [00:35<00:59,  2.69s/it] 30%|███       | 9/30 [00:37<00:54,  2.60s/it] 33%|███▎      | 10/30 [00:40<00:49,  2.47s/it] 37%|███▋      | 11/30 [00:42<00:45,  2.37s/it] 40%|████      | 12/30 [00:44<00:41,  2.31s/it] 43%|████▎     | 13/30 [00:46<00:38,  2.26s/it] 47%|████▋     | 14/30 [00:48<00:36,  2.27s/it] 50%|█████     | 15/30 [00:50<00:33,  2.25s/it]                                                50%|█████     | 15/30 [00:50<00:33,  2.25s/it] 53%|█████▎    | 16/30 [00:53<00:31,  2.23s/it] 57%|█████▋    | 17/30 [00:55<00:28,  2.20s/it] 60%|██████    | 18/30 [00:57<00:26,  2.24s/it] 63%|██████▎   | 19/30 [00:59<00:24,  2.23s/it] 67%|██████▋   | 20/30 [01:02<00:22,  2.22s/it] 70%|███████   | 21/30 [01:04<00:19,  2.21s/it] 73%|███████▎  | 22/30 [01:06<00:17,  2.21s/it] 77%|███████▋  | 23/30 [01:08<00:15,  2.20s/it] 80%|████████  | 24/30 [01:10<00:13,  2.25s/it] 83%|████████▎ | 25/30 [01:13<00:11,  2.23s/it] 87%|████████▋ | 26/30 [01:15<00:09,  2.27s/it] 90%|█████████ | 27/30 [01:17<00:06,  2.26s/it] 93%|█████████▎| 28/30 [01:19<00:04,  2.24s/it] 97%|█████████▋| 29/30 [01:22<00:02,  2.23s/it]100%|██████████| 30/30 [01:24<00:00,  2.23s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.23s/it]                                               100%|██████████| 30/30 [01:24<00:00,  2.23s/it]100%|██████████| 30/30 [01:24<00:00,  2.81s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.0414, 'grad_norm': 0.2561299204826355, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8185, 'grad_norm': 0.12990421056747437, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 84.374, 'train_samples_per_second': 2.844, 'train_steps_per_second': 0.356, 'train_loss': 0.9299407641092936, 'epoch': 0.24}
END_PROFILE: 1747615434.1775603
Run  3: 104.127894885 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 104.127894885 seconds
  Max elapsed : 108.192935163 seconds
  Avg elapsed : 105.58780242533333333333 seconds
===================================
--------------------------------------------
--------------------------------------------
Timing mistral-7b with NONE

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.31s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.58s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:14, 70.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 213.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:03, 289.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  14%|█▍        | 140/1000 [00:00<00:02, 333.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:00<00:02, 370.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  25%|██▌       | 252/1000 [00:00<00:01, 389.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:00<00:01, 396.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:00<00:01, 407.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:01<00:01, 412.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:01<00:01, 415.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:01<00:01, 420.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:01<00:01, 419.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:01<00:00, 422.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:01<00:00, 423.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:01<00:00, 424.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:02<00:00, 423.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:02<00:00, 424.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:02<00:00, 425.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:02<00:00, 434.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 388.20 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747615540.9848785
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:43, 18.05s/it]  7%|▋         | 2/30 [00:19<03:47,  8.14s/it] 10%|█         | 3/30 [00:20<02:14,  4.98s/it] 13%|█▎        | 4/30 [00:21<01:30,  3.49s/it] 17%|█▋        | 5/30 [00:23<01:08,  2.73s/it] 20%|██        | 6/30 [00:24<00:53,  2.24s/it] 23%|██▎       | 7/30 [00:25<00:43,  1.90s/it] 27%|██▋       | 8/30 [00:26<00:36,  1.68s/it] 30%|███       | 9/30 [00:28<00:32,  1.57s/it] 33%|███▎      | 10/30 [00:29<00:29,  1.46s/it] 37%|███▋      | 11/30 [00:30<00:26,  1.39s/it] 40%|████      | 12/30 [00:31<00:23,  1.33s/it] 43%|████▎     | 13/30 [00:32<00:21,  1.28s/it] 47%|████▋     | 14/30 [00:34<00:20,  1.26s/it] 50%|█████     | 15/30 [00:35<00:18,  1.24s/it]                                                50%|█████     | 15/30 [00:35<00:18,  1.24s/it] 53%|█████▎    | 16/30 [00:36<00:17,  1.24s/it] 57%|█████▋    | 17/30 [00:37<00:15,  1.22s/it] 60%|██████    | 18/30 [00:38<00:14,  1.25s/it] 63%|██████▎   | 19/30 [00:40<00:13,  1.23s/it] 67%|██████▋   | 20/30 [00:41<00:12,  1.22s/it] 70%|███████   | 21/30 [00:42<00:10,  1.21s/it] 73%|███████▎  | 22/30 [00:43<00:09,  1.21s/it] 77%|███████▋  | 23/30 [00:44<00:08,  1.21s/it] 80%|████████  | 24/30 [00:46<00:07,  1.24s/it] 83%|████████▎ | 25/30 [00:47<00:06,  1.22s/it] 87%|████████▋ | 26/30 [00:48<00:04,  1.23s/it] 90%|█████████ | 27/30 [00:49<00:03,  1.23s/it] 93%|█████████▎| 28/30 [00:51<00:02,  1.22s/it] 97%|█████████▋| 29/30 [00:52<00:01,  1.21s/it]100%|██████████| 30/30 [00:53<00:00,  1.21s/it]                                               100%|██████████| 30/30 [00:53<00:00,  1.21s/it]                                               100%|██████████| 30/30 [00:53<00:00,  1.21s/it]100%|██████████| 30/30 [00:53<00:00,  1.78s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9527, 'grad_norm': 0.464251309633255, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8668, 'grad_norm': 0.33162376284599304, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 53.5389, 'train_samples_per_second': 4.483, 'train_steps_per_second': 0.56, 'train_loss': 0.9097311337788899, 'epoch': 0.24}
END_PROFILE: 1747615595.0075188
Run  1: 55.402814115 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.30s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:12, 13.00s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.57s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:14, 68.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 210.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:03, 286.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:00<00:02, 341.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:00<00:02, 373.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:00<00:01, 391.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  31%|███       | 308/1000 [00:00<00:01, 398.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:01<00:01, 408.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:01<00:01, 414.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:01<00:01, 415.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:01<00:01, 419.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:01<00:01, 420.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:01<00:00, 419.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:01<00:00, 422.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:01<00:00, 421.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:02<00:00, 422.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:02<00:00, 422.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:02<00:00, 419.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:02<00:00, 425.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 386.11 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747615679.060478
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:17<08:21, 17.28s/it]  7%|▋         | 2/30 [00:18<03:39,  7.84s/it] 10%|█         | 3/30 [00:19<02:10,  4.84s/it] 13%|█▎        | 4/30 [00:20<01:28,  3.40s/it] 17%|█▋        | 5/30 [00:22<01:06,  2.67s/it] 20%|██        | 6/30 [00:23<00:52,  2.19s/it] 23%|██▎       | 7/30 [00:24<00:42,  1.86s/it] 27%|██▋       | 8/30 [00:26<00:36,  1.65s/it] 30%|███       | 9/30 [00:27<00:32,  1.56s/it] 33%|███▎      | 10/30 [00:28<00:28,  1.45s/it] 37%|███▋      | 11/30 [00:29<00:26,  1.38s/it] 40%|████      | 12/30 [00:30<00:23,  1.32s/it] 43%|████▎     | 13/30 [00:32<00:21,  1.28s/it] 47%|████▋     | 14/30 [00:33<00:20,  1.25s/it] 50%|█████     | 15/30 [00:34<00:18,  1.24s/it]                                                50%|█████     | 15/30 [00:34<00:18,  1.24s/it] 53%|█████▎    | 16/30 [00:35<00:17,  1.23s/it] 57%|█████▋    | 17/30 [00:36<00:15,  1.22s/it] 60%|██████    | 18/30 [00:38<00:14,  1.25s/it] 63%|██████▎   | 19/30 [00:39<00:13,  1.23s/it] 67%|██████▋   | 20/30 [00:40<00:12,  1.22s/it] 70%|███████   | 21/30 [00:41<00:10,  1.21s/it] 73%|███████▎  | 22/30 [00:43<00:09,  1.21s/it] 77%|███████▋  | 23/30 [00:44<00:08,  1.21s/it] 80%|████████  | 24/30 [00:45<00:07,  1.25s/it] 83%|████████▎ | 25/30 [00:46<00:06,  1.23s/it] 87%|████████▋ | 26/30 [00:48<00:04,  1.24s/it] 90%|█████████ | 27/30 [00:49<00:03,  1.23s/it] 93%|█████████▎| 28/30 [00:50<00:02,  1.22s/it] 97%|█████████▋| 29/30 [00:51<00:01,  1.21s/it]100%|██████████| 30/30 [00:52<00:00,  1.21s/it]                                               100%|██████████| 30/30 [00:52<00:00,  1.21s/it]                                               100%|██████████| 30/30 [00:52<00:00,  1.21s/it]100%|██████████| 30/30 [00:52<00:00,  1.76s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9526, 'grad_norm': 0.46676650643348694, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8667, 'grad_norm': 0.3340544104576111, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 52.8646, 'train_samples_per_second': 4.54, 'train_steps_per_second': 0.567, 'train_loss': 0.9096551259358724, 'epoch': 0.24}
END_PROFILE: 1747615732.3951044
Run  2: 54.806113311 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.37s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.62s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:14, 68.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 211.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:00<00:02, 304.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:00<00:02, 351.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:00<00:02, 375.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:00<00:01, 393.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:00<00:01, 403.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:01<00:01, 412.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:01<00:01, 418.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:01<00:01, 422.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:01<00:01, 423.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:01<00:00, 422.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:01<00:00, 422.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:01<00:00, 423.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:02<00:00, 422.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:02<00:00, 424.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:02<00:00, 424.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:02<00:00, 429.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 436.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 388.51 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747615813.9013572
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:54, 18.45s/it]  7%|▋         | 2/30 [00:19<03:53,  8.34s/it] 10%|█         | 3/30 [00:20<02:17,  5.10s/it] 13%|█▎        | 4/30 [00:22<01:32,  3.57s/it] 17%|█▋        | 5/30 [00:23<01:09,  2.79s/it] 20%|██        | 6/30 [00:24<00:54,  2.27s/it] 23%|██▎       | 7/30 [00:26<00:44,  1.93s/it] 27%|██▋       | 8/30 [00:27<00:37,  1.71s/it] 30%|███       | 9/30 [00:28<00:33,  1.60s/it] 33%|███▎      | 10/30 [00:29<00:29,  1.48s/it] 37%|███▋      | 11/30 [00:31<00:26,  1.41s/it] 40%|████      | 12/30 [00:32<00:24,  1.35s/it] 43%|████▎     | 13/30 [00:33<00:22,  1.31s/it] 47%|████▋     | 14/30 [00:34<00:20,  1.28s/it] 50%|█████     | 15/30 [00:35<00:18,  1.26s/it]                                                50%|█████     | 15/30 [00:35<00:18,  1.26s/it] 53%|█████▎    | 16/30 [00:37<00:17,  1.26s/it] 57%|█████▋    | 17/30 [00:38<00:16,  1.25s/it] 60%|██████    | 18/30 [00:39<00:15,  1.27s/it] 63%|██████▎   | 19/30 [00:41<00:13,  1.25s/it] 67%|██████▋   | 20/30 [00:42<00:12,  1.24s/it] 70%|███████   | 21/30 [00:43<00:11,  1.24s/it] 73%|███████▎  | 22/30 [00:44<00:09,  1.23s/it] 77%|███████▋  | 23/30 [00:45<00:08,  1.23s/it] 80%|████████  | 24/30 [00:47<00:07,  1.26s/it] 83%|████████▎ | 25/30 [00:48<00:06,  1.25s/it] 87%|████████▋ | 26/30 [00:49<00:05,  1.25s/it] 90%|█████████ | 27/30 [00:50<00:03,  1.25s/it] 93%|█████████▎| 28/30 [00:52<00:02,  1.24s/it] 97%|█████████▋| 29/30 [00:53<00:01,  1.24s/it]100%|██████████| 30/30 [00:54<00:00,  1.23s/it]                                               100%|██████████| 30/30 [00:54<00:00,  1.23s/it]                                               100%|██████████| 30/30 [00:54<00:00,  1.23s/it]100%|██████████| 30/30 [00:54<00:00,  1.82s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9522, 'grad_norm': 0.4640897512435913, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8667, 'grad_norm': 0.33242306113243103, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 54.629, 'train_samples_per_second': 4.393, 'train_steps_per_second': 0.549, 'train_loss': 0.9094638824462891, 'epoch': 0.24}
END_PROFILE: 1747615869.0028284
Run  3: 57.529019823 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 54.806113311 seconds
  Max elapsed : 57.529019823 seconds
  Avg elapsed : 55.91264908300000000000 seconds
===================================
NSYS

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.36s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.61s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:13, 70.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:03, 242.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:02, 318.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:00<00:02, 362.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:00<00:01, 386.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:00<00:01, 401.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:00<00:01, 411.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:01<00:01, 420.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:01<00:01, 423.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:01<00:01, 426.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:01<00:00, 429.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:01<00:00, 432.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:01<00:00, 432.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:01<00:00, 432.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:02<00:00, 430.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:02<00:00, 433.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:02<00:00, 432.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:02<00:00, 443.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 396.25 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747615973.5079036
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:17<08:33, 17.72s/it]  7%|▋         | 2/30 [00:19<03:51,  8.25s/it] 10%|█         | 3/30 [00:20<02:21,  5.22s/it] 13%|█▎        | 4/30 [00:22<01:38,  3.80s/it] 17%|█▋        | 5/30 [00:24<01:16,  3.07s/it] 20%|██        | 6/30 [00:25<01:01,  2.58s/it] 23%|██▎       | 7/30 [00:27<00:52,  2.28s/it] 27%|██▋       | 8/30 [00:29<00:45,  2.07s/it] 30%|███       | 9/30 [00:31<00:41,  1.96s/it] 33%|███▎      | 10/30 [00:32<00:37,  1.86s/it] 37%|███▋      | 11/30 [00:34<00:33,  1.79s/it] 40%|████      | 12/30 [00:35<00:31,  1.73s/it] 43%|████▎     | 13/30 [00:37<00:28,  1.69s/it] 47%|████▋     | 14/30 [00:39<00:26,  1.67s/it] 50%|█████     | 15/30 [00:40<00:24,  1.65s/it]                                                50%|█████     | 15/30 [00:40<00:24,  1.65s/it] 53%|█████▎    | 16/30 [00:42<00:23,  1.65s/it] 57%|█████▋    | 17/30 [00:43<00:21,  1.64s/it] 60%|██████    | 18/30 [00:45<00:19,  1.65s/it] 63%|██████▎   | 19/30 [00:47<00:18,  1.64s/it] 67%|██████▋   | 20/30 [00:48<00:16,  1.63s/it] 70%|███████   | 21/30 [00:50<00:14,  1.63s/it] 73%|███████▎  | 22/30 [00:52<00:12,  1.62s/it] 77%|███████▋  | 23/30 [00:53<00:11,  1.62s/it] 80%|████████  | 24/30 [00:55<00:09,  1.64s/it] 83%|████████▎ | 25/30 [00:57<00:08,  1.63s/it] 87%|████████▋ | 26/30 [00:58<00:06,  1.62s/it] 90%|█████████ | 27/30 [01:00<00:04,  1.62s/it] 93%|█████████▎| 28/30 [01:01<00:03,  1.63s/it] 97%|█████████▋| 29/30 [01:03<00:01,  1.62s/it]100%|██████████| 30/30 [01:05<00:00,  1.62s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.62s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.62s/it]100%|██████████| 30/30 [01:05<00:00,  2.17s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9534, 'grad_norm': 0.457348495721817, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8671, 'grad_norm': 0.33050695061683655, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 65.1221, 'train_samples_per_second': 3.685, 'train_steps_per_second': 0.461, 'train_loss': 0.9102387428283691, 'epoch': 0.24}
END_PROFILE: 1747616039.123646
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/mistral-7b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-a34d.qdstrm'
[1/1] [0%                          ] nsys-report-4370.nsys-rep[1/1] [0%                          ] nsys-report-4370.nsys-rep[1/1] [0%                          ] nsys-report-4370.nsys-rep[1/1] [7%                          ] nsys-report-4370.nsys-rep[1/1] [6%                          ] nsys-report-4370.nsys-rep[1/1] [5%                          ] nsys-report-4370.nsys-rep[1/1] [8%                          ] nsys-report-4370.nsys-rep[1/1] [7%                          ] nsys-report-4370.nsys-rep[1/1] [6%                          ] nsys-report-4370.nsys-rep[1/1] [5%                          ] nsys-report-4370.nsys-rep[1/1] [5%                          ] nsys-report-4370.nsys-rep[1/1] [6%                          ] nsys-report-4370.nsys-rep[1/1] [7%                          ] nsys-report-4370.nsys-rep[1/1] [8%                          ] nsys-report-4370.nsys-rep[1/1] [9%                          ] nsys-report-4370.nsys-rep[1/1] [10%                         ] nsys-report-4370.nsys-rep[1/1] [11%                         ] nsys-report-4370.nsys-rep[1/1] [12%                         ] nsys-report-4370.nsys-rep[1/1] [13%                         ] nsys-report-4370.nsys-rep[1/1] [14%                         ] nsys-report-4370.nsys-rep[1/1] [=15%                        ] nsys-report-4370.nsys-rep[1/1] [=16%                        ] nsys-report-4370.nsys-rep[1/1] [=17%                        ] nsys-report-4370.nsys-rep[1/1] [==18%                       ] nsys-report-4370.nsys-rep[1/1] [==19%                       ] nsys-report-4370.nsys-rep[1/1] [==20%                       ] nsys-report-4370.nsys-rep[1/1] [==21%                       ] nsys-report-4370.nsys-rep[1/1] [===22%                      ] nsys-report-4370.nsys-rep[1/1] [===23%                      ] nsys-report-4370.nsys-rep[1/1] [===24%                      ] nsys-report-4370.nsys-rep[1/1] [====25%                     ] nsys-report-4370.nsys-rep[1/1] [====26%                     ] nsys-report-4370.nsys-rep[1/1] [====27%                     ] nsys-report-4370.nsys-rep[1/1] [====28%                     ] nsys-report-4370.nsys-rep[1/1] [=====29%                    ] nsys-report-4370.nsys-rep[1/1] [=====30%                    ] nsys-report-4370.nsys-rep[1/1] [=====31%                    ] nsys-report-4370.nsys-rep[1/1] [=====32%                    ] nsys-report-4370.nsys-rep[1/1] [======33%                   ] nsys-report-4370.nsys-rep[1/1] [======34%                   ] nsys-report-4370.nsys-rep[1/1] [======35%                   ] nsys-report-4370.nsys-rep[1/1] [=======36%                  ] nsys-report-4370.nsys-rep[1/1] [=======37%                  ] nsys-report-4370.nsys-rep[1/1] [=======38%                  ] nsys-report-4370.nsys-rep[1/1] [=======39%                  ] nsys-report-4370.nsys-rep[1/1] [========40%                 ] nsys-report-4370.nsys-rep[1/1] [========41%                 ] nsys-report-4370.nsys-rep[1/1] [========42%                 ] nsys-report-4370.nsys-rep[1/1] [=========43%                ] nsys-report-4370.nsys-rep[1/1] [=========44%                ] nsys-report-4370.nsys-rep[1/1] [=========45%                ] nsys-report-4370.nsys-rep[1/1] [=========46%                ] nsys-report-4370.nsys-rep[1/1] [==========47%               ] nsys-report-4370.nsys-rep[1/1] [==========48%               ] nsys-report-4370.nsys-rep[1/1] [==========49%               ] nsys-report-4370.nsys-rep[1/1] [===========50%              ] nsys-report-4370.nsys-rep[1/1] [===========51%              ] nsys-report-4370.nsys-rep[1/1] [===========52%              ] nsys-report-4370.nsys-rep[1/1] [===========53%              ] nsys-report-4370.nsys-rep[1/1] [============54%             ] nsys-report-4370.nsys-rep[1/1] [============55%             ] nsys-report-4370.nsys-rep[1/1] [============56%             ] nsys-report-4370.nsys-rep[1/1] [============57%             ] nsys-report-4370.nsys-rep[1/1] [=============58%            ] nsys-report-4370.nsys-rep[1/1] [=============59%            ] nsys-report-4370.nsys-rep[1/1] [=============60%            ] nsys-report-4370.nsys-rep[1/1] [==============61%           ] nsys-report-4370.nsys-rep[1/1] [==============62%           ] nsys-report-4370.nsys-rep[1/1] [==============63%           ] nsys-report-4370.nsys-rep[1/1] [==============64%           ] nsys-report-4370.nsys-rep[1/1] [===============65%          ] nsys-report-4370.nsys-rep[1/1] [===============66%          ] nsys-report-4370.nsys-rep[1/1] [===============67%          ] nsys-report-4370.nsys-rep[1/1] [================68%         ] nsys-report-4370.nsys-rep[1/1] [================69%         ] nsys-report-4370.nsys-rep[1/1] [================70%         ] nsys-report-4370.nsys-rep[1/1] [================71%         ] nsys-report-4370.nsys-rep[1/1] [=================72%        ] nsys-report-4370.nsys-rep[1/1] [=================73%        ] nsys-report-4370.nsys-rep[1/1] [=================74%        ] nsys-report-4370.nsys-rep[1/1] [==================75%       ] nsys-report-4370.nsys-rep[1/1] [==================76%       ] nsys-report-4370.nsys-rep[1/1] [==================77%       ] nsys-report-4370.nsys-rep[1/1] [==================78%       ] nsys-report-4370.nsys-rep[1/1] [===================79%      ] nsys-report-4370.nsys-rep[1/1] [===================80%      ] nsys-report-4370.nsys-rep[1/1] [===================81%      ] nsys-report-4370.nsys-rep[1/1] [========================100%] nsys-report-4370.nsys-rep[1/1] [========================100%] nsys-report-4370.nsys-rep
Generated:
	/tmp/nsys-report-4370.nsys-rep
Run  1: 77.859803997 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.65s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:13, 71.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:03, 246.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:02, 324.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:00<00:02, 365.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:00<00:01, 391.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:00<00:01, 408.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:00<00:01, 414.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:01<00:01, 425.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:01<00:01, 430.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:01<00:01, 436.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:01<00:00, 432.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:01<00:00, 437.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:01<00:00, 436.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:01<00:00, 440.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:01<00:00, 440.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:02<00:00, 438.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:02<00:00, 438.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:02<00:00, 442.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 400.46 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747616134.5393748
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:44, 18.09s/it]  7%|▋         | 2/30 [00:19<03:55,  8.41s/it] 10%|█         | 3/30 [00:21<02:23,  5.32s/it] 13%|█▎        | 4/30 [00:23<01:40,  3.87s/it] 17%|█▋        | 5/30 [00:24<01:18,  3.12s/it] 20%|██        | 6/30 [00:26<01:02,  2.62s/it] 23%|██▎       | 7/30 [00:28<00:52,  2.29s/it] 27%|██▋       | 8/30 [00:29<00:45,  2.09s/it] 30%|███       | 9/30 [00:31<00:41,  1.98s/it] 33%|███▎      | 10/30 [00:33<00:37,  1.87s/it] 37%|███▋      | 11/30 [00:34<00:34,  1.81s/it] 40%|████      | 12/30 [00:36<00:31,  1.76s/it] 43%|████▎     | 13/30 [00:38<00:29,  1.72s/it] 47%|████▋     | 14/30 [00:39<00:27,  1.69s/it] 50%|█████     | 15/30 [00:41<00:25,  1.67s/it]                                                50%|█████     | 15/30 [00:41<00:25,  1.67s/it] 53%|█████▎    | 16/30 [00:42<00:23,  1.67s/it] 57%|█████▋    | 17/30 [00:44<00:21,  1.66s/it] 60%|██████    | 18/30 [00:46<00:20,  1.68s/it] 63%|██████▎   | 19/30 [00:47<00:18,  1.67s/it] 67%|██████▋   | 20/30 [00:49<00:16,  1.66s/it] 70%|███████   | 21/30 [00:51<00:14,  1.65s/it] 73%|███████▎  | 22/30 [00:52<00:13,  1.65s/it] 77%|███████▋  | 23/30 [00:54<00:11,  1.65s/it] 80%|████████  | 24/30 [00:56<00:10,  1.67s/it] 83%|████████▎ | 25/30 [00:57<00:08,  1.66s/it] 87%|████████▋ | 26/30 [00:59<00:06,  1.65s/it] 90%|█████████ | 27/30 [01:01<00:04,  1.65s/it] 93%|█████████▎| 28/30 [01:02<00:03,  1.65s/it] 97%|█████████▋| 29/30 [01:04<00:01,  1.64s/it]100%|██████████| 30/30 [01:06<00:00,  1.65s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.65s/it]                                               100%|██████████| 30/30 [01:06<00:00,  1.65s/it]100%|██████████| 30/30 [01:06<00:00,  2.20s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9532, 'grad_norm': 0.46680036187171936, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.867, 'grad_norm': 0.3319351077079773, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 66.1054, 'train_samples_per_second': 3.631, 'train_steps_per_second': 0.454, 'train_loss': 0.9100515365600585, 'epoch': 0.24}
END_PROFILE: 1747616201.1323988
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/mistral-7b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-7c32.qdstrm'
[1/1] [0%                          ] nsys-report-50c9.nsys-rep[1/1] [0%                          ] nsys-report-50c9.nsys-rep[1/1] [0%                          ] nsys-report-50c9.nsys-rep[1/1] [7%                          ] nsys-report-50c9.nsys-rep[1/1] [6%                          ] nsys-report-50c9.nsys-rep[1/1] [9%                          ] nsys-report-50c9.nsys-rep[1/1] [8%                          ] nsys-report-50c9.nsys-rep[1/1] [7%                          ] nsys-report-50c9.nsys-rep[1/1] [6%                          ] nsys-report-50c9.nsys-rep[1/1] [5%                          ] nsys-report-50c9.nsys-rep[1/1] [5%                          ] nsys-report-50c9.nsys-rep[1/1] [6%                          ] nsys-report-50c9.nsys-rep[1/1] [7%                          ] nsys-report-50c9.nsys-rep[1/1] [8%                          ] nsys-report-50c9.nsys-rep[1/1] [9%                          ] nsys-report-50c9.nsys-rep[1/1] [10%                         ] nsys-report-50c9.nsys-rep[1/1] [11%                         ] nsys-report-50c9.nsys-rep[1/1] [12%                         ] nsys-report-50c9.nsys-rep[1/1] [13%                         ] nsys-report-50c9.nsys-rep[1/1] [14%                         ] nsys-report-50c9.nsys-rep[1/1] [=15%                        ] nsys-report-50c9.nsys-rep[1/1] [=16%                        ] nsys-report-50c9.nsys-rep[1/1] [=17%                        ] nsys-report-50c9.nsys-rep[1/1] [==18%                       ] nsys-report-50c9.nsys-rep[1/1] [==19%                       ] nsys-report-50c9.nsys-rep[1/1] [==20%                       ] nsys-report-50c9.nsys-rep[1/1] [==21%                       ] nsys-report-50c9.nsys-rep[1/1] [===22%                      ] nsys-report-50c9.nsys-rep[1/1] [===23%                      ] nsys-report-50c9.nsys-rep[1/1] [===24%                      ] nsys-report-50c9.nsys-rep[1/1] [====25%                     ] nsys-report-50c9.nsys-rep[1/1] [====26%                     ] nsys-report-50c9.nsys-rep[1/1] [====27%                     ] nsys-report-50c9.nsys-rep[1/1] [====28%                     ] nsys-report-50c9.nsys-rep[1/1] [=====29%                    ] nsys-report-50c9.nsys-rep[1/1] [=====30%                    ] nsys-report-50c9.nsys-rep[1/1] [=====31%                    ] nsys-report-50c9.nsys-rep[1/1] [=====32%                    ] nsys-report-50c9.nsys-rep[1/1] [======33%                   ] nsys-report-50c9.nsys-rep[1/1] [======34%                   ] nsys-report-50c9.nsys-rep[1/1] [======35%                   ] nsys-report-50c9.nsys-rep[1/1] [=======36%                  ] nsys-report-50c9.nsys-rep[1/1] [=======37%                  ] nsys-report-50c9.nsys-rep[1/1] [=======38%                  ] nsys-report-50c9.nsys-rep[1/1] [=======39%                  ] nsys-report-50c9.nsys-rep[1/1] [========40%                 ] nsys-report-50c9.nsys-rep[1/1] [========41%                 ] nsys-report-50c9.nsys-rep[1/1] [========42%                 ] nsys-report-50c9.nsys-rep[1/1] [=========43%                ] nsys-report-50c9.nsys-rep[1/1] [=========44%                ] nsys-report-50c9.nsys-rep[1/1] [=========45%                ] nsys-report-50c9.nsys-rep[1/1] [=========46%                ] nsys-report-50c9.nsys-rep[1/1] [==========47%               ] nsys-report-50c9.nsys-rep[1/1] [==========48%               ] nsys-report-50c9.nsys-rep[1/1] [==========49%               ] nsys-report-50c9.nsys-rep[1/1] [===========50%              ] nsys-report-50c9.nsys-rep[1/1] [===========51%              ] nsys-report-50c9.nsys-rep[1/1] [===========52%              ] nsys-report-50c9.nsys-rep[1/1] [===========53%              ] nsys-report-50c9.nsys-rep[1/1] [============54%             ] nsys-report-50c9.nsys-rep[1/1] [============55%             ] nsys-report-50c9.nsys-rep[1/1] [============56%             ] nsys-report-50c9.nsys-rep[1/1] [============57%             ] nsys-report-50c9.nsys-rep[1/1] [=============58%            ] nsys-report-50c9.nsys-rep[1/1] [=============59%            ] nsys-report-50c9.nsys-rep[1/1] [=============60%            ] nsys-report-50c9.nsys-rep[1/1] [==============61%           ] nsys-report-50c9.nsys-rep[1/1] [==============62%           ] nsys-report-50c9.nsys-rep[1/1] [==============63%           ] nsys-report-50c9.nsys-rep[1/1] [==============64%           ] nsys-report-50c9.nsys-rep[1/1] [===============65%          ] nsys-report-50c9.nsys-rep[1/1] [===============66%          ] nsys-report-50c9.nsys-rep[1/1] [===============67%          ] nsys-report-50c9.nsys-rep[1/1] [================68%         ] nsys-report-50c9.nsys-rep[1/1] [================69%         ] nsys-report-50c9.nsys-rep[1/1] [================70%         ] nsys-report-50c9.nsys-rep[1/1] [================71%         ] nsys-report-50c9.nsys-rep[1/1] [=================72%        ] nsys-report-50c9.nsys-rep[1/1] [=================73%        ] nsys-report-50c9.nsys-rep[1/1] [=================74%        ] nsys-report-50c9.nsys-rep[1/1] [==================75%       ] nsys-report-50c9.nsys-rep[1/1] [==================76%       ] nsys-report-50c9.nsys-rep[1/1] [==================77%       ] nsys-report-50c9.nsys-rep[1/1] [==================78%       ] nsys-report-50c9.nsys-rep[1/1] [===================79%      ] nsys-report-50c9.nsys-rep[1/1] [===================80%      ] nsys-report-50c9.nsys-rep[1/1] [===================81%      ] nsys-report-50c9.nsys-rep[1/1] [========================100%] nsys-report-50c9.nsys-rep[1/1] [========================100%] nsys-report-50c9.nsys-rep
Generated:
	/tmp/nsys-report-50c9.nsys-rep
Run  2: 78.772982925 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.62s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:15, 65.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:03, 232.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:02, 311.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:00<00:02, 357.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:00<00:01, 382.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:00<00:01, 402.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:00<00:01, 414.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:01<00:01, 419.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:01<00:01, 424.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:01<00:01, 429.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:01<00:00, 434.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:01<00:00, 435.15 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:01<00:00, 439.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:01<00:00, 439.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:02<00:00, 441.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:02<00:00, 442.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:02<00:00, 434.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:02<00:00, 443.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 397.10 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747616299.089403
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:18<08:48, 18.22s/it]  7%|▋         | 2/30 [00:19<03:56,  8.44s/it] 10%|█         | 3/30 [00:21<02:23,  5.32s/it] 13%|█▎        | 4/30 [00:23<01:40,  3.85s/it] 17%|█▋        | 5/30 [00:24<01:17,  3.10s/it] 20%|██        | 6/30 [00:26<01:02,  2.59s/it] 23%|██▎       | 7/30 [00:27<00:52,  2.26s/it] 27%|██▋       | 8/30 [00:29<00:45,  2.05s/it] 30%|███       | 9/30 [00:31<00:40,  1.94s/it] 33%|███▎      | 10/30 [00:32<00:36,  1.83s/it] 37%|███▋      | 11/30 [00:34<00:33,  1.76s/it] 40%|████      | 12/30 [00:36<00:30,  1.72s/it] 43%|████▎     | 13/30 [00:37<00:28,  1.67s/it] 47%|████▋     | 14/30 [00:39<00:26,  1.65s/it] 50%|█████     | 15/30 [00:40<00:24,  1.63s/it]                                                50%|█████     | 15/30 [00:40<00:24,  1.63s/it] 53%|█████▎    | 16/30 [00:42<00:22,  1.62s/it] 57%|█████▋    | 17/30 [00:43<00:20,  1.61s/it] 60%|██████    | 18/30 [00:45<00:19,  1.62s/it] 63%|██████▎   | 19/30 [00:47<00:17,  1.60s/it] 67%|██████▋   | 20/30 [00:48<00:15,  1.59s/it] 70%|███████   | 21/30 [00:50<00:14,  1.59s/it] 73%|███████▎  | 22/30 [00:51<00:12,  1.58s/it] 77%|███████▋  | 23/30 [00:53<00:11,  1.58s/it] 80%|████████  | 24/30 [00:55<00:09,  1.60s/it] 83%|████████▎ | 25/30 [00:56<00:07,  1.58s/it] 87%|████████▋ | 26/30 [00:58<00:06,  1.58s/it] 90%|█████████ | 27/30 [00:59<00:04,  1.57s/it] 93%|█████████▎| 28/30 [01:01<00:03,  1.58s/it] 97%|█████████▋| 29/30 [01:02<00:01,  1.57s/it]100%|██████████| 30/30 [01:04<00:00,  1.57s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.57s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.57s/it]100%|██████████| 30/30 [01:04<00:00,  2.15s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9532, 'grad_norm': 0.46365317702293396, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.867, 'grad_norm': 0.3325628936290741, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 64.5149, 'train_samples_per_second': 3.72, 'train_steps_per_second': 0.465, 'train_loss': 0.9101152420043945, 'epoch': 0.24}
END_PROFILE: 1747616364.0866597
The target application terminated. One or more process it created re-parented.
Waiting for termination of re-parented processes.
Use the `--wait` option to modify this behavior.
Failed to create '/scratch/jlee436/experiments/overhead-bench/end2end/unsloth/mistral-7b.nsys.nsys-rep': File exists.
Use `--force-overwrite true` to overwrite existing files.
Collecting data...
Generating '/tmp/nsys-report-73a1.qdstrm'
[1/1] [0%                          ] nsys-report-218f.nsys-rep[1/1] [0%                          ] nsys-report-218f.nsys-rep[1/1] [7%                          ] nsys-report-218f.nsys-rep[1/1] [6%                          ] nsys-report-218f.nsys-rep[1/1] [7%                          ] nsys-report-218f.nsys-rep[1/1] [6%                          ] nsys-report-218f.nsys-rep[1/1] [7%                          ] nsys-report-218f.nsys-rep[1/1] [6%                          ] nsys-report-218f.nsys-rep[1/1] [7%                          ] nsys-report-218f.nsys-rep[1/1] [6%                          ] nsys-report-218f.nsys-rep[1/1] [7%                          ] nsys-report-218f.nsys-rep[1/1] [6%                          ] nsys-report-218f.nsys-rep[1/1] [5%                          ] nsys-report-218f.nsys-rep[1/1] [8%                          ] nsys-report-218f.nsys-rep[1/1] [7%                          ] nsys-report-218f.nsys-rep[1/1] [6%                          ] nsys-report-218f.nsys-rep[1/1] [5%                          ] nsys-report-218f.nsys-rep[1/1] [5%                          ] nsys-report-218f.nsys-rep[1/1] [6%                          ] nsys-report-218f.nsys-rep[1/1] [7%                          ] nsys-report-218f.nsys-rep[1/1] [8%                          ] nsys-report-218f.nsys-rep[1/1] [9%                          ] nsys-report-218f.nsys-rep[1/1] [10%                         ] nsys-report-218f.nsys-rep[1/1] [11%                         ] nsys-report-218f.nsys-rep[1/1] [12%                         ] nsys-report-218f.nsys-rep[1/1] [13%                         ] nsys-report-218f.nsys-rep[1/1] [14%                         ] nsys-report-218f.nsys-rep[1/1] [=15%                        ] nsys-report-218f.nsys-rep[1/1] [=16%                        ] nsys-report-218f.nsys-rep[1/1] [=17%                        ] nsys-report-218f.nsys-rep[1/1] [==18%                       ] nsys-report-218f.nsys-rep[1/1] [==19%                       ] nsys-report-218f.nsys-rep[1/1] [==20%                       ] nsys-report-218f.nsys-rep[1/1] [==21%                       ] nsys-report-218f.nsys-rep[1/1] [===22%                      ] nsys-report-218f.nsys-rep[1/1] [===23%                      ] nsys-report-218f.nsys-rep[1/1] [===24%                      ] nsys-report-218f.nsys-rep[1/1] [====25%                     ] nsys-report-218f.nsys-rep[1/1] [====26%                     ] nsys-report-218f.nsys-rep[1/1] [====27%                     ] nsys-report-218f.nsys-rep[1/1] [====28%                     ] nsys-report-218f.nsys-rep[1/1] [=====29%                    ] nsys-report-218f.nsys-rep[1/1] [=====30%                    ] nsys-report-218f.nsys-rep[1/1] [=====31%                    ] nsys-report-218f.nsys-rep[1/1] [=====32%                    ] nsys-report-218f.nsys-rep[1/1] [======33%                   ] nsys-report-218f.nsys-rep[1/1] [======34%                   ] nsys-report-218f.nsys-rep[1/1] [======35%                   ] nsys-report-218f.nsys-rep[1/1] [=======36%                  ] nsys-report-218f.nsys-rep[1/1] [=======37%                  ] nsys-report-218f.nsys-rep[1/1] [=======38%                  ] nsys-report-218f.nsys-rep[1/1] [=======39%                  ] nsys-report-218f.nsys-rep[1/1] [========40%                 ] nsys-report-218f.nsys-rep[1/1] [========41%                 ] nsys-report-218f.nsys-rep[1/1] [========42%                 ] nsys-report-218f.nsys-rep[1/1] [=========43%                ] nsys-report-218f.nsys-rep[1/1] [=========44%                ] nsys-report-218f.nsys-rep[1/1] [=========45%                ] nsys-report-218f.nsys-rep[1/1] [=========46%                ] nsys-report-218f.nsys-rep[1/1] [==========47%               ] nsys-report-218f.nsys-rep[1/1] [==========48%               ] nsys-report-218f.nsys-rep[1/1] [==========49%               ] nsys-report-218f.nsys-rep[1/1] [===========50%              ] nsys-report-218f.nsys-rep[1/1] [===========51%              ] nsys-report-218f.nsys-rep[1/1] [===========52%              ] nsys-report-218f.nsys-rep[1/1] [===========53%              ] nsys-report-218f.nsys-rep[1/1] [============54%             ] nsys-report-218f.nsys-rep[1/1] [============55%             ] nsys-report-218f.nsys-rep[1/1] [============56%             ] nsys-report-218f.nsys-rep[1/1] [============57%             ] nsys-report-218f.nsys-rep[1/1] [=============58%            ] nsys-report-218f.nsys-rep[1/1] [=============59%            ] nsys-report-218f.nsys-rep[1/1] [=============60%            ] nsys-report-218f.nsys-rep[1/1] [==============61%           ] nsys-report-218f.nsys-rep[1/1] [==============62%           ] nsys-report-218f.nsys-rep[1/1] [==============63%           ] nsys-report-218f.nsys-rep[1/1] [==============64%           ] nsys-report-218f.nsys-rep[1/1] [===============65%          ] nsys-report-218f.nsys-rep[1/1] [===============66%          ] nsys-report-218f.nsys-rep[1/1] [===============67%          ] nsys-report-218f.nsys-rep[1/1] [================68%         ] nsys-report-218f.nsys-rep[1/1] [================69%         ] nsys-report-218f.nsys-rep[1/1] [================70%         ] nsys-report-218f.nsys-rep[1/1] [================71%         ] nsys-report-218f.nsys-rep[1/1] [=================72%        ] nsys-report-218f.nsys-rep[1/1] [=================73%        ] nsys-report-218f.nsys-rep[1/1] [=================74%        ] nsys-report-218f.nsys-rep[1/1] [==================75%       ] nsys-report-218f.nsys-rep[1/1] [==================76%       ] nsys-report-218f.nsys-rep[1/1] [==================77%       ] nsys-report-218f.nsys-rep[1/1] [==================78%       ] nsys-report-218f.nsys-rep[1/1] [===================79%      ] nsys-report-218f.nsys-rep[1/1] [===================80%      ] nsys-report-218f.nsys-rep[1/1] [===================81%      ] nsys-report-218f.nsys-rep[1/1] [========================100%] nsys-report-218f.nsys-rep[1/1] [========================100%] nsys-report-218f.nsys-rep
Generated:
	/tmp/nsys-report-218f.nsys-rep
Run  3: 77.118294933 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 77.118294933 seconds
  Max elapsed : 78.772982925 seconds
  Avg elapsed : 77.91702728500000000000 seconds
===================================
--------------------------------------------
PROTON

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:12<00:25, 12.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:25<00:12, 12.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.39s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:14, 69.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 212.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:00<00:02, 305.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:00<00:02, 354.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:00<00:02, 378.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:00<00:01, 393.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:00<00:01, 405.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:01<00:01, 414.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:01<00:01, 419.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:01<00:01, 423.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:01<00:01, 422.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:01<00:00, 425.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:01<00:00, 426.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:01<00:00, 429.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:02<00:00, 428.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:02<00:00, 427.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:02<00:00, 423.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:02<00:00, 434.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 438.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 390.83 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747616482.9475057
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:22, 19.41s/it]  7%|▋         | 2/30 [00:20<04:09,  8.89s/it] 10%|█         | 3/30 [00:22<02:29,  5.54s/it] 13%|█▎        | 4/30 [00:23<01:42,  3.95s/it] 17%|█▋        | 5/30 [00:25<01:18,  3.13s/it] 20%|██        | 6/30 [00:27<01:02,  2.58s/it] 23%|██▎       | 7/30 [00:28<00:51,  2.24s/it] 27%|██▋       | 8/30 [00:30<00:44,  2.00s/it] 30%|███       | 9/30 [00:31<00:39,  1.89s/it] 33%|███▎      | 10/30 [00:33<00:35,  1.77s/it] 37%|███▋      | 11/30 [00:34<00:32,  1.70s/it] 40%|████      | 12/30 [00:36<00:30,  1.71s/it] 43%|████▎     | 13/30 [00:38<00:28,  1.66s/it] 47%|████▋     | 14/30 [00:39<00:26,  1.63s/it] 50%|█████     | 15/30 [00:41<00:24,  1.62s/it]                                                50%|█████     | 15/30 [00:41<00:24,  1.62s/it] 53%|█████▎    | 16/30 [00:42<00:22,  1.60s/it] 57%|█████▋    | 17/30 [00:44<00:20,  1.58s/it] 60%|██████    | 18/30 [00:46<00:19,  1.59s/it] 63%|██████▎   | 19/30 [00:47<00:17,  1.56s/it] 67%|██████▋   | 20/30 [00:49<00:15,  1.54s/it] 70%|███████   | 21/30 [00:50<00:13,  1.53s/it] 73%|███████▎  | 22/30 [00:52<00:12,  1.53s/it] 77%|███████▋  | 23/30 [00:53<00:10,  1.52s/it] 80%|████████  | 24/30 [00:55<00:09,  1.63s/it] 83%|████████▎ | 25/30 [00:57<00:08,  1.62s/it] 87%|████████▋ | 26/30 [00:58<00:06,  1.61s/it] 90%|█████████ | 27/30 [01:00<00:04,  1.60s/it] 93%|█████████▎| 28/30 [01:01<00:03,  1.59s/it] 97%|█████████▋| 29/30 [01:03<00:01,  1.61s/it]100%|██████████| 30/30 [01:04<00:00,  1.59s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.59s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.59s/it]100%|██████████| 30/30 [01:04<00:00,  2.17s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9524, 'grad_norm': 0.46648961305618286, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8667, 'grad_norm': 0.3321075141429901, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 64.9527, 'train_samples_per_second': 3.695, 'train_steps_per_second': 0.462, 'train_loss': 0.9095236778259277, 'epoch': 0.24}
END_PROFILE: 1747616548.6615982
Run  1: 67.285785328 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:12<00:25, 12.79s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:25<00:12, 12.84s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.31s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.45s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:13, 70.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 215.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:00<00:02, 309.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:00<00:02, 343.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  20%|█▉        | 196/1000 [00:00<00:02, 359.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:00<00:02, 375.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:00<00:01, 381.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:00<00:01, 387.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  36%|███▋      | 364/1000 [00:01<00:01, 394.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:01<00:01, 396.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:01<00:01, 396.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  49%|████▉     | 490/1000 [00:01<00:01, 396.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:01<00:01, 400.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:01<00:01, 410.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:01<00:00, 421.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:01<00:00, 424.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:01<00:00, 426.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:02<00:00, 427.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:02<00:00, 429.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:02<00:00, 420.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:02<00:00, 434.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 383.06 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747616635.5653794
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:17, 19.22s/it]  7%|▋         | 2/30 [00:20<04:06,  8.80s/it] 10%|█         | 3/30 [00:22<02:27,  5.47s/it] 13%|█▎        | 4/30 [00:23<01:41,  3.89s/it] 17%|█▋        | 5/30 [00:25<01:16,  3.08s/it] 20%|██        | 6/30 [00:26<01:01,  2.54s/it] 23%|██▎       | 7/30 [00:28<00:50,  2.20s/it] 27%|██▋       | 8/30 [00:29<00:43,  1.96s/it] 30%|███       | 9/30 [00:31<00:38,  1.84s/it] 33%|███▎      | 10/30 [00:32<00:34,  1.74s/it] 37%|███▋      | 11/30 [00:34<00:31,  1.67s/it] 40%|████      | 12/30 [00:36<00:30,  1.68s/it] 43%|████▎     | 13/30 [00:37<00:27,  1.64s/it] 47%|████▋     | 14/30 [00:39<00:25,  1.60s/it] 50%|█████     | 15/30 [00:40<00:23,  1.58s/it]                                                50%|█████     | 15/30 [00:40<00:23,  1.58s/it] 53%|█████▎    | 16/30 [00:42<00:21,  1.57s/it] 57%|█████▋    | 17/30 [00:43<00:20,  1.54s/it] 60%|██████    | 18/30 [00:45<00:18,  1.54s/it] 63%|██████▎   | 19/30 [00:46<00:16,  1.51s/it] 67%|██████▋   | 20/30 [00:48<00:14,  1.49s/it] 70%|███████   | 21/30 [00:49<00:13,  1.48s/it] 73%|███████▎  | 22/30 [00:51<00:11,  1.48s/it] 77%|███████▋  | 23/30 [00:52<00:10,  1.47s/it] 80%|████████  | 24/30 [00:54<00:09,  1.57s/it] 83%|████████▎ | 25/30 [00:55<00:07,  1.57s/it] 87%|████████▋ | 26/30 [00:57<00:06,  1.56s/it] 90%|█████████ | 27/30 [00:58<00:04,  1.55s/it] 93%|█████████▎| 28/30 [01:00<00:03,  1.54s/it] 97%|█████████▋| 29/30 [01:02<00:01,  1.56s/it]100%|██████████| 30/30 [01:03<00:00,  1.54s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.54s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.54s/it]100%|██████████| 30/30 [01:03<00:00,  2.12s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9532, 'grad_norm': 0.4706241488456726, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8671, 'grad_norm': 0.33528265357017517, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 63.5801, 'train_samples_per_second': 3.775, 'train_steps_per_second': 0.472, 'train_loss': 0.9101489384969076, 'epoch': 0.24}
END_PROFILE: 1747616699.9800522
Run  2: 65.907356075 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:12<00:25, 12.71s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:25<00:12, 12.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.39s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:14, 69.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 213.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:00<00:02, 308.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:00<00:02, 354.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:00<00:02, 380.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:00<00:01, 397.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:00<00:01, 406.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:01<00:01, 415.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:01<00:01, 420.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:01<00:01, 426.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:01<00:01, 427.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:01<00:00, 425.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:01<00:00, 427.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:01<00:00, 427.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:01<00:00, 427.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:02<00:00, 429.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:02<00:00, 429.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:02<00:00, 442.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 415.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 388.87 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747616785.0130033
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:18, 19.26s/it]  7%|▋         | 2/30 [00:20<04:06,  8.79s/it] 10%|█         | 3/30 [00:22<02:27,  5.46s/it] 13%|█▎        | 4/30 [00:23<01:40,  3.87s/it] 17%|█▋        | 5/30 [00:25<01:16,  3.06s/it] 20%|██        | 6/30 [00:26<01:00,  2.52s/it] 23%|██▎       | 7/30 [00:28<00:50,  2.17s/it] 27%|██▋       | 8/30 [00:29<00:42,  1.94s/it] 30%|███       | 9/30 [00:31<00:38,  1.83s/it] 33%|███▎      | 10/30 [00:32<00:34,  1.71s/it] 37%|███▋      | 11/30 [00:34<00:31,  1.64s/it] 40%|████      | 12/30 [00:35<00:29,  1.64s/it] 43%|████▎     | 13/30 [00:37<00:27,  1.60s/it] 47%|████▋     | 14/30 [00:38<00:25,  1.57s/it] 50%|█████     | 15/30 [00:40<00:23,  1.55s/it]                                                50%|█████     | 15/30 [00:40<00:23,  1.55s/it] 53%|█████▎    | 16/30 [00:41<00:21,  1.54s/it] 57%|█████▋    | 17/30 [00:43<00:19,  1.51s/it] 60%|██████    | 18/30 [00:44<00:18,  1.52s/it] 63%|██████▎   | 19/30 [00:46<00:16,  1.50s/it] 67%|██████▋   | 20/30 [00:47<00:14,  1.48s/it] 70%|███████   | 21/30 [00:49<00:13,  1.48s/it] 73%|███████▎  | 22/30 [00:50<00:11,  1.48s/it] 77%|███████▋  | 23/30 [00:52<00:10,  1.47s/it] 80%|████████  | 24/30 [00:53<00:09,  1.56s/it] 83%|████████▎ | 25/30 [00:55<00:07,  1.56s/it] 87%|████████▋ | 26/30 [00:56<00:06,  1.55s/it] 90%|█████████ | 27/30 [00:58<00:04,  1.54s/it] 93%|█████████▎| 28/30 [00:59<00:03,  1.53s/it] 97%|█████████▋| 29/30 [01:01<00:01,  1.56s/it]100%|██████████| 30/30 [01:03<00:00,  1.53s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.53s/it]                                               100%|██████████| 30/30 [01:03<00:00,  1.53s/it]100%|██████████| 30/30 [01:03<00:00,  2.10s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9528, 'grad_norm': 0.4676167666912079, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8669, 'grad_norm': 0.34035173058509827, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 63.0637, 'train_samples_per_second': 3.806, 'train_steps_per_second': 0.476, 'train_loss': 0.9098782857259115, 'epoch': 0.24}
END_PROFILE: 1747616848.837088
Run  3: 65.398578816 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 65.398578816 seconds
  Max elapsed : 67.285785328 seconds
  Avg elapsed : 66.19724007300000000000 seconds
===================================
--------------------------------------------
TORCH

>>> Run #1 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.34s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.03s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.60s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:14, 70.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   7%|▋         | 70/1000 [00:00<00:03, 243.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  13%|█▎        | 126/1000 [00:00<00:02, 320.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  18%|█▊        | 182/1000 [00:00<00:02, 364.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  24%|██▍       | 238/1000 [00:00<00:01, 388.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  29%|██▉       | 294/1000 [00:00<00:01, 405.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  35%|███▌      | 350/1000 [00:00<00:01, 418.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  41%|████      | 406/1000 [00:01<00:01, 423.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  46%|████▌     | 462/1000 [00:01<00:01, 427.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  52%|█████▏    | 518/1000 [00:01<00:01, 432.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  57%|█████▋    | 574/1000 [00:01<00:00, 434.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  63%|██████▎   | 630/1000 [00:01<00:00, 434.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  69%|██████▊   | 686/1000 [00:01<00:00, 437.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  74%|███████▍  | 742/1000 [00:01<00:00, 437.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  80%|███████▉  | 798/1000 [00:01<00:00, 435.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  85%|████████▌ | 854/1000 [00:02<00:00, 437.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  91%|█████████ | 909/1000 [00:02<00:00, 437.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  96%|█████████▌| 961/1000 [00:02<00:00, 445.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 399.73 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747616953.1046393
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:17, 19.22s/it]  7%|▋         | 2/30 [00:20<04:07,  8.84s/it] 10%|█         | 3/30 [00:22<02:29,  5.54s/it] 13%|█▎        | 4/30 [00:23<01:43,  3.97s/it] 17%|█▋        | 5/30 [00:25<01:19,  3.17s/it] 20%|██        | 6/30 [00:27<01:03,  2.63s/it] 23%|██▎       | 7/30 [00:28<00:52,  2.28s/it] 27%|██▋       | 8/30 [00:30<00:45,  2.06s/it] 30%|███       | 9/30 [00:32<00:40,  1.94s/it] 33%|███▎      | 10/30 [00:33<00:36,  1.83s/it] 37%|███▋      | 11/30 [00:35<00:33,  1.76s/it] 40%|████      | 12/30 [00:36<00:30,  1.70s/it] 43%|████▎     | 13/30 [00:38<00:28,  1.66s/it] 47%|████▋     | 14/30 [00:39<00:26,  1.63s/it] 50%|█████     | 15/30 [00:41<00:24,  1.61s/it]                                                50%|█████     | 15/30 [00:41<00:24,  1.61s/it] 53%|█████▎    | 16/30 [00:43<00:22,  1.61s/it] 57%|█████▋    | 17/30 [00:44<00:20,  1.60s/it] 60%|██████    | 18/30 [00:46<00:19,  1.62s/it] 63%|██████▎   | 19/30 [00:48<00:17,  1.62s/it] 67%|██████▋   | 20/30 [00:49<00:16,  1.60s/it] 70%|███████   | 21/30 [00:51<00:14,  1.60s/it] 73%|███████▎  | 22/30 [00:52<00:12,  1.59s/it] 77%|███████▋  | 23/30 [00:54<00:11,  1.59s/it] 80%|████████  | 24/30 [00:56<00:09,  1.61s/it] 83%|████████▎ | 25/30 [00:57<00:07,  1.60s/it] 87%|████████▋ | 26/30 [00:59<00:06,  1.59s/it] 90%|█████████ | 27/30 [01:00<00:04,  1.59s/it] 93%|█████████▎| 28/30 [01:02<00:03,  1.59s/it] 97%|█████████▋| 29/30 [01:03<00:01,  1.58s/it]100%|██████████| 30/30 [01:05<00:00,  1.58s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.58s/it]                                               100%|██████████| 30/30 [01:05<00:00,  1.58s/it]100%|██████████| 30/30 [01:05<00:00,  2.18s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9521, 'grad_norm': 0.467964768409729, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8667, 'grad_norm': 0.33352968096733093, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 65.464, 'train_samples_per_second': 3.666, 'train_steps_per_second': 0.458, 'train_loss': 0.9094204902648926, 'epoch': 0.24}
END_PROFILE: 1747617032.2778225
Run  1: 81.040194471 seconds

>>> Run #2 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.03s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.59s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:13, 71.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 217.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  11%|█         | 112/1000 [00:00<00:02, 310.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  17%|█▋        | 168/1000 [00:00<00:02, 359.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  22%|██▏       | 224/1000 [00:00<00:02, 384.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  28%|██▊       | 280/1000 [00:00<00:01, 399.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  34%|███▎      | 336/1000 [00:00<00:01, 411.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  39%|███▉      | 392/1000 [00:01<00:01, 418.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  45%|████▍     | 448/1000 [00:01<00:01, 425.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  50%|█████     | 504/1000 [00:01<00:01, 429.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  56%|█████▌    | 560/1000 [00:01<00:01, 430.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  62%|██████▏   | 616/1000 [00:01<00:00, 432.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  67%|██████▋   | 672/1000 [00:01<00:00, 434.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  73%|███████▎  | 728/1000 [00:01<00:00, 436.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  78%|███████▊  | 784/1000 [00:01<00:00, 437.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  84%|████████▍ | 840/1000 [00:02<00:00, 437.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  90%|████████▉ | 896/1000 [00:02<00:00, 438.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  95%|█████████▍| 948/1000 [00:02<00:00, 448.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 437.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 396.86 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747617115.8333478
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:20<09:52, 20.42s/it]  7%|▋         | 2/30 [00:22<04:22,  9.36s/it] 10%|█         | 3/30 [00:23<02:37,  5.84s/it] 13%|█▎        | 4/30 [00:25<01:48,  4.18s/it] 17%|█▋        | 5/30 [00:27<01:22,  3.31s/it] 20%|██        | 6/30 [00:28<01:05,  2.74s/it] 23%|██▎       | 7/30 [00:30<00:54,  2.37s/it] 27%|██▋       | 8/30 [00:31<00:46,  2.13s/it] 30%|███       | 9/30 [00:33<00:42,  2.01s/it] 33%|███▎      | 10/30 [00:35<00:37,  1.89s/it] 37%|███▋      | 11/30 [00:36<00:34,  1.81s/it] 40%|████      | 12/30 [00:38<00:31,  1.75s/it] 43%|████▎     | 13/30 [00:40<00:29,  1.71s/it] 47%|████▋     | 14/30 [00:41<00:26,  1.68s/it] 50%|█████     | 15/30 [00:43<00:24,  1.66s/it]                                                50%|█████     | 15/30 [00:43<00:24,  1.66s/it] 53%|█████▎    | 16/30 [00:45<00:23,  1.66s/it] 57%|█████▋    | 17/30 [00:46<00:21,  1.65s/it] 60%|██████    | 18/30 [00:48<00:19,  1.66s/it] 63%|██████▎   | 19/30 [00:49<00:18,  1.65s/it] 67%|██████▋   | 20/30 [00:51<00:16,  1.65s/it] 70%|███████   | 21/30 [00:53<00:14,  1.64s/it] 73%|███████▎  | 22/30 [00:54<00:13,  1.63s/it] 77%|███████▋  | 23/30 [00:56<00:11,  1.63s/it] 80%|████████  | 24/30 [00:58<00:09,  1.65s/it] 83%|████████▎ | 25/30 [00:59<00:08,  1.64s/it] 87%|████████▋ | 26/30 [01:01<00:06,  1.64s/it] 90%|█████████ | 27/30 [01:03<00:04,  1.63s/it] 93%|█████████▎| 28/30 [01:04<00:03,  1.63s/it] 97%|█████████▋| 29/30 [01:06<00:01,  1.63s/it]100%|██████████| 30/30 [01:07<00:00,  1.63s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.63s/it]                                               100%|██████████| 30/30 [01:07<00:00,  1.63s/it]100%|██████████| 30/30 [01:07<00:00,  2.26s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.952, 'grad_norm': 0.47666406631469727, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8666, 'grad_norm': 0.33197537064552307, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 67.9386, 'train_samples_per_second': 3.533, 'train_steps_per_second': 0.442, 'train_loss': 0.9092994372049967, 'epoch': 0.24}
END_PROFILE: 1747617197.4927363
Run  2: 83.543501385 seconds

>>> Run #3 of 3
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
Initializing model and tokenizer for mistral-7b...
==((====))==  Unsloth 2025.5.3: Fast Mistral patching. Transformers: 4.51.3.
   \\   /|    NVIDIA GH200 480GB. Num GPUs = 1. Max memory: 95.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30+1298453.d20250514. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:26, 13.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:26<00:13, 13.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.27s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.51s/it]
Adding LoRA adapters for mistral-7b...
Unsloth: Making `model.base_model.model.model` require gradients
Loading and preparing dataset...
Initializing trainer...
Unsloth: Tokenizing ["text"] (num_proc=72):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   1%|▏         | 14/1000 [00:00<00:14, 69.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):   6%|▌         | 56/1000 [00:00<00:04, 213.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  10%|▉         | 98/1000 [00:00<00:03, 287.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  15%|█▌        | 154/1000 [00:00<00:02, 346.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  21%|██        | 210/1000 [00:00<00:02, 374.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  27%|██▋       | 266/1000 [00:00<00:01, 390.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  32%|███▏      | 322/1000 [00:00<00:01, 401.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  38%|███▊      | 378/1000 [00:01<00:01, 413.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  42%|████▏     | 420/1000 [00:01<00:01, 414.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  48%|████▊     | 476/1000 [00:01<00:01, 420.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  53%|█████▎    | 532/1000 [00:01<00:01, 424.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  59%|█████▉    | 588/1000 [00:01<00:00, 425.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  64%|██████▍   | 644/1000 [00:01<00:00, 425.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  70%|███████   | 700/1000 [00:01<00:00, 426.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  76%|███████▌  | 756/1000 [00:01<00:00, 426.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  81%|████████  | 812/1000 [00:02<00:00, 428.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  87%|████████▋ | 868/1000 [00:02<00:00, 427.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  92%|█████████▏| 922/1000 [00:02<00:00, 422.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72):  97%|█████████▋| 974/1000 [00:02<00:00, 435.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=72): 100%|██████████| 1000/1000 [00:02<00:00, 389.40 examples/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8
 "-____-"     Trainable parameters = 20,971,520/7,268,995,072 (0.29% trained)
START_PROFILE: 1747617281.4640129
  0%|          | 0/30 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  3%|▎         | 1/30 [00:19<09:37, 19.93s/it]  7%|▋         | 2/30 [00:21<04:14,  9.09s/it] 10%|█         | 3/30 [00:22<02:32,  5.64s/it] 13%|█▎        | 4/30 [00:24<01:44,  4.01s/it] 17%|█▋        | 5/30 [00:26<01:19,  3.17s/it] 20%|██        | 6/30 [00:27<01:02,  2.61s/it] 23%|██▎       | 7/30 [00:29<00:51,  2.25s/it] 27%|██▋       | 8/30 [00:30<00:44,  2.01s/it] 30%|███       | 9/30 [00:32<00:39,  1.89s/it] 33%|███▎      | 10/30 [00:33<00:35,  1.77s/it] 37%|███▋      | 11/30 [00:35<00:32,  1.70s/it] 40%|████      | 12/30 [00:36<00:29,  1.64s/it] 43%|████▎     | 13/30 [00:38<00:27,  1.60s/it] 47%|████▋     | 14/30 [00:39<00:25,  1.57s/it] 50%|█████     | 15/30 [00:41<00:23,  1.55s/it]                                                50%|█████     | 15/30 [00:41<00:23,  1.55s/it] 53%|█████▎    | 16/30 [00:42<00:21,  1.55s/it] 57%|█████▋    | 17/30 [00:44<00:19,  1.54s/it] 60%|██████    | 18/30 [00:46<00:18,  1.56s/it] 63%|██████▎   | 19/30 [00:47<00:16,  1.54s/it] 67%|██████▋   | 20/30 [00:49<00:15,  1.53s/it] 70%|███████   | 21/30 [00:50<00:13,  1.53s/it] 73%|███████▎  | 22/30 [00:52<00:12,  1.53s/it] 77%|███████▋  | 23/30 [00:53<00:10,  1.52s/it] 80%|████████  | 24/30 [00:55<00:09,  1.54s/it] 83%|████████▎ | 25/30 [00:56<00:07,  1.53s/it] 87%|████████▋ | 26/30 [00:58<00:06,  1.53s/it] 90%|█████████ | 27/30 [00:59<00:04,  1.52s/it] 93%|█████████▎| 28/30 [01:01<00:03,  1.52s/it] 97%|█████████▋| 29/30 [01:02<00:01,  1.51s/it]100%|██████████| 30/30 [01:04<00:00,  1.52s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.52s/it]                                               100%|██████████| 30/30 [01:04<00:00,  1.52s/it]100%|██████████| 30/30 [01:04<00:00,  2.14s/it]
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.9527, 'grad_norm': 0.462904155254364, 'learning_rate': 0.00012800000000000002, 'epoch': 0.12}
{'loss': 0.8667, 'grad_norm': 0.3329416811466217, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.24}
{'train_runtime': 64.26, 'train_samples_per_second': 3.735, 'train_steps_per_second': 0.467, 'train_loss': 0.9097428639729818, 'epoch': 0.24}
END_PROFILE: 1747617359.46184
Run  3: 79.909844868 seconds

=== SUMMARY over 3 runs ===
  Min elapsed : 79.909844868 seconds
  Max elapsed : 83.543501385 seconds
  Avg elapsed : 81.49784690800000000000 seconds
===================================
--------------------------------------------
